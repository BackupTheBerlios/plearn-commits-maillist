From nouiz at mail.berlios.de  Mon Mar  2 17:19:25 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Mar 2009 17:19:25 +0100
Subject: [Plearn-commits] r9972 - in trunk: commands plearn_learners/cgi
Message-ID: <200903021619.n22GJPwm014584@sheep.berlios.de>

Author: nouiz
Date: 2009-03-02 17:19:24 +0100 (Mon, 02 Mar 2009)
New Revision: 9972

Added:
   trunk/plearn_learners/cgi/StabilisationLearner.cc
   trunk/plearn_learners/cgi/StabilisationLearner.h
Modified:
   trunk/commands/plearn_desjardins.cc
Log:
Added a class that do the stabilisation of the prediction.


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2009-02-27 22:59:28 UTC (rev 9971)
+++ trunk/commands/plearn_desjardins.cc	2009-03-02 16:19:24 UTC (rev 9972)
@@ -38,7 +38,11 @@
 
 //! All includes should go into plearn_inc.h.
 #include "plearn_version.h"
-//#include "plearn_noblas_inc.h"
+#ifndef WIN32
+#include <plearn/misc/ShellScript.h>
+#include <plearn/misc/Redirect.h>
+#endif
+
 /*****************
  * Miscellaneous *
  *****************/
@@ -74,6 +78,7 @@
 #include <plearn_learners/hyper/HyperLearner.h>
 #include <plearn_learners/hyper/HyperOptimize.h>
 #include <plearn_learners/hyper/EarlyStoppingOracle.h>
+#include <plearn_learners/cgi/StabilisationLearner.h>
 
 /************
  * Splitter *

Added: trunk/plearn_learners/cgi/StabilisationLearner.cc
===================================================================
--- trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-02-27 22:59:28 UTC (rev 9971)
+++ trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-03-02 16:19:24 UTC (rev 9972)
@@ -0,0 +1,169 @@
+// -*- C++ -*-
+
+// StabilisationLearner.cc
+//
+// Copyright (C) 2009 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file StabilisationLearner.cc */
+
+
+#include "StabilisationLearner.h"
+#include <plearn/io/pl_log.h>
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    StabilisationLearner,
+    "Stabilise the prediction to the old one if the confidence of the new one is under a threshold.",
+    "This class is used to don't have example that ping-pong between too"
+    " different class prediction. If the new prediction if different from"
+    " the old one, we need to have at least the 'thresold' as *confidence*. The"
+    " confidence is not well grouned in the theory, but is a good euristic.");
+
+StabilisationLearner::StabilisationLearner()
+    :threshold(0)
+{
+}
+
+void StabilisationLearner::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "threshold", &StabilisationLearner::threshold,
+                  OptionBase::buildoption,
+                  "The distance needed from 0.5 to accept the change");
+
+    inherited::declareOptions(ol);
+}
+
+void StabilisationLearner::build_()
+{
+}
+
+// ### Nothing to add here, simply calls build_
+void StabilisationLearner::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void StabilisationLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+
+int StabilisationLearner::outputsize() const
+{
+    return 1;
+}
+
+void StabilisationLearner::forget()
+{
+    inherited::forget();
+}
+
+void StabilisationLearner::train()
+{
+}
+
+
+void StabilisationLearner::computeOutput(const Vec& input, Vec& output) const
+{
+    real pred=int(input[0]);
+    real l1=input[1];
+    real l2=input[2];
+    real old_=int(input[3]);
+    real old,ret;
+    if(old_==3) old=2;
+    else old=old_;
+
+//    if not isNaN(real):     ret = real
+    if(old==pred)           ret = pred;
+    else if(old==0 and pred==2) ret = 1;
+    else if(old==2 and pred==0) ret = 1;
+    else if(old==0 and pred==1)
+        ret = (l1-threshold)>=0.5;//#(l1-0.5)>threshold
+    else if(old==1 and pred==0)
+        ret = (l1+threshold)>=0.5;
+    else if(old==1 and pred==2)
+        ret = ((l2-threshold)>=0.5)+1;
+    else if(old==2 and pred==1)
+        ret = ((l2+threshold)>=0.5)+1;
+    else{
+        ret = pred;
+        NORMAL_LOG<< "We don't know what to do with old="<<old<<" and pred="<<pred<<endl;    
+    }
+    output[0]=ret;
+
+}
+
+void StabilisationLearner::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    costs[0]=target[0]!=output[0];
+    real old_=int(input[3]);
+    real old;
+    if(old_==3) old=2;
+    else old=old_;
+    costs[1]=output[0]!=old;
+}
+
+TVec<string> StabilisationLearner::getTestCostNames() const
+{
+    TVec<string> names;
+    names.append("class_error");
+    names.append("changed");
+    return names;
+}
+
+TVec<string> StabilisationLearner::getTrainCostNames() const
+{
+    TVec<string> names;
+    return names;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/cgi/StabilisationLearner.h
===================================================================
--- trunk/plearn_learners/cgi/StabilisationLearner.h	2009-02-27 22:59:28 UTC (rev 9971)
+++ trunk/plearn_learners/cgi/StabilisationLearner.h	2009-03-02 16:19:24 UTC (rev 9972)
@@ -0,0 +1,162 @@
+// -*- C++ -*-
+
+// StabilisationLearner.h
+//
+// Copyright (C) 2009 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file StabilisationLearner.h */
+
+
+#ifndef StabilisationLearner_INC
+#define StabilisationLearner_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class StabilisationLearner : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! the threshold value. i.e. the distance needed from 0.5 to accept the change.
+    real threshold;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    StabilisationLearner();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    PLEARN_DECLARE_OBJECT(StabilisationLearner);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(StabilisationLearner);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Mon Mar  2 17:43:06 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Mar 2009 17:43:06 +0100
Subject: [Plearn-commits] r9973 - trunk/commands/PLearnCommands
Message-ID: <200903021643.n22Gh6sf018856@sheep.berlios.de>

Author: nouiz
Date: 2009-03-02 17:43:06 +0100 (Mon, 02 Mar 2009)
New Revision: 9973

Modified:
   trunk/commands/PLearnCommands/plearn_main.cc
Log:
print less stuff with the option no-version


Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2009-03-02 16:19:24 UTC (rev 9972)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2009-03-02 16:43:06 UTC (rev 9973)
@@ -297,9 +297,12 @@
         }
     command_line.resize( cleaned ); // Truncating the end of the vector.
   
-    if (no_version_pos == -1)
+    if (no_version_pos == -1){
         output_version( );
-
+#ifdef _OPENMP
+        pout<<"Using OPENMP with "+tostring(omp_get_max_threads())+" threads."<<endl;
+#endif
+    }
     if (no_progress_bars != -1)
         ProgressBar::setPlugin(new NullProgressBarPlugin);
 
@@ -344,9 +347,6 @@
         PLMPI::init(&argc, &argv);
 #endif
 
-#ifdef _OPENMP
-        pout<<"Using OPENMP with "+tostring(omp_get_max_threads())+" threads."<<endl;
-#endif
         seed();
 
         // set program name



From nouiz at mail.berlios.de  Mon Mar  2 21:45:53 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Mar 2009 21:45:53 +0100
Subject: [Plearn-commits] r9974 - trunk/python_modules/plearn/pymake
Message-ID: <200903022045.n22KjrRW030824@sheep.berlios.de>

Author: nouiz
Date: 2009-03-02 21:45:52 +0100 (Mon, 02 Mar 2009)
New Revision: 9974

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
ignore PYMAKE_OPTIONS env variables with the -clean option.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-03-02 16:43:06 UTC (rev 9973)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-03-02 20:45:52 UTC (rev 9974)
@@ -2872,13 +2872,12 @@
 
     ##  Special options that will not compile, but perform various operations
     if 'clean' in optionargs:
-        #remove some option that are save to have.
-        #some people can put those option in their PYMAKE_OPTIONS and 
+        #some people can put options in their PYMAKE_OPTIONS and 
         #we want them to be able to do pymake -clean .
-        allowed_options=['local_ofiles','logging=dbg','logging=dbg-profile', 'goto']
-        for i in allowed_options:
+        for i in os.getenv('PYMAKE_OPTIONS').split():
+            i=i[1:]
             if i in optionargs: optionargs.remove(i)
-
+                        
         if len(optionargs)!=1 or len(otherargs)==0:
             print 'BAD ARGUMENTS: with -clean, specify one or more directories to clean, but no other -option:', optionargs
             sys.exit(100)



From nouiz at mail.berlios.de  Mon Mar  2 22:24:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Mar 2009 22:24:58 +0100
Subject: [Plearn-commits] r9975 - trunk/plearn/vmat
Message-ID: <200903022124.n22LOwk1003020@sheep.berlios.de>

Author: nouiz
Date: 2009-03-02 22:24:58 +0100 (Mon, 02 Mar 2009)
New Revision: 9975

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
small optimization


Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2009-03-02 20:45:52 UTC (rev 9974)
+++ trunk/plearn/vmat/VMatLanguage.cc	2009-03-02 21:24:58 UTC (rev 9975)
@@ -272,7 +272,10 @@
 }
 
 void VMatLanguage::setSourceFieldNames(TVec<string> the_srcfieldnames)
-{ srcfieldnames = the_srcfieldnames; }
+{ 
+    srcfieldnames = the_srcfieldnames; 
+    myvec.resize(srcfieldnames.length());
+}
 
 //! Make it an empty program by clearing outputfieldnames, program, mappings
 void VMatLanguage::clear()
@@ -1373,7 +1376,6 @@
 
 void VMatLanguage::run(int rowindex, const Vec& result) const
 {
-    myvec.resize(srcfieldnames.length());
     vmsource->getRow(rowindex,myvec);
     run(myvec, result, rowindex);
 }



From nouiz at mail.berlios.de  Mon Mar  2 23:56:46 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 2 Mar 2009 23:56:46 +0100
Subject: [Plearn-commits] r9976 - trunk/scripts
Message-ID: <200903022256.n22Muk3w015789@sheep.berlios.de>

Author: nouiz
Date: 2009-03-02 23:56:46 +0100 (Mon, 02 Mar 2009)
New Revision: 9976

Modified:
   trunk/scripts/dbidispatch
Log:
added the option --only_n_first=N


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-03-02 21:24:58 UTC (rev 9975)
+++ trunk/scripts/dbidispatch	2009-03-02 22:56:46 UTC (rev 9976)
@@ -5,7 +5,7 @@
 from subprocess import Popen,PIPE
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart_jobs condor_jobs_number... }
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [--only_n_first=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart_jobs condor_jobs_number... }
 
 <back-end parameter>:
     bqtools, cluster, condor option  : [--mem=N]
@@ -79,6 +79,7 @@
     history on the local host and relaunch those jobs. We only take the command
     line that was executed, all other options are are those passed to 
     dbidispatch this time. Work only with jobs launched with dbidispatch.
+  The '--only_n_first=N' option tell to launch only the first N jobs from the list.
 
 bqtools, cluster and condor option:
   The '--mem=X' speficify the number of ram in meg the program need to execute.
@@ -311,7 +312,8 @@
                                 "--req", "--files", "--raw", "--rank", "--env",
                                 "--universe", "--exp_dir", "--machine", "--machines",
                                 "--queue", "--nano", "--submit_options",
-                                "--jobs_name", "--file", "--tasks_filename"]:
+                                "--jobs_name", "--file", "--tasks_filename",
+                                "--only_n_first" ]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -487,6 +489,10 @@
 else:
     (commands,choise_args)=generate_commands(command_argv)
 
+if dbi_param.has_key("only_n_first"):
+    commands=commands[:int(dbi_param["only_n_first"])]
+    del dbi_param["only_n_first"]
+
 #we duplicate the command so that everything else work correctly.
 if dbi_param.has_key("to_all"):
     assert(len(commands)==1)



From ducharme at mail.berlios.de  Tue Mar  3 16:56:49 2009
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 3 Mar 2009 16:56:49 +0100
Subject: [Plearn-commits] r9977 - trunk/plearn_learners/generic
Message-ID: <200903031556.n23FunoR030619@sheep.berlios.de>

Author: ducharme
Date: 2009-03-03 16:56:49 +0100 (Tue, 03 Mar 2009)
New Revision: 9977

Added:
   trunk/plearn_learners/generic/IdentityPLearner.cc
   trunk/plearn_learners/generic/IdentityPLearner.h
Log:
Utility learner that simply outputs its inputs.


Added: trunk/plearn_learners/generic/IdentityPLearner.cc
===================================================================
--- trunk/plearn_learners/generic/IdentityPLearner.cc	2009-03-02 22:56:46 UTC (rev 9976)
+++ trunk/plearn_learners/generic/IdentityPLearner.cc	2009-03-03 15:56:49 UTC (rev 9977)
@@ -0,0 +1,100 @@
+// -*- C++ -*-
+
+// IdentityPLearner.cc
+//
+// Copyright (C) 2009 R?jean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: R?jean Ducharme
+
+/*! \file IdentityPLearner.cc */
+
+
+#include "IdentityPLearner.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    IdentityPLearner,
+    "Simple learner that outputs its inputs",
+    "Simple learner that outputs its inputs");
+
+IdentityPLearner::IdentityPLearner()
+{}
+
+int IdentityPLearner::outputsize() const
+{
+    return inputsize();
+}
+
+// Nothing to do!
+void IdentityPLearner::train()
+{}
+
+
+void IdentityPLearner::computeOutput(const Vec& input, Vec& output) const
+{
+    // Copy input to output vector
+    int nout = inputsize();
+    output.resize(nout);
+    output << input;
+}
+
+void IdentityPLearner::computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const
+{
+    costs.resize(0);
+}
+
+TVec<string> IdentityPLearner::getTestCostNames() const
+{
+    return TVec<string>();
+}
+
+TVec<string> IdentityPLearner::getTrainCostNames() const
+{
+    return TVec<string>();
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/IdentityPLearner.h
===================================================================
--- trunk/plearn_learners/generic/IdentityPLearner.h	2009-03-02 22:56:46 UTC (rev 9976)
+++ trunk/plearn_learners/generic/IdentityPLearner.h	2009-03-03 15:56:49 UTC (rev 9977)
@@ -0,0 +1,110 @@
+// -*- C++ -*-
+
+// IdentityPLearner.h
+//
+// Copyright (C) 2009 R?jean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: R?jean Ducharme
+
+/*! \file IdentityPLearner.h */
+
+
+#ifndef IdentityPLearner_INC
+#define IdentityPLearner_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+class IdentityPLearner : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    IdentityPLearner();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutputs (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(IdentityPLearner);
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(IdentityPLearner);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From ducharme at mail.berlios.de  Tue Mar  3 17:10:49 2009
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 3 Mar 2009 17:10:49 +0100
Subject: [Plearn-commits] r9978 - trunk/plearn_learners/regressors
Message-ID: <200903031610.n23GAn3m031927@sheep.berlios.de>

Author: ducharme
Date: 2009-03-03 17:10:49 +0100 (Tue, 03 Mar 2009)
New Revision: 9978

Modified:
   trunk/plearn_learners/regressors/BasisSelectionRegressor.h
Log:
Code cleaning...


Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2009-03-03 15:56:49 UTC (rev 9977)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2009-03-03 16:10:49 UTC (rev 9978)
@@ -144,22 +144,6 @@
     virtual TVec<std::string> getTrainCostNames() const;
 
 
-    // *** SUBCLASS WRITING: ***
-    // While in general not necessary, in case of particular needs
-    // (efficiency concerns for ex) you may also want to overload
-    // some of the following methods:
-    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
-    //                                    Vec& output, Vec& costs) const;
-    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
-    //                               Vec& costs) const;
-    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
-    //                   VMat testoutputs=0, VMat testcosts=0) const;
-    // virtual int nTestCosts() const;
-    // virtual int nTrainCosts() const;
-    // virtual void resetInternalState();
-    // virtual bool isStatefulLearner() const;
-
-
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From ducharme at mail.berlios.de  Tue Mar  3 17:12:02 2009
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 3 Mar 2009 17:12:02 +0100
Subject: [Plearn-commits] r9979 - trunk/plearn_learners/regressors
Message-ID: <200903031612.n23GC22t032030@sheep.berlios.de>

Author: ducharme
Date: 2009-03-03 17:12:02 +0100 (Tue, 03 Mar 2009)
New Revision: 9979

Modified:
   trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
Log:
We permit, in special cases, to have a sub-learner with outputsize>1.


Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2009-03-03 16:10:49 UTC (rev 9978)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2009-03-03 16:12:02 UTC (rev 9979)
@@ -311,7 +311,8 @@
 
 int BasisSelectionRegressor::outputsize() const
 {
-    return 1;
+    //return 1;
+    return template_learner->outputsize();
 }
 
 void BasisSelectionRegressor::forget()
@@ -1124,6 +1125,7 @@
         newtrainset= new RealFunctionsProcessedVMatrix(train_set, selected_functions, false, true, true);
     newtrainset->defineSizes(nf,1,weighted?1:0);
     learner->setTrainingSet(newtrainset);
+    template_learner->setTrainingSet(newtrainset);
     learner->forget();
     learner->train();
     // resize features matrix so it contains only the features
@@ -1158,7 +1160,8 @@
                 recomputeFeatures();
                 if(stage==0) // only mandatory funcs.
                     retrainLearner();
-                recomputeResidue();
+                if (candidate_functions.length()>0)
+                    recomputeResidue();
             }
         }
 
@@ -1269,8 +1272,8 @@
 void BasisSelectionRegressor::computeOutputFromFeaturevec(const Vec& featurevec, Vec& output) const
 {
     int nout = outputsize();
-    if(nout!=1)
-        PLERROR("outputsize should always be one for this learner");
+    if(nout!=1 && !use_all_basis)
+        PLERROR("outputsize should always be 1 for this learner (=%d)", nout);
     output.resize(nout);
 
     if(learner.isNull())



From tihocan at mail.berlios.de  Tue Mar  3 21:15:28 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 3 Mar 2009 21:15:28 +0100
Subject: [Plearn-commits] r9980 - trunk/plearn/vmat
Message-ID: <200903032015.n23KFS2a001617@sheep.berlios.de>

Author: tihocan
Date: 2009-03-03 21:15:28 +0100 (Tue, 03 Mar 2009)
New Revision: 9980

Modified:
   trunk/plearn/vmat/BinaryOpVMatrix.cc
   trunk/plearn/vmat/BinaryOpVMatrix.h
Log:
Better implementation of the build mechanism in constructor

Modified: trunk/plearn/vmat/BinaryOpVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BinaryOpVMatrix.cc	2009-03-03 16:12:02 UTC (rev 9979)
+++ trunk/plearn/vmat/BinaryOpVMatrix.cc	2009-03-03 20:15:28 UTC (rev 9980)
@@ -48,20 +48,23 @@
 
 
 BinaryOpVMatrix::BinaryOpVMatrix()
-{
-}
+{}
 
-BinaryOpVMatrix::BinaryOpVMatrix(VMat source1, VMat source2, string op):
+BinaryOpVMatrix::BinaryOpVMatrix(VMat source1, VMat source2, const string& op,
+                                 bool call_build_):
+    inherited(call_build_),
     source1(source1),
     source2(source2),
     op(op)
 {
-    build();
+    if (call_build_)
+        build_();
 }
+
 PLEARN_IMPLEMENT_OBJECT(BinaryOpVMatrix,
-                        "This VMat allows simple binary operations on two VMatrix.",
-                        "It is assumed that the two source matrices are the same size"
-    );
+        "This VMat allows simple binary operations on two VMatrix.",
+        "It is assumed that the two source matrices have the same size."
+);
 
 void BinaryOpVMatrix::getNewRow(int i, const Vec& v) const
 {

Modified: trunk/plearn/vmat/BinaryOpVMatrix.h
===================================================================
--- trunk/plearn/vmat/BinaryOpVMatrix.h	2009-03-03 16:12:02 UTC (rev 9979)
+++ trunk/plearn/vmat/BinaryOpVMatrix.h	2009-03-03 20:15:28 UTC (rev 9980)
@@ -82,7 +82,9 @@
     //! Default constructor.
     BinaryOpVMatrix();
 
-    BinaryOpVMatrix(VMat source1, VMat source2, string op);
+    BinaryOpVMatrix(VMat source1, VMat source2, const string& op,
+                    bool call_build_ = true);
+
     // ******************
     // * Object methods *
     // ******************



From nouiz at mail.berlios.de  Wed Mar  4 15:19:00 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 4 Mar 2009 15:19:00 +0100
Subject: [Plearn-commits] r9981 - in trunk: . plearn/python
Message-ID: <200903041419.n24EJ0Oi016478@sheep.berlios.de>

Author: nouiz
Date: 2009-03-04 15:18:59 +0100 (Wed, 04 Mar 2009)
New Revision: 9981

Modified:
   trunk/plearn/python/PythonIncludes.h
   trunk/pymake.config.model
Log:
added python2.6 as a version of python that is accepted. Made this version the default version on mammouth


Modified: trunk/plearn/python/PythonIncludes.h
===================================================================
--- trunk/plearn/python/PythonIncludes.h	2009-03-03 20:15:28 UTC (rev 9980)
+++ trunk/plearn/python/PythonIncludes.h	2009-03-04 14:18:59 UTC (rev 9981)
@@ -73,8 +73,24 @@
 #  error "Symbols PL_USE_NUMARRAY and PL_USE_NUMPY are mutually exclusive; they should not both be defined"
 #endif
 
-#if PL_PYTHON_VERSION >= 250
+#if PL_PYTHON_VERSION >= 260
 
+#include <python2.6/Python.h>
+#include <python2.6/compile.h>  // define PyCodeObject
+#include <python2.6/eval.h>     // for accessing PyEval_EvalCode: not included by default
+#ifdef PL_USE_NUMARRAY
+#  include <python2.6/numarray/libnumarray.h>
+#else
+#  ifdef PL_USE_NUMPY
+#    pragma GCC system_header //suppress all warnings/errors for numpy
+#    include <libnumarray.h>
+#  else
+#    error "should use either NumPy (preferred) or NUMARRAY (deprecated)"
+#  endif //def PL_USE_NUMPY
+#endif //def PL_USE_NUMARRAY
+
+#elif PL_PYTHON_VERSION >= 250
+
 #include <python2.5/Python.h>
 #include <python2.5/compile.h>  // define PyCodeObject
 #include <python2.5/eval.h>     // for accessing PyEval_EvalCode: not included by default

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-03-03 20:15:28 UTC (rev 9980)
+++ trunk/pymake.config.model	2009-03-04 14:18:59 UTC (rev 9981)
@@ -48,7 +48,6 @@
 # create_dll: we link the object files in a Windows library, not an executable
 #####
 
-
 # Qt Specific stuff
 
 qtdir = '/usr/lib/qt-3.1/'
@@ -268,7 +267,7 @@
   [ 'double', 'float' ],
   
   # [ 'throwerrors', 'exiterrors' ],
-  [ 'autopython', 'python23', 'python24', 'python25', 'nopython' ],
+  [ 'autopython', 'python23', 'python24', 'python25', 'python26','nopython' ],
   
   [ 'blas', 'noblas' ],
   [ 'defblas', 'nolibblas', 'p3blas','p4blas','athlonblas','pentiumblas',
@@ -317,7 +316,6 @@
 
 pyver = sys.version.split()[0][0:3]
 pyoption = 'python%s' % pyver.replace('.', '')
-
 # Verify/set optionargs for python and numpy
 python_choices= [x for x in options_choices if 'autopython' in x][0]
 if [x for x in python_choices if x in optionargs]==[] and not domain_name.endswith('.apstat.com'):
@@ -370,9 +368,9 @@
             #numpy_site_packages = '/u/lisa/local/' + target_platform + '/lib/python%s/site-packages/numarray -lutil' % pyver
     elif domain_name.endswith('.m'):
         #numpy_site_packages = join(homedir, '../delallea/local/lib/python2.5/site-packages/numarray -lutil')
-        python_version = '2.5'
+        python_version = '2.6'
         optionargs += [ 'python%s' % python_version.replace('.', '') ]
-        python_lib_root = '/opt/python64/2.5.1/lib'
+        python_lib_root = '/opt/python64/2.6.1/lib'
     elif domain_name.endswith('.rqchp.qc.ca'):
         numpy_includedirs   = [ '/usr/network.ALTIX/python-2.4.1/include' ]
         numpy_site_packages = join(homedir, '../delallea/local/lib/python2.4/site-packages/numarray -lutil')
@@ -387,6 +385,8 @@
             python_version = '2.4'
         elif 'python25' in optionargs:
             python_version = '2.5'
+        elif 'python26' in optionargs:
+            python_version = '2.6'
         else:
             python_version = pyver
             
@@ -653,7 +653,7 @@
               # remark #981: operands are evaluated in unspecified order
               # remark #383: value copied to temporary, reference to temporary used
               # remark #1418: external function definition with no prior declaration
-              compiler = 'icpc -wd981 -wd383 -wd1418',
+              compiler = 'icpc -wd981 -wd383 -wd1418 -wd869',
               cpp_definitions = ['USING_MPI=0'],
               linker = 'icpc  '
               )
@@ -798,6 +798,9 @@
 pymakeOption( name = 'python25',
               description = 'the installed version of python is 2.5.X',
               cpp_definitions = ['PL_PYTHON_VERSION=250'] )
+pymakeOption( name = 'python26',
+              description = 'the installed version of python is 2.6.X',
+              cpp_definitions = ['PL_PYTHON_VERSION=260'] )
 pymakeOption( name = 'nopython',
               description = 'compile w/o python')
 
@@ -867,7 +870,7 @@
 
 pymakeLinkOption( name = 'mammouthblas',
               description = 'linking BLAS for P4 Mammouth-Serie cluster',
-              linkeroptions = '-L/opt/intel/mkl/9.0/lib/em64t -lmkl -lmkl_lapack -openmp' ) #-lmkl_lapack -lmkl_p4 -lmkl_vml_p4 -lpthread ' )
+              linkeroptions = '-lmkl -lmkl_lapack -openmp' ) #-lmkl_lapack -lmkl_p4 -lmkl_vml_p4 -lpthread ' )
 
 pymakeLinkOption( name = 'apintelblas',
               description = 'Intel BLAS+LAPACK for generic install in /usr/local/lib (incl. ApSTAT)',



From nouiz at mail.berlios.de  Wed Mar  4 17:08:04 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 4 Mar 2009 17:08:04 +0100
Subject: [Plearn-commits] r9982 - trunk/scripts
Message-ID: <200903041608.n24G84iH030201@sheep.berlios.de>

Author: nouiz
Date: 2009-03-04 17:08:03 +0100 (Wed, 04 Mar 2009)
New Revision: 9982

Modified:
   trunk/scripts/appendresults
   trunk/scripts/makeresults
Log:
we wait 60s between each lock try in case the filesystem is too busy.
in that case retrying too often worsen the trouble.



Modified: trunk/scripts/appendresults
===================================================================
--- trunk/scripts/appendresults	2009-03-04 14:18:59 UTC (rev 9981)
+++ trunk/scripts/appendresults	2009-03-04 16:08:03 UTC (rev 9982)
@@ -31,7 +31,9 @@
 done
 
 # Wait until nobody else is messing with the output file.
-lockfile $OUTPUT.lock
+# we wait 60s in case the filesystem is too busy.
+# in that case retrying too often worsen the trouble.
+lockfile -n60 $OUTPUT.lock
 
 if [ -d $DIR ];then
     echo -n "$PARAMS" >> $OUTPUT

Modified: trunk/scripts/makeresults
===================================================================
--- trunk/scripts/makeresults	2009-03-04 14:18:59 UTC (rev 9981)
+++ trunk/scripts/makeresults	2009-03-04 16:08:03 UTC (rev 9982)
@@ -25,7 +25,9 @@
 shift
 
 # Wait until nobody else is messing with the output file.
-lockfile $MATNAME.amat.lock
+# we wait 60s in case the filesystem is too busy.
+# in that case retrying too often worsen the trouble.
+lockfile -n60 $MATNAME.amat.lock
 
 # Don't do anything if files already exist.
 if [ -f $MATNAME.amat ] && [ -f $MATNAME.vmat ]; then



From nouiz at mail.berlios.de  Wed Mar  4 19:29:14 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 4 Mar 2009 19:29:14 +0100
Subject: [Plearn-commits] r9983 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200903041829.n24ITE2t027997@sheep.berlios.de>

Author: nouiz
Date: 2009-03-04 19:29:13 +0100 (Wed, 04 Mar 2009)
New Revision: 9983

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added condor option --no_machine


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-03-04 16:08:03 UTC (rev 9982)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-03-04 18:29:13 UTC (rev 9983)
@@ -796,6 +796,7 @@
         self.universe = "vanilla"
         self.machine = []
         self.machines = []
+        self.no_machine = []
         self.to_all = False
         self.keep_failed_jobs_in_queue = False
         self.clean_up = True
@@ -1349,14 +1350,20 @@
             assert(len(self.machines)==0)
             for m in self.machine:
                 self.tasks_req.append(self.req+'&&(Machine=="'+m+'")')
+        
+        for m in self.machines:
+            machine_choice.append('regexp("'+m+'", Machine)')
 
-        for m in self.machines:
-            machine_choice.append('regexp("'+m+'", target.Machine)')
-        if machine_choice:
+        if len(machine_choice)==1:
+            self.req+="&&("+machine_choice[0]+")"
+        elif machine_choice:
             self.req+="&&(False "
             for m in machine_choice:
                 self.req+="||"+m
             self.req+=")"
+
+        for m in self.no_machine:
+            self.req+='&&(Machine!="'+m+'")'
         #if no mem requirement added, use the executable size.
         #todo: if they are not the same executable, take the biggest
         if self.mem<=0:

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-03-04 16:08:03 UTC (rev 9982)
+++ trunk/scripts/dbidispatch	2009-03-04 18:29:13 UTC (rev 9983)
@@ -32,6 +32,7 @@
                               [--universe={vanilla*, standard, grid, java,
                                            scheduler, local, parallel, vm}]
                               [--machine=HOSTNAME+] [--machines=regex+]
+                              [--no_machine=HOSTNAME+]
                               [--[*no_]keep_failed_jobs_in_queue]
 
 An * after '[', '{' or ',' signals the default value.
@@ -173,6 +174,8 @@
      host is full_host_name. If multiple --machine or --machines options,
     take anyone of them. Is equivalent to
      dbidispatch '--req=Machine=="full_host_name"'
+  The '--no_machine=full_host_name' option remove the machines from possible 
+    host to run your jobs.
   The '--machines=regexp' option add the requirement that the executing host 
     name must be match the regexp. If multiple --machine or --machines options,
     take anyone of them.
@@ -313,7 +316,7 @@
                                 "--universe", "--exp_dir", "--machine", "--machines",
                                 "--queue", "--nano", "--submit_options",
                                 "--jobs_name", "--file", "--tasks_filename",
-                                "--only_n_first" ]:
+                                "--only_n_first", "--no_machine" ]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -327,7 +330,7 @@
         elif param=="env":
             dbi_param.setdefault(param,"")
             dbi_param[param]+='"'+val+'"'
-        elif param in ["machines", "machine", "tasks_filename"]:
+        elif param in ["machine", "machines", "no_machine", "tasks_filename"]:
             dbi_param.setdefault(param,[])
             dbi_param[param]+=val.split(",")
         else:
@@ -376,7 +379,7 @@
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
-                       "universe", "machine", "machines", "to_all", 
+                       "universe", "machine", "machines", "no_machine","to_all",
                        "keep_failed_jobs_in_queue", "tasks_filename"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",



From nouiz at mail.berlios.de  Wed Mar  4 23:04:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 4 Mar 2009 23:04:12 +0100
Subject: [Plearn-commits] r9984 - trunk/python_modules/plearn/utilities
Message-ID: <200903042204.n24M4CpR017940@sheep.berlios.de>

Author: nouiz
Date: 2009-03-04 23:04:12 +0100 (Wed, 04 Mar 2009)
New Revision: 9984

Modified:
   trunk/python_modules/plearn/utilities/toolkit.py
Log:
moved an include to the function that use it. I did this as this include is deprecated and the function is not used often, so we see the warning less often. This fix 2 test under python 2.6.1


Modified: trunk/python_modules/plearn/utilities/toolkit.py
===================================================================
--- trunk/python_modules/plearn/utilities/toolkit.py	2009-03-04 18:29:13 UTC (rev 9983)
+++ trunk/python_modules/plearn/utilities/toolkit.py	2009-03-04 22:04:12 UTC (rev 9984)
@@ -5,7 +5,7 @@
 module seems to manage similar tasks, it is probably time to create a
 I{similar_tasks.py} L{utilities} submodule to move those functions to.
 """
-import inspect, os, popen2, shutil, string, sys, time, types
+import inspect, os, shutil, string, sys, time, types
 from os.path import exists, join, abspath
 from string import split
 
@@ -66,6 +66,7 @@
     @return: Output lines.
     @rtype:  Array of strings.
     """
+    import popen2
     if stderr and stdout:
         (stdout_and_stderr, stdin) = popen2.popen4(command)
         return stdout_and_stderr.readlines()



From nouiz at mail.berlios.de  Thu Mar  5 16:17:13 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 5 Mar 2009 16:17:13 +0100
Subject: [Plearn-commits] r9985 - trunk/python_modules/plearn/pymake
Message-ID: <200903051517.n25FHDhi000738@sheep.berlios.de>

Author: nouiz
Date: 2009-03-05 16:17:12 +0100 (Thu, 05 Mar 2009)
New Revision: 9985

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
better comment.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-03-04 22:04:12 UTC (rev 9984)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-03-05 15:17:12 UTC (rev 9985)
@@ -2872,7 +2872,7 @@
 
     ##  Special options that will not compile, but perform various operations
     if 'clean' in optionargs:
-        #some people can put options in their PYMAKE_OPTIONS and 
+        #some people put options in their PYMAKE_OPTIONS env variable and
         #we want them to be able to do pymake -clean .
         for i in os.getenv('PYMAKE_OPTIONS').split():
             i=i[1:]



From saintmlx at mail.berlios.de  Thu Mar  5 19:22:02 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 5 Mar 2009 19:22:02 +0100
Subject: [Plearn-commits] r9986 - trunk/plearn/python
Message-ID: <200903051822.n25IM20c028176@sheep.berlios.de>

Author: saintmlx
Date: 2009-03-05 19:22:01 +0100 (Thu, 05 Mar 2009)
New Revision: 9986

Modified:
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonExtension.h
   trunk/plearn/python/PythonObjectWrapper.cc
Log:
- added python exception PLearnError for PLERRORs



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2009-03-05 15:17:12 UTC (rev 9985)
+++ trunk/plearn/python/PythonExtension.cc	2009-03-05 18:22:01 UTC (rev 9986)
@@ -396,6 +396,30 @@
     }
 }
 
+PyObject* the_PLearn_python_exception= 0;
+void injectPLearnException(PyObject* module)
+{
+    PyObject* res= PyRun_String("class PLearnError(Exception):\n\tpass\n", 
+                                Py_file_input, 
+                                PyModule_GetDict(module), 
+                                PyModule_GetDict(module));
+    if(!res)
+    {
+        if(PyErr_Occurred()) PyErr_Print();
+        PLERROR("in injectPLearnException : cannot create PLearnException class.");
+    }
+    Py_DECREF(res);
+
+    the_PLearn_python_exception= PyObject_GetAttrString(module, "PLearnError");
+    if(!the_PLearn_python_exception)
+    {
+        if(PyErr_Occurred()) PyErr_Print();
+        PLERROR("in injectPLearnException : cannot retrieve PLearnException class.");
+    }
+    //keep ref. to PLearnError forever.
+}
+
+
 void createWrappedObjectsSet(PyObject* module)
 {
     /* can't set logging before this gets called
@@ -461,7 +485,7 @@
 // init module, then inject global funcs
 void initPythonExtensionModule(char const * module_name)
 {
-    /* can't set logging before this gets called
+    /* // can't set logging before this gets called
     perr << "[pid=" << getPid() << "] "
          << "initPythonExtensionModule name=" << module_name << endl;
     */
@@ -472,12 +496,13 @@
 
 void setPythonModuleAndInject(PyObject* module)
 {
-    /* can't set logging before this gets called
+    /* //can't set logging before this gets called
     perr << "[pid=" << getPid() << "] "
          << "setPythonModuleAndInject for module: " << PythonObjectWrapper(module) << "\tat " << (void*)module << endl;
     */
     injectPLearnGlobalFunctions(module);
     injectPLearnClasses(module);
+    injectPLearnException(module);
     createWrappedObjectsSet(module);
     the_PLearn_python_module= module;   
 }

Modified: trunk/plearn/python/PythonExtension.h
===================================================================
--- trunk/plearn/python/PythonExtension.h	2009-03-05 15:17:12 UTC (rev 9985)
+++ trunk/plearn/python/PythonExtension.h	2009-03-05 18:22:01 UTC (rev 9986)
@@ -59,6 +59,7 @@
 
 
 extern PyObject* the_PLearn_python_module;
+extern PyObject* the_PLearn_python_exception;
 
 } // end of namespace PLearn
 

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2009-03-05 15:17:12 UTC (rev 9985)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2009-03-05 18:22:01 UTC (rev 9986)
@@ -551,7 +551,7 @@
     }
     catch(const PLearnError& e)
     {
-        PyErr_SetString(PyExc_Exception, e.message().c_str());
+        PyErr_SetString(the_PLearn_python_exception, e.message().c_str());
         return 0;
     }
     catch(const std::exception& e)



From saintmlx at mail.berlios.de  Thu Mar  5 19:41:44 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 5 Mar 2009 19:41:44 +0100
Subject: [Plearn-commits] r9987 - trunk/python_modules/plearn/pybridge
Message-ID: <200903051841.n25IfitN028522@sheep.berlios.de>

Author: saintmlx
Date: 2009-03-05 19:41:43 +0100 (Thu, 05 Mar 2009)
New Revision: 9987

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- __getitem__ for VMats now faster and w/ better exception handling



Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2009-03-05 18:22:01 UTC (rev 9986)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2009-03-05 18:41:43 UTC (rev 9987)
@@ -215,22 +215,37 @@
         return self.length
 
     def __getitem__(self, key):
-        l= len(self)
-        if isinstance(key,int):
-            if key < 0: key+= l
-            if key < 0 or key >= l:
-                raise IndexError
-            return self.getRow(key)
-        if isinstance(key,slice):
+        class len_thunk(object):
+            """
+            get length only as needed, and only once.
+            """
+            __slots__ = ['vm','l']
+            def __init__(self, vm):
+                self.vm= vm
+                self.l= None
+            def __call__(self):
+                self.l= self.l or len(self.vm)
+                return self.l
+        lt= len_thunk(self)
+        if isinstance(key, int):
+            if key < 0: key+= lt()
+            try:
+                return self.getRow(key)
+            except get_plearn_module().PLearnError, e:
+                if 'OUT OF BOUND' in e.message:
+                    raise IndexError(e)
+                else:
+                    raise
+        if isinstance(key, slice):
             start= key.start or 0
-            stop= key.stop or l-1
+            stop= key.stop or lt()-1
             step= key.step or 1
-            if start < 0: start+= l
-            if stop < 0: stop+= l
+            if start < 0: start+= lt()
+            if stop < 0: stop+= lt()
             if step==1:
                 return self.subMat(start, 0, stop-start, self.width)
-            raise NotImplementedError, 'slice step != 1'
-        raise TypeError, "key should be an int or a slice"
+            raise NotImplementedError('slice step != 1')
+        raise TypeError("key should be an int or a slice")
     
 class RealRange:
     """



From saintmlx at mail.berlios.de  Thu Mar  5 20:06:02 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 5 Mar 2009 20:06:02 +0100
Subject: [Plearn-commits] r9988 - trunk/plearn/python
Message-ID: <200903051906.n25J62re030295@sheep.berlios.de>

Author: saintmlx
Date: 2009-03-05 20:06:02 +0100 (Thu, 05 Mar 2009)
New Revision: 9988

Modified:
   trunk/plearn/python/PythonExtension.cc
Log:
- use PLearnError python exception for function calls, not just method calls



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2009-03-05 18:41:43 UTC (rev 9987)
+++ trunk/plearn/python/PythonExtension.cc	2009-03-05 19:06:02 UTC (rev 9988)
@@ -69,7 +69,7 @@
     }
     catch(const PLearnError& e) 
     {
-        PyErr_SetString(PyExc_Exception, e.message().c_str());
+        PyErr_SetString(the_PLearn_python_exception, e.message().c_str());
         return 0;
     }
     catch(const std::exception& e) 



From nouiz at mail.berlios.de  Fri Mar  6 15:33:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Mar 2009 15:33:12 +0100
Subject: [Plearn-commits] r9989 - trunk/plearn/vmat
Message-ID: <200903061433.n26EXCx0008943@sheep.berlios.de>

Author: nouiz
Date: 2009-03-06 15:33:11 +0100 (Fri, 06 Mar 2009)
New Revision: 9989

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
more information in warning and print multiple log message in at once. 


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-05 19:06:02 UTC (rev 9988)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-06 14:33:11 UTC (rev 9989)
@@ -256,6 +256,7 @@
     if(partial_match)
     {
         TVec< pair<string, string> > new_fieldspec;
+        TVec<string> no_expended_fields;
         PLCHECK_MSG(reorder_fieldspec_from_headers,
                     "In TextFilesVMatrix::setColumnNamesAndWidth - "
                     "when partial_match is true, reorder_fieldspec_from_headers"
@@ -283,9 +284,16 @@
                 }
             }
             if(!expended)
-                NORMAL_LOG<<"In TextFilesVMatrix::setColumnNamesAndWidth - "
-                    "Don't have find any partial match to "<<fieldspec[i].first;
+                no_expended_fields.append(fieldspec[i].first);
         }
+        if(no_expended_fields.length()>0){
+            NORMAL_LOG<<"In TextFilesVMatrix::setColumnNamesAndWidth - "
+                      <<"Don't have find any partial match for:";
+            for(int i=0;i<no_expended_fields.length();i++)
+                NORMAL_LOG<<" "<<fieldspec[i].first;
+            NORMAL_LOG<<endl;
+        }
+            
         fieldspec = new_fieldspec;
     }
 
@@ -330,14 +338,14 @@
 
         if(not_used_fs.size()!=0)
             PLWARNING("TextFilesVMatrix::setColumnNamesAndWidth() - "
-                      "Fieldspecs exists for field(s) that are not in the source: %s\n"
+                      "%d fieldspecs exists for field(s) that are not in the source: %s\n"
                       "They will be skipped.",
-                      tostring(not_used_fs).c_str());
+                      not_used_fs.length(), tostring(not_used_fs).c_str());
         if(not_used_fn.size()!=0)
             PLWARNING("TextFilesVMatrix::setColumnNamesAndWidth() - "
-                      "Fieldnames in source that don't have fieldspec: %s\n"
+                      "%d fieldnames in source that don't have fieldspec: %s\n"
                       "They will be skipped.",
-                      tostring(not_used_fn).c_str());
+                      not_used_fn.length(), tostring(not_used_fn).c_str());
     
 
         //the new order for fieldspecs



From nouiz at mail.berlios.de  Fri Mar  6 15:41:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Mar 2009 15:41:26 +0100
Subject: [Plearn-commits] r9990 - trunk/plearn/vmat
Message-ID: <200903061441.n26EfQqT009404@sheep.berlios.de>

Author: nouiz
Date: 2009-03-06 15:41:26 +0100 (Fri, 06 Mar 2009)
New Revision: 9990

Modified:
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
Log:
Print one warning that contain all the information of many previous warning.


Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2009-03-06 14:33:11 UTC (rev 9989)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2009-03-06 14:41:26 UTC (rev 9990)
@@ -165,7 +165,7 @@
                   "we suppose that the default instruction apply only to input fields");
         skip_instruction_input = source->width() - missing_instructions.size();
     }
-    int missing_field = 0;
+    TVec< pair<string, string> > not_used_spec;
     for (int ins_col = 0; ins_col < missing_instructions.size(); ins_col++)
     {
         int source_col = 0;
@@ -189,12 +189,7 @@
         {
             if(missing_instructions[ins_col].second!="skip"){
                 //if the instruction is skip, we don't care that it is missing in the source!
-                PLWARNING("In MissingInstructionVMatrix::build_() -"
-                          " missing_instructions '%d': no field with this name: '%s'."
-                          " It have '%s' as spec"
-                          ,ins_col,(missing_instructions[ins_col].first).c_str(),
-                          (missing_instructions[ins_col].second).c_str());
-                missing_field++;
+                not_used_spec.append(missing_instructions[ins_col]);
             }
             continue;
         }
@@ -230,6 +225,13 @@
             else skip_instruction_extra++;
         }
     }
+    if(not_used_spec.length()>0){
+        TVec< pair<string, string> >  m;
+        PLWARNING("In MissingInstructionVMatrix::build_() -"
+                  " There is %d instructions where the field are not in the source: %s"
+                  ,not_used_spec.length(),tostring(not_used_spec).c_str());
+
+    }
     setMetaInfoFromSource();
     defineSizes(source->inputsize() - skip_instruction_input,
                 source->targetsize() - skip_instruction_target,
@@ -252,10 +254,10 @@
                 " Their have been %d field in the source"
                 " matrix that have no instruction. Do you want"
                 " to set the default_instruction option?",missing_instruction);
-    if(missing_field && missing_field_error)
+    if(not_used_spec.length()>0 && missing_field_error)
         PLERROR("In MissingInstructionVMatrix::build_ - Their have been %d"
                 " instruction that have no correcponding field in the"
-                " source matrix",missing_field);
+                " source matrix",not_used_spec.length());
 
     // Copy the appropriate VMFields
     fieldinfos.resize(width());



From nouiz at mail.berlios.de  Fri Mar  6 16:25:48 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Mar 2009 16:25:48 +0100
Subject: [Plearn-commits] r9991 - trunk/plearn/vmat
Message-ID: <200903061525.n26FPmlE012984@sheep.berlios.de>

Author: nouiz
Date: 2009-03-06 16:25:48 +0100 (Fri, 06 Mar 2009)
New Revision: 9991

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
more informative warning and more compact warning.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2009-03-06 14:41:26 UTC (rev 9990)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2009-03-06 15:25:48 UTC (rev 9991)
@@ -214,8 +214,9 @@
       else if (variable_imputation_instruction[source_col] == 4)
 	;//do nothing
       else if (variable_imputation_instruction[source_col] == 5)
-	return PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d) - the value is"
-		       " missing and have a instruction err!",i);
+	return PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d) -"
+		       " the value is missing for column %s"
+		       " and have a instruction err!",i, fieldName(source_col).c_str());
       else
 	PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d, Vec) - "
 		"unknow variable_imputation_instruction value of %d",i,
@@ -297,7 +298,7 @@
     //We sho
     TVec<pair<string,string> > save_imputation_spec = imputation_spec;
     imputation_spec = save_imputation_spec.copy();
-
+    TVec<string> not_expanded;
     for (int spec_col = 0; spec_col < imputation_spec.size(); spec_col++)
     {
         int train_col;
@@ -328,14 +329,10 @@
 	    }
 	  }
 	  if(!expended){
-	    PLWARN_ERR(!missing_field_error,
-		       "In MeanMedianModeImputationVMatrix::build_() - "
-		       "Didn't found partial match for '%s'",
-		       imputation_spec[spec_col].first.c_str());
+	    not_expanded.append(imputation_spec[spec_col].first);
 	  }
 	  continue;
 	}
-	
         if (imputation_spec[spec_col].second == "mean") variable_imputation_instruction[train_col] = 1;
         else if (imputation_spec[spec_col].second == "median") variable_imputation_instruction[train_col] = 2;
         else if (imputation_spec[spec_col].second == "mode") variable_imputation_instruction[train_col] = 3;
@@ -345,6 +342,14 @@
 	  PLERROR("In MeanMedianModeImputationVMatrix: unsupported imputation instruction: %s : %s",
 		     (imputation_spec[spec_col].first).c_str(), (imputation_spec[spec_col].second).c_str());
     }
+    if(not_expanded.length()>0){
+      PLWARN_ERR(!missing_field_error,
+		 "In MeanMedianModeImputationVMatrix::build_() - "
+		 "For %d spec, we didn't found partial match '%s'",
+		 not_expanded.length(), tostring(not_expanded).c_str());
+    }
+	
+
     imputation_spec = save_imputation_spec;
 
     if(nofields.length()>0)



From saintmlx at mail.berlios.de  Fri Mar  6 17:35:13 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 6 Mar 2009 17:35:13 +0100
Subject: [Plearn-commits] r9992 - in trunk: plearn/python
	python_modules/plearn/pyplearn
Message-ID: <200903061635.n26GZDGx017516@sheep.berlios.de>

Author: saintmlx
Date: 2009-03-06 17:35:12 +0100 (Fri, 06 Mar 2009)
New Revision: 9992

Modified:
   trunk/plearn/python/PythonExtension.cc
   trunk/python_modules/plearn/pyplearn/PyPLearnObject.py
Log:
- don't pass params to object.__new__ or object.__init__ (deprecated in python2.6)



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2009-03-06 15:25:48 UTC (rev 9991)
+++ trunk/plearn/python/PythonExtension.cc	2009-03-06 16:35:12 UTC (rev 9992)
@@ -277,7 +277,7 @@
             "    #get_plearn_module().loggingControl(500, ['__ALL__'])"
             "    #print '** "+pyclassname+".__new__',args,kwargs\n"
             "    #import sys; sys.stdout.flush()\n"
-            "    obj= object.__new__(cls,*args,**kwargs)\n"
+            "    obj= object.__new__(cls)\n"
             "    if '_cptr' not in kwargs:\n"
             "      obj._cptr= cls._newCPPObj('"+classname+"')\n"
             "      cls._refCPPObj(obj)\n"

Modified: trunk/python_modules/plearn/pyplearn/PyPLearnObject.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/PyPLearnObject.py	2009-03-06 15:25:48 UTC (rev 9991)
+++ trunk/python_modules/plearn/pyplearn/PyPLearnObject.py	2009-03-06 16:35:12 UTC (rev 9992)
@@ -239,7 +239,7 @@
             return cls
         
         def __init__(cls, name, bases, dict):
-            super(type, cls).__init__(name, bases, dict) 
+            super(type, cls).__init__()
             for rop_name in cls._rop_names:
                 lop_name = rop_name[0:2] + rop_name[3:]
                 if ( not dict.has_key(rop_name)



From chrish at mail.berlios.de  Fri Mar  6 19:06:27 2009
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Fri, 6 Mar 2009 19:06:27 +0100
Subject: [Plearn-commits] r9993 - trunk/python_modules/plearn/pymake
Message-ID: <200903061806.n26I6RYV018938@sheep.berlios.de>

Author: chrish
Date: 2009-03-06 19:06:25 +0100 (Fri, 06 Mar 2009)
New Revision: 9993

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Fix exception when running pymake -clean without PYMAKE_OPTIONS environment variable set.

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-03-06 16:35:12 UTC (rev 9992)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-03-06 18:06:25 UTC (rev 9993)
@@ -2874,9 +2874,11 @@
     if 'clean' in optionargs:
         #some people put options in their PYMAKE_OPTIONS env variable and
         #we want them to be able to do pymake -clean .
-        for i in os.getenv('PYMAKE_OPTIONS').split():
-            i=i[1:]
-            if i in optionargs: optionargs.remove(i)
+        env_options = os.getenv('PYMAKE_OPTIONS')
+        if env_options:
+            for i in env_options.split():
+                i=i[1:]
+                if i in optionargs: optionargs.remove(i)
                         
         if len(optionargs)!=1 or len(otherargs)==0:
             print 'BAD ARGUMENTS: with -clean, specify one or more directories to clean, but no other -option:', optionargs



From nouiz at mail.berlios.de  Fri Mar  6 21:11:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 6 Mar 2009 21:11:26 +0100
Subject: [Plearn-commits] r9994 - trunk/scripts
Message-ID: <200903062011.n26KBQ2C008898@sheep.berlios.de>

Author: nouiz
Date: 2009-03-06 21:11:25 +0100 (Fri, 06 Mar 2009)
New Revision: 9994

Modified:
   trunk/scripts/dbidispatch
Log:
correc the help message and remove useless warning.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-03-06 18:06:25 UTC (rev 9993)
+++ trunk/scripts/dbidispatch	2009-03-06 20:11:25 UTC (rev 9994)
@@ -5,7 +5,7 @@
 from subprocess import Popen,PIPE
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [--only_n_first=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart_jobs condor_jobs_number... }
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [--only_n_first=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart condor_jobs_number... }
 
 <back-end parameter>:
     bqtools, cluster, condor option  : [--mem=N]
@@ -75,7 +75,7 @@
     option for dbidispatch. They can be overrided on the command line.
   The 'DBIDISPATCH_LOGDIR' environnement variable set the name of the directory
     where all the individual logs directory will be put. Default to LOGS.
-  The '--[*no_]restart_jobs' option work only for condor. The parameter 
+  The '--[*no_]restart' option work only for condor. The parameter 
     following this option should be condor jobs number. We will parse the
     history on the local host and relaunch those jobs. We only take the command
     line that was executed, all other options are are those passed to 
@@ -380,7 +380,7 @@
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
                        "universe", "machine", "machines", "no_machine","to_all",
-                       "keep_failed_jobs_in_queue", "tasks_filename"]
+                       "keep_failed_jobs_in_queue", "tasks_filename", "restart"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
                        "nano", "queue", "raw", "submit_options", "jobs_name",



From tihocan at mail.berlios.de  Mon Mar  9 15:11:50 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 9 Mar 2009 15:11:50 +0100
Subject: [Plearn-commits] r9995 - trunk/commands/PLearnCommands
Message-ID: <200903091411.n29EBomJ029909@sheep.berlios.de>

Author: tihocan
Date: 2009-03-09 15:11:49 +0100 (Mon, 09 Mar 2009)
New Revision: 9995

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
Log:
Cosmetic changes in help

Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2009-03-06 20:11:25 UTC (rev 9994)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2009-03-09 14:11:49 UTC (rev 9995)
@@ -65,9 +65,9 @@
         "To get help on datasets:                            " + prgname() + " help datasets \n\n" 
         ""
         "Global parameter:\n"
-        "                 --no-progress: don't print progress bar\n"
+        "                 --no-progress: do not print progress bar\n"
         "                 --no-version: do not print the version \n"
-        "                 --windows_endl: use windows end of line\n"
+        "                 --windows_endl: use Windows end of line\n"
         "                 --profile: print some profiling information\n"
         "                     Must have been compiled with '-logging=dbg-profile'\n"
         "                 --verbosity LEVEL: The level of log to print.\n"



From nouiz at mail.berlios.de  Mon Mar  9 18:24:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 9 Mar 2009 18:24:45 +0100
Subject: [Plearn-commits] r9996 - trunk/plearn_learners/meta
Message-ID: <200903091724.n29HOjBb018074@sheep.berlios.de>

Author: nouiz
Date: 2009-03-09 18:24:44 +0100 (Mon, 09 Mar 2009)
New Revision: 9996

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
added the option MultiClassAdaBoost::warn_once_target_gt_2 to hide multiple copy of one warninng.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-09 14:11:49 UTC (rev 9995)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-09 17:24:44 UTC (rev 9996)
@@ -63,6 +63,8 @@
     test_time(0),
     total_test_time(0),
     time_costs(true),
+    warn_once_target_gt_2(false),
+    done_warn_once_target_gt_2(false),
     timer(new PTimer()),
     time_sum(0),
     time_sum_rtr(0),
@@ -147,6 +149,16 @@
                   &MultiClassAdaBoost::time_costs, OptionBase::buildoption,
                   "If true, generate the time costs. Else they are nan.");
 
+    declareOption(ol, "warn_once_target_gt_2",
+                  &MultiClassAdaBoost::warn_once_target_gt_2,
+                  OptionBase::buildoption,
+                  "If true, generate only one warning if we find target > 2.");
+
+    declareOption(ol, "done_warn_once_target_gt_2",
+                  &MultiClassAdaBoost::done_warn_once_target_gt_2,
+                  OptionBase::learntoption,
+                  "Used to keep track if we have done the warning or not.");
+
     declareOption(ol, "time_sum",
                   &MultiClassAdaBoost::time_sum, 
                   OptionBase::learntoption|OptionBase::nosave,
@@ -530,7 +542,7 @@
 }
 
 void MultiClassAdaBoost::getSubLearnerTarget(const Vec target,
-                                             TVec<Vec> sub_target) 
+                                             TVec<Vec> sub_target) const
 {
     if(fast_is_equal(target[0],0.)){
         sub_target[0][0]=0;
@@ -542,9 +554,14 @@
         sub_target[0][0]=1;
         sub_target[1][0]=1;
     }else if(target[0]>2){
-        PLWARNING("In MultiClassAdaBoost::getSubLearnerTarget - "
-                  "We only support target 0/1/2. We got %f. We transform "
-                  "it to a target of 2.", target[0]);
+        if(!warn_once_target_gt_2 || ! done_warn_once_target_gt_2){
+            PLWARNING("In MultiClassAdaBoost::getSubLearnerTarget - "
+                      "We only support target 0/1/2. We got %f. We transform "
+                      "it to a target of 2.", target[0]);
+            done_warn_once_target_gt_2=true;
+            if(warn_once_target_gt_2)
+                PLWARNING("We will show this warning only once.");
+        }
         sub_target[0][0]=1;
         sub_target[1][0]=1;
     }else{

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-03-09 14:11:49 UTC (rev 9995)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-03-09 17:24:44 UTC (rev 9996)
@@ -85,6 +85,8 @@
     real total_test_time;
 
     bool time_costs;
+    bool warn_once_target_gt_2;
+    mutable bool done_warn_once_target_gt_2;
 
     PP<PTimer> timer;
 
@@ -124,7 +126,6 @@
 
     //! Returns the size of this learner's output, (which typically
     //! may depend on its inputsize(), targetsize() and set options).
-    // (PLEASE IMPLEMENT IN .cc)
     virtual int outputsize() const;
 
     virtual void finalize();
@@ -132,21 +133,17 @@
     //! (Re-)initializes the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!).
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void forget();
 
     //! The role of the train method is to bring the learner up to
     //! stage==nstages, updating the train_stats collector with training costs
     //! measured on-line in the process.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void train();
 
     //! Computes the output from the input.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
     //! Computes the costs from already computed output.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
                                          const Vec& target, Vec& costs) const;
 
@@ -157,19 +154,14 @@
 
     //! Returns the names of the costs computed by computeCostsFromOutpus (and
     //! thus the test method).
-    // (PLEASE IMPLEMENT IN .cc)
     virtual TVec<std::string> getTestCostNames() const;
 
     //! Returns the names of the objective costs that the train method computes
     //! and for which it updates the VecStatsCollector train_stats.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual TVec<std::string> getTrainCostNames() const;
 
 
     // *** SUBCLASS WRITING: ***
-    // While in general not necessary, in case of particular needs
-    // (efficiency concerns for ex) you may also want to overload
-    // some of the following methods:
     virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
                                        Vec& output, Vec& costs) const;
     // virtual void computeCostsOnly(const Vec& input, const Vec& target,
@@ -185,15 +177,12 @@
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
     PLEARN_DECLARE_OBJECT(MultiClassAdaBoost);
 
     // Simply calls inherited::build() then build_()
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 
@@ -203,26 +192,23 @@
     //#####  Protected Options  ###############################################
 
     // ### Declare protected option fields (such as learned parameters) here
-    // ...
 
 protected:
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.
-    // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList& ol);
 
 private:
     //#####  Private Member Functions  ########################################
 
     //! This does the actual building.
-    // (PLEASE IMPLEMENT IN .cc)
     void build_();
 
-    static void getSubLearnerTarget(const Vec target, TVec<Vec> sub_target);
+    void getSubLearnerTarget(const Vec target, TVec<Vec> sub_target)const;
 private:
     //#####  Private Data Members  ############################################
-    TVec<Vec> sub_target_tmp;
+    mutable TVec<Vec> sub_target_tmp;
 
     string targetname;
     string input_prg;



From nouiz at mail.berlios.de  Mon Mar  9 18:30:57 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 9 Mar 2009 18:30:57 +0100
Subject: [Plearn-commits] r9997 - trunk/plearn_learners/regressors
Message-ID: <200903091730.n29HUvML020109@sheep.berlios.de>

Author: nouiz
Date: 2009-03-09 18:30:56 +0100 (Mon, 09 Mar 2009)
New Revision: 9997

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
bugfix warning.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-09 17:24:44 UTC (rev 9996)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-09 17:30:56 UTC (rev 9997)
@@ -331,7 +331,7 @@
             PLWARNING("In RegressionTreeRegisters::sortEachDim(%d) - "
                       "sort is not stable. make it stable to be more optimized:"
                       " reg1=%d, reg2=%d, v1=%f, v2=%f", 
-                      reg1, reg2, v1, v2);
+                      dim, reg1, reg2, v1, v2);
     }
 #endif
     return;



From tihocan at mail.berlios.de  Mon Mar  9 18:53:21 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 9 Mar 2009 18:53:21 +0100
Subject: [Plearn-commits] r9998 - trunk/plearn_learners/generic
Message-ID: <200903091753.n29HrLc8006085@sheep.berlios.de>

Author: tihocan
Date: 2009-03-09 18:53:10 +0100 (Mon, 09 Mar 2009)
New Revision: 9998

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Minor typo fix in help

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2009-03-09 17:30:56 UTC (rev 9997)
+++ trunk/plearn_learners/generic/PLearner.cc	2009-03-09 17:53:10 UTC (rev 9998)
@@ -370,8 +370,8 @@
         (BodyDoc("Test on a given testset and return stats, outputs and costs."),
          ArgDoc("testset","test set"),
          ArgDoc("test_stats","VecStatsCollector to use"),
-         ArgDoc("rtestoutputs","wether to return outputs"),
-         ArgDoc("rtestcosts","wether to return costs"),
+         ArgDoc("rtestoutputs","whether to return outputs"),
+         ArgDoc("rtestcosts","whether to return costs"),
          RetDoc ("tuple of (stats, outputs, costs)")));
 
 



From nouiz at mail.berlios.de  Mon Mar  9 20:29:30 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 9 Mar 2009 20:29:30 +0100
Subject: [Plearn-commits] r9999 - trunk/plearn/vmat
Message-ID: <200903091929.n29JTUiR016475@sheep.berlios.de>

Author: nouiz
Date: 2009-03-09 20:29:30 +0100 (Mon, 09 Mar 2009)
New Revision: 9999

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
make the fct VMatrix::saveFieldInfos() more robust. When we read the old file to check if we need to write it, if their was an error, we received an PLERROR. We just ignore the error and write the file again as it was corrupted.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2009-03-09 17:53:10 UTC (rev 9998)
+++ trunk/plearn/vmat/VMatrix.cc	2009-03-09 19:29:30 UTC (rev 9999)
@@ -814,7 +814,10 @@
 {
     // check if we need to save the fieldinfos
     if(fieldinfos.size() > 0) {
-        Array<VMField> current_fieldinfos = getSavedFieldInfos();
+        Array<VMField> current_fieldinfos;
+        try{
+            current_fieldinfos = getSavedFieldInfos();
+        }catch(PLearnError){}
         if (current_fieldinfos != fieldinfos) {
 
             // Ensure that the metadatadir exists



From tihocan at mail.berlios.de  Tue Mar 10 15:33:29 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 10 Mar 2009 15:33:29 +0100
Subject: [Plearn-commits] r10000 - trunk/plearn/math
Message-ID: <200903101433.n2AEXT44012402@sheep.berlios.de>

Author: tihocan
Date: 2009-03-10 15:33:28 +0100 (Tue, 10 Mar 2009)
New Revision: 10000

Modified:
   trunk/plearn/math/TMat_impl.h
Log:
Minor optimizations

Modified: trunk/plearn/math/TMat_impl.h
===================================================================
--- trunk/plearn/math/TMat_impl.h	2009-03-09 19:29:30 UTC (rev 9999)
+++ trunk/plearn/math/TMat_impl.h	2009-03-10 14:33:28 UTC (rev 10000)
@@ -245,12 +245,14 @@
       const TVec<T>& m_values;
       index_missing_cmp(const Vec& values): m_values(values) { }
       bool operator()(int x, int y) {
-          T v1=m_values[x];
-          T v2=m_values[y];
-          if(is_missing(v1) && is_missing(v2)) return false;
-          else if(is_missing(v1)) return false;
-          else if(is_missing(v2)) return true;
-          return v1 < v2;
+          const T& v1 = m_values[x];
+          const T& v2 = m_values[y];
+          if (is_missing(v1))
+              return false;
+          else if (is_missing(v2))
+              return true;
+          else
+              return v1 < v2;
       }
   };
 }



From tihocan at mail.berlios.de  Tue Mar 10 15:34:31 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 10 Mar 2009 15:34:31 +0100
Subject: [Plearn-commits] r10001 - trunk/plearn/vmat
Message-ID: <200903101434.n2AEYVUb013794@sheep.berlios.de>

Author: tihocan
Date: 2009-03-10 15:34:31 +0100 (Tue, 10 Mar 2009)
New Revision: 10001

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
Typo fix in log output

Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-10 14:33:28 UTC (rev 10000)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-10 14:34:31 UTC (rev 10001)
@@ -288,7 +288,7 @@
         }
         if(no_expended_fields.length()>0){
             NORMAL_LOG<<"In TextFilesVMatrix::setColumnNamesAndWidth - "
-                      <<"Don't have find any partial match for:";
+                      <<"Did not find any partial match for:";
             for(int i=0;i<no_expended_fields.length();i++)
                 NORMAL_LOG<<" "<<fieldspec[i].first;
             NORMAL_LOG<<endl;



From tihocan at mail.berlios.de  Tue Mar 10 15:34:55 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 10 Mar 2009 15:34:55 +0100
Subject: [Plearn-commits] r10002 - trunk/plearn/vmat
Message-ID: <200903101434.n2AEYtdY013986@sheep.berlios.de>

Author: tihocan
Date: 2009-03-10 15:34:55 +0100 (Tue, 10 Mar 2009)
New Revision: 10002

Modified:
   trunk/plearn/vmat/SubVMatrix.cc
Log:
Automatically created metadata directory now ends with .metadata for the sake of consistency

Modified: trunk/plearn/vmat/SubVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SubVMatrix.cc	2009-03-10 14:34:31 UTC (rev 10001)
+++ trunk/plearn/vmat/SubVMatrix.cc	2009-03-10 14:34:55 UTC (rev 10002)
@@ -247,7 +247,8 @@
                        "_istart=" + tostring(istart) + 
                        "_jstart=" +tostring(jstart) + 
                        "_length="+tostring(length()) + 
-                       "_width="+tostring(width()));
+                       "_width="+tostring(width()) +
+                       ".metadata");
 
     //  cerr << "inputsize: "<<inputsize_ << "  targetsize:"<<targetsize_<<"weightsize:"<<weightsize_<<endl;
 }



From tihocan at mail.berlios.de  Tue Mar 10 18:06:50 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 10 Mar 2009 18:06:50 +0100
Subject: [Plearn-commits] r10003 - trunk/plearn/vmat
Message-ID: <200903101706.n2AH6oOD023495@sheep.berlios.de>

Author: tihocan
Date: 2009-03-10 18:06:50 +0100 (Tue, 10 Mar 2009)
New Revision: 10003

Modified:
   trunk/plearn/vmat/DiskVMatrix.cc
   trunk/plearn/vmat/DiskVMatrix.h
Log:
Proper build mechanism in convenience constructor

Modified: trunk/plearn/vmat/DiskVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DiskVMatrix.cc	2009-03-10 14:34:55 UTC (rev 10002)
+++ trunk/plearn/vmat/DiskVMatrix.cc	2009-03-10 17:06:50 UTC (rev 10003)
@@ -118,8 +118,9 @@
     writable = false;
 }
 
-DiskVMatrix::DiskVMatrix(const PPath& the_dirname, bool readwrite):
-    inherited   (true),
+DiskVMatrix::DiskVMatrix(const PPath& the_dirname, bool readwrite,
+                         bool call_build_):
+    inherited   (call_build_),
     indexf      (0),
     freshnewfile(false),
     old_format  (false),
@@ -130,7 +131,8 @@
 {
     writable = readwrite;
     dirname.removeTrailingSlash(); // For safety.
-    build_();
+    if (call_build_)
+        build_();
 }
 
 DiskVMatrix::DiskVMatrix(const PPath& the_dirname, int the_width,

Modified: trunk/plearn/vmat/DiskVMatrix.h
===================================================================
--- trunk/plearn/vmat/DiskVMatrix.h	2009-03-10 14:34:55 UTC (rev 10002)
+++ trunk/plearn/vmat/DiskVMatrix.h	2009-03-10 17:06:50 UTC (rev 10003)
@@ -81,7 +81,8 @@
   If readwrite is false (the default), then the files are opened in read only mode, and calling appendRow
   will issue an error.
 */
-    DiskVMatrix(const PPath& the_dirname, bool readwrite=false);
+    DiskVMatrix(const PPath& the_dirname, bool readwrite=false,
+                bool call_build_ = true);
 
 /*!     Create a new one.
   If directory already exist an error is issued



From tihocan at mail.berlios.de  Tue Mar 10 18:07:47 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 10 Mar 2009 18:07:47 +0100
Subject: [Plearn-commits] r10004 - trunk/plearn/vmat
Message-ID: <200903101707.n2AH7luP023732@sheep.berlios.de>

Author: tihocan
Date: 2009-03-10 18:07:47 +0100 (Tue, 10 Mar 2009)
New Revision: 10004

Modified:
   trunk/plearn/vmat/TemporaryDiskVMatrix.cc
   trunk/plearn/vmat/TemporaryDiskVMatrix.h
Log:
Proper build mechanism in convenience constructor

Modified: trunk/plearn/vmat/TemporaryDiskVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TemporaryDiskVMatrix.cc	2009-03-10 17:06:50 UTC (rev 10003)
+++ trunk/plearn/vmat/TemporaryDiskVMatrix.cc	2009-03-10 17:07:47 UTC (rev 10004)
@@ -61,10 +61,11 @@
 {}
 
 TemporaryDiskVMatrix::TemporaryDiskVMatrix(const PPath& filename,
-                                           bool writable):
-    inherited(filename, writable)
+                                           bool writable, bool call_build_):
+    inherited(filename, writable, call_build_)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
 ////////////////////

Modified: trunk/plearn/vmat/TemporaryDiskVMatrix.h
===================================================================
--- trunk/plearn/vmat/TemporaryDiskVMatrix.h	2009-03-10 17:06:50 UTC (rev 10003)
+++ trunk/plearn/vmat/TemporaryDiskVMatrix.h	2009-03-10 17:07:47 UTC (rev 10004)
@@ -73,7 +73,8 @@
     TemporaryDiskVMatrix();
 
     //! Convenient constructor.
-    TemporaryDiskVMatrix(const PPath& filename, bool writable = true);
+    TemporaryDiskVMatrix(const PPath& filename, bool writable = true,
+                         bool call_build_ = true);
 
     //! Destructor.
     virtual ~TemporaryDiskVMatrix();



From tihocan at mail.berlios.de  Tue Mar 10 18:09:49 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 10 Mar 2009 18:09:49 +0100
Subject: [Plearn-commits] r10005 - trunk/plearn/vmat
Message-ID: <200903101709.n2AH9nov024032@sheep.berlios.de>

Author: tihocan
Date: 2009-03-10 18:09:49 +0100 (Tue, 10 Mar 2009)
New Revision: 10005

Modified:
   trunk/plearn/vmat/PrecomputedVMatrix.cc
Log:
The reloaded VMat should not be writable

Modified: trunk/plearn/vmat/PrecomputedVMatrix.cc
===================================================================
--- trunk/plearn/vmat/PrecomputedVMatrix.cc	2009-03-10 17:07:47 UTC (rev 10004)
+++ trunk/plearn/vmat/PrecomputedVMatrix.cc	2009-03-10 17:09:49 UTC (rev 10005)
@@ -136,7 +136,8 @@
 
         if ( isdir(dmatdir) )
         {
-            precomp_source = temporary ? new TemporaryDiskVMatrix(dmatdir)
+            precomp_source = temporary ? new TemporaryDiskVMatrix(dmatdir,
+                                                                  false)
                                        : new DiskVMatrix(dmatdir);
             if(isUpToDate(precomp_source))
                 recompute = false;
@@ -146,7 +147,8 @@
         {
             force_rmdir(dmatdir);
             source->saveDMAT(dmatdir);
-            precomp_source = temporary ? new TemporaryDiskVMatrix(dmatdir)
+            precomp_source = temporary ? new TemporaryDiskVMatrix(dmatdir,
+                                                                  false)
                                        : new DiskVMatrix(dmatdir);
         }
         length_ = precomp_source->length();



From tihocan at mail.berlios.de  Tue Mar 10 18:15:05 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 10 Mar 2009 18:15:05 +0100
Subject: [Plearn-commits] r10006 - in trunk/examples/data/test_suite: .
	linear_4x_2y.pmat.metadata
Message-ID: <200903101715.n2AHF5nq025281@sheep.berlios.de>

Author: tihocan
Date: 2009-03-10 18:15:05 +0100 (Tue, 10 Mar 2009)
New Revision: 10006

Modified:
   trunk/examples/data/test_suite/
   trunk/examples/data/test_suite/linear_4x_2y.pmat.metadata/
Log:
Ignoring metadata directories that are automatically generated during tests


Property changes on: trunk/examples/data/test_suite
___________________________________________________________________
Name: svn:ignore
   - gauss_1D_100pt.amat.metadata
linear_2x_2y.amat.metadata
linear_4x_2y.amat.metadata
sin_signcos_1x_2y.amat.metadata
multi_gaussian_data.amat.metadata
data_with_strings.amat.metadata

   + gauss_1D_100pt.amat.metadata
linear_2x_2y.amat.metadata
linear_4x_2y.amat.metadata
linear_4x_2y_binary_class.vmat.metadata
linear_4x_2y_multi_class.vmat.metadata
linear_4x_2y_multi_class_3.vmat.metadata
sin_signcos_1x_2y.amat.metadata
multi_gaussian_data.amat.metadata
data_with_strings.amat.metadata



Property changes on: trunk/examples/data/test_suite/linear_4x_2y.pmat.metadata
___________________________________________________________________
Name: svn:ignore
   + fieldnames
SubVMatrix_istart=0_jstart=0_length=150_width=5.metadata




From nouiz at mail.berlios.de  Tue Mar 10 18:59:07 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 10 Mar 2009 18:59:07 +0100
Subject: [Plearn-commits] r10007 - trunk/plearn/base
Message-ID: <200903101759.n2AHx70p024048@sheep.berlios.de>

Author: nouiz
Date: 2009-03-10 18:59:04 +0100 (Tue, 10 Mar 2009)
New Revision: 10007

Modified:
   trunk/plearn/base/stringutils.cc
Log:
bugfix.


Modified: trunk/plearn/base/stringutils.cc
===================================================================
--- trunk/plearn/base/stringutils.cc	2009-03-10 17:15:05 UTC (rev 10006)
+++ trunk/plearn/base/stringutils.cc	2009-03-10 17:59:04 UTC (rev 10007)
@@ -407,7 +407,7 @@
         bool bw=f[0]==double_quote;
         bool ew=f[f.size()-1]==double_quote;
         if(bw && ew){
-            ret2.push_back(f.substr(1,f.size()-1)); 
+            ret2.push_back(f.substr(1,f.size()-2)); 
         }else if(bw){
             string tmp=f.substr(1);
             tmp+=delimiter;



From nouiz at mail.berlios.de  Tue Mar 10 19:00:27 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 10 Mar 2009 19:00:27 +0100
Subject: [Plearn-commits] r10008 - trunk/plearn/vmat
Message-ID: <200903101800.n2AI0Rro026003@sheep.berlios.de>

Author: nouiz
Date: 2009-03-10 19:00:26 +0100 (Tue, 10 Mar 2009)
New Revision: 10008

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
-bugfix in case their is a PLERROR while building the object. (The destructor was not working generating another error...)
-Made a check before.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-10 17:59:04 UTC (rev 10007)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-10 18:00:26 UTC (rev 10008)
@@ -388,6 +388,8 @@
 
 void TextFilesVMatrix::build_()
 {
+    if (!default_spec.empty() && !reorder_fieldspec_from_headers)
+        PLERROR("In TextFilesVMatrix::build_() when the option default_spec is used, reorder_fieldspec_from_headers must be true");
     if (metadatapath != "") {
         PLWARNING("In TextFilesVMatrix::build_() metadatapath option is deprecated. "
                   "You should use metadatadir instead.\n");
@@ -395,8 +397,6 @@
         metadatadir = metadatapath;
         setMetaDataDir(metadatapath);
     }
-    if (!default_spec.empty() && !reorder_fieldspec_from_headers)
-        PLERROR("In TextFilesVMatrix::build_() when the option default_spec is used, reorder_fieldspec_from_headers must be true");
 }
 ////////////////////
 // setMetaDataDir //
@@ -1019,7 +1019,7 @@
     for(int k=0; k<txtfiles.length(); k++)
         fclose(txtfiles[k]);
 
-    for(int k=0; k<fieldspec.size(); k++)
+    for(int k=0; k<mapfiles.size(); k++)
     {
         if(mapfiles[k])
             fclose(mapfiles[k]);



From nouiz at mail.berlios.de  Wed Mar 11 16:28:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Mar 2009 16:28:58 +0100
Subject: [Plearn-commits] r10009 - trunk/plearn/vmat
Message-ID: <200903111528.n2BFSwAP017794@sheep.berlios.de>

Author: nouiz
Date: 2009-03-11 16:28:58 +0100 (Wed, 11 Mar 2009)
New Revision: 10009

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.h
Log:
put a method as public as the default VMatrix have it public.


Modified: trunk/plearn/vmat/TextFilesVMatrix.h
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.h	2009-03-10 18:00:26 UTC (rev 10008)
+++ trunk/plearn/vmat/TextFilesVMatrix.h	2009-03-11 15:28:58 UTC (rev 10009)
@@ -177,7 +177,6 @@
 
     //! Return true iff 'ftype' is a valid type that does not need to be skipped.
     virtual bool isValidNonSkipFieldType(const string& ftype) const;
-    virtual void setMetaDataDir(const PPath& the_metadatadir);
 
 public:
 
@@ -212,6 +211,7 @@
     //! Returns the index in a split text row of the given named text field
     //! (this is the position of that named field in fieldspec )
     int getIndexOfTextField(const string& fieldname) const;
+    virtual void setMetaDataDir(const PPath& the_metadatadir);
 
     //! Transform field-k value strval according to its fieldtype into one ore more reals
     //! and write those into dest (which should be of appropriate size: colrange[k].second)



From nouiz at mail.berlios.de  Wed Mar 11 16:29:28 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Mar 2009 16:29:28 +0100
Subject: [Plearn-commits] r10010 - trunk/plearn/vmat
Message-ID: <200903111529.n2BFTSo8017884@sheep.berlios.de>

Author: nouiz
Date: 2009-03-11 16:29:27 +0100 (Wed, 11 Mar 2009)
New Revision: 10010

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
fix LOG messages.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-11 15:28:58 UTC (rev 10009)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-11 15:29:27 UTC (rev 10010)
@@ -288,9 +288,10 @@
         }
         if(no_expended_fields.length()>0){
             NORMAL_LOG<<"In TextFilesVMatrix::setColumnNamesAndWidth - "
-                      <<"Did not find any partial match for:";
+                      <<"Did not find any partial match for "
+                      <<no_expended_fields.length()<<" spec:";
             for(int i=0;i<no_expended_fields.length();i++)
-                NORMAL_LOG<<" "<<fieldspec[i].first;
+                NORMAL_LOG<<" "<<no_expended_fields[i];
             NORMAL_LOG<<endl;
         }
             



From nouiz at mail.berlios.de  Wed Mar 11 18:13:17 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Mar 2009 18:13:17 +0100
Subject: [Plearn-commits] r10011 - in trunk: commands plearn_learners/cgi
Message-ID: <200903111713.n2BHDHFi012663@sheep.berlios.de>

Author: nouiz
Date: 2009-03-11 18:13:17 +0100 (Wed, 11 Mar 2009)
New Revision: 10011

Added:
   trunk/plearn_learners/cgi/ConfigParsing.cc
   trunk/plearn_learners/cgi/ConfigParsing.h
Modified:
   trunk/commands/plearn_desjardins.cc
Log:
Added a classe that I use for the configuration. It allow having multiple configuration file in one csv file.


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2009-03-11 15:29:27 UTC (rev 10010)
+++ trunk/commands/plearn_desjardins.cc	2009-03-11 17:13:17 UTC (rev 10011)
@@ -79,6 +79,7 @@
 #include <plearn_learners/hyper/HyperOptimize.h>
 #include <plearn_learners/hyper/EarlyStoppingOracle.h>
 #include <plearn_learners/cgi/StabilisationLearner.h>
+#include <plearn_learners/cgi/ConfigParsing.h>
 
 /************
  * Splitter *
@@ -96,13 +97,13 @@
 #include <plearn/vmat/ConcatColumnsVMatrix.h>
 #include <plearn/vmat/DichotomizeVMatrix.h>
 #include <plearn/vmat/FilteredVMatrix.h>
-#include <plearn/vmat/GaussianizeVMatrix.h>
-#include <plearn/vmat/ConstantVMatrix.h>
+//#include <plearn/vmat/GaussianizeVMatrix.h>
+//#include <plearn/vmat/ConstantVMatrix.h>
 #include <plearn/vmat/MemoryVMatrixNoSave.h>
 #include <plearn/vmat/MissingInstructionVMatrix.h>
 #include <plearn/vmat/ProcessingVMatrix.h>
 #include <plearn/vmat/TextFilesVMatrix.h>
-#include <plearn/vmat/TransposeVMatrix.h>
+//#include <plearn/vmat/TransposeVMatrix.h>
 #include <plearn/vmat/VariableDeletionVMatrix.h>
 #include <plearn/vmat/MeanMedianModeImputationVMatrix.h>
 #include <plearn/vmat/MissingIndicatorVMatrix.h>

Added: trunk/plearn_learners/cgi/ConfigParsing.cc
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-11 15:29:27 UTC (rev 10010)
+++ trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-11 17:13:17 UTC (rev 10011)
@@ -0,0 +1,154 @@
+// -*- C++ -*-
+
+// ConfigParsing.cc
+//
+// Copyright (C) 2009 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file ConfigParsing.cc */
+
+
+#include "ConfigParsing.h"
+#include <plearn/vmat/TextFilesVMatrix.h>
+#include <plearn/io/openFile.h>
+#include <plearn/base/stringutils.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This allows to register the 'ConfigParsing' command in the command registry
+PLearnCommandRegistry ConfigParsing::reg_(new ConfigParsing);
+
+ConfigParsing::ConfigParsing()
+    : PLearnCommand(
+        "ConfigParsing",
+        ">>>> INSERT A SHORT ONE LINE DESCRIPTION HERE",
+        ">>>> INSERT SYNTAX AND \n"
+        "FULL DETAILED HELP HERE \n"
+        )
+{}
+
+//! The actual implementation of the 'ConfigParsing' command
+void ConfigParsing::run(const vector<string>& args)
+{
+    // *** PLEASE COMPLETE HERE ****
+/*    args1 = conf/conf.all.csv;
+    args2 = conf/1convertCSV0709toPLearn.inc;
+    args3 = conf/3b_remove_col.inc;
+    args4 = conf/3fix_missing.inc;
+    args5 = conf/9dichotomize.inc;
+    args6 = conf/global_imputation_specifications.inc;
+*/
+    PLCHECK(args.size()==6);
+    TextFilesVMatrix input = TextFilesVMatrix();
+    input.auto_build_map = 0  ;
+    input.default_spec="char";
+//#auto_extend_map = 0  ;
+    input.build_vmatrix_stringmap = 1  ;
+    input.delimiter = ","  ;//TODO ; or auto?
+    input.quote_delimiter = '"';
+    input.skipheader.append(1);
+    input.reorder_fieldspec_from_headers=1;
+    input.txtfilenames.append(args[0]);
+    input.partial_match=1;
+    input.setMetaDataDir(args[0]+".metadatadir");
+    input.build();
+    PStream f_csv = openFile(PPath(args[1]),PStream::raw_ascii,"w");
+    PStream f_remove = openFile(args[2],PStream::raw_ascii,"w");
+    PStream f_missing = openFile(args[3],PStream::raw_ascii,"w");
+    PStream f_dichotomize = openFile(args[4],PStream::raw_ascii,"w");
+    PStream f_imputation = openFile(args[5],PStream::raw_ascii,"w");
+
+    f_csv<<"$INCLUDE{conf/date.inc}"<<endl;
+    f_remove<<"$INCLUDE{conf/date.inc}"<<endl;
+    f_missing<<"$INCLUDE{conf/date.inc}"<<endl;
+    f_dichotomize<<"$INCLUDE{conf/date.inc}"<<endl;
+    f_imputation<<"$INCLUDE{conf/date.inc}"<<endl;
+    for(int i=0;i<input.length();i++){
+        TVec<string> r = input.getTextFields(i);
+        char c = r[0][0];
+        if(c=='#' || r[0].empty())//comment
+            continue;
+        if(!r[1].empty()){
+            f_csv << (r[0]);
+            f_csv << (" : ");
+            f_csv << (r[1]) << endl;
+        }
+        string y = lowerstring(r[2]);//TODO check that this is an accepted command.
+        if(y=="y" ||y=="yes"){//comment
+            f_remove << (r[0]) << endl;
+        }else if(y=="n" ||y=="no"||y==""){
+        }else{
+            PLERROR("Unknow value in column C:'%s'",r[2].c_str());
+        }
+        if(!r[3].empty()){
+            f_missing << (r[0]);
+            f_missing << (" : ");
+            f_missing << (r[3]);//TODO check that this is an accepted command.
+            f_missing << endl;
+        }
+        if(!r[4].empty()){
+            f_imputation << (r[0]);
+            f_imputation << (" : ");
+            f_imputation << (r[4]);//TODO check that this is an accepted command.
+            f_imputation << endl;
+        }
+        if(!r[5].empty()){
+            f_dichotomize <<r[0]<<" : ["<< (r[5]) << " ]"<<endl;
+        }
+
+        
+    }
+    f_csv<<"$INCLUDE{conf/date_undef.inc}"<<endl;
+    f_remove<<"$INCLUDE{conf/date_undef.inc}"<<endl;
+    f_missing<<"$INCLUDE{conf/date_undef.inc}"<<endl;
+    f_dichotomize<<"$INCLUDE{conf/date_undef.inc}"<<endl;
+    f_imputation<<"$INCLUDE{conf/date_undef.inc}"<<endl;
+
+        
+
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/cgi/ConfigParsing.h
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.h	2009-03-11 15:29:27 UTC (rev 10010)
+++ trunk/plearn_learners/cgi/ConfigParsing.h	2009-03-11 17:13:17 UTC (rev 10011)
@@ -0,0 +1,86 @@
+// -*- C++ -*-
+
+// ConfigParsing.h
+//
+// Copyright (C) 2009 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file ConfigParsing.h */
+
+
+#ifndef ConfigParsing_INC
+#define ConfigParsing_INC
+
+#include <commands/PLearnCommands/PLearnCommand.h>
+#include <commands/PLearnCommands/PLearnCommandRegistry.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class ConfigParsing : public PLearnCommand
+{
+    typedef PLearnCommand inherited;
+
+public:
+    ConfigParsing();
+    virtual void run(const std::vector<std::string>& args);
+
+protected:
+    static PLearnCommandRegistry reg_;
+};
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Wed Mar 11 19:17:38 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Mar 2009 19:17:38 +0100
Subject: [Plearn-commits] r10012 - trunk/plearn_learners/online
Message-ID: <200903111817.n2BIHcwo028410@sheep.berlios.de>

Author: nouiz
Date: 2009-03-11 19:17:37 +0100 (Wed, 11 Mar 2009)
New Revision: 10012

Modified:
   trunk/plearn_learners/online/ShuntingNNetLayerModule.cc
Log:
fix compilation on a compiler that don't do the casting automatically from int to real.


Modified: trunk/plearn_learners/online/ShuntingNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/ShuntingNNetLayerModule.cc	2009-03-11 17:13:17 UTC (rev 10011)
+++ trunk/plearn_learners/online/ShuntingNNetLayerModule.cc	2009-03-11 18:17:37 UTC (rev 10012)
@@ -411,8 +411,8 @@
         Mat tmp(n, output_size);
         // tmp = (1 + E + S ).^2;
         tmp.fill(1.);
-        multiplyAcc(tmp, batch_excitations, 1);
-        multiplyAcc(tmp, batch_inhibitions, 1);
+        multiplyAcc(tmp, batch_excitations, (real)1);
+        multiplyAcc(tmp, batch_inhibitions, (real)1);
         squareElements(tmp);
         
         Vec bias_updates(output_size);



From nouiz at mail.berlios.de  Wed Mar 11 21:40:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Mar 2009 21:40:58 +0100
Subject: [Plearn-commits] r10013 - trunk/plearn/misc
Message-ID: <200903112040.n2BKewd9026904@sheep.berlios.de>

Author: nouiz
Date: 2009-03-11 21:40:58 +0100 (Wed, 11 Mar 2009)
New Revision: 10013

Modified:
   trunk/plearn/misc/PTimer.cc
   trunk/plearn/misc/PTimer.h
Log:
Added the option PTimer::use_times_fct . If true, we will always use the times(0) fct, so we will always report the walltime. Otherwise, we use the clock() fct for short time(under 30minutes) and that report the sum of all chield time. Usefull when we use multiple threads.


Modified: trunk/plearn/misc/PTimer.cc
===================================================================
--- trunk/plearn/misc/PTimer.cc	2009-03-11 18:17:37 UTC (rev 10012)
+++ trunk/plearn/misc/PTimer.cc	2009-03-11 20:40:58 UTC (rev 10013)
@@ -53,14 +53,22 @@
     "\n"
     "Note that for advanced profiling, one should probably use the\n"
     "Profiler class instead.\n"
+    "For short duration and when the option use_times_fct is false\n"
+    "we will use the clock() fct that take the sum of all chield threads\n"
+    "Otherwise we use the times(0) fct that report the wall time.\n"
     );
 
 ////////////
 // PTimer //
 ////////////
-PTimer::PTimer() 
+PTimer::PTimer()
+    :use_times_fct(false)
 {}
 
+PTimer::PTimer(bool use_times_fct_)
+    :use_times_fct(use_times_fct_)
+{}
+
 ///////////
 // build //
 ///////////
@@ -100,6 +108,11 @@
                                      OptionBase::learntoption,
         "Contains the current total times of all timers.");
 
+    declareOption(ol, "use_times_fct", &PTimer::use_times_fct,
+                  OptionBase::buildoption,
+                  "If true, we will use the times(0) fct. So we will report the"
+                  " wall time. Usefull in multithread case.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -189,7 +202,7 @@
     int timer_id = name_to_idx[timer_name];
     clock_t time_clock_t = clock() - start_clock_t[timer_id];
     long time_long = long(time(0)) - start_long[timer_id];
-    if (time_long > 1800)
+    if (time_long > 1800 || use_times_fct)
         // More than 30 mins: we use the approximate length.
         total_times[timer_id] += time_long;
     else

Modified: trunk/plearn/misc/PTimer.h
===================================================================
--- trunk/plearn/misc/PTimer.h	2009-03-11 18:17:37 UTC (rev 10012)
+++ trunk/plearn/misc/PTimer.h	2009-03-11 20:40:58 UTC (rev 10013)
@@ -63,12 +63,13 @@
 
     //! ### declare public option fields (such as build options) here
     //! Start your comments with Doxygen-compatible comments such as //!
-
+    bool use_times_fct;
 public:
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
     PTimer();
+    PTimer(bool use_times_fct_);
 
     // Your other public member functions go here
 



From nouiz at mail.berlios.de  Wed Mar 11 21:42:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Mar 2009 21:42:36 +0100
Subject: [Plearn-commits] r10014 - trunk/plearn_learners/meta
Message-ID: <200903112042.n2BKgaiB027209@sheep.berlios.de>

Author: nouiz
Date: 2009-03-11 21:42:36 +0100 (Wed, 11 Mar 2009)
New Revision: 10014

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
-use the new PTimer::use_times_fct option to report walltime.
-set the verbosity level of the sublearner to false in multi thread.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-11 20:40:58 UTC (rev 10013)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-11 20:42:36 UTC (rev 10014)
@@ -65,7 +65,7 @@
     time_costs(true),
     warn_once_target_gt_2(false),
     done_warn_once_target_gt_2(false),
-    timer(new PTimer()),
+    timer(new PTimer(true)),
     time_sum(0),
     time_sum_rtr(0),
     time_last_stage(0),
@@ -279,12 +279,12 @@
 //Otherwise this will cause crash due to the parallel printing to stdout stderr.
 #ifdef _OPENMP
     //the AdaBoost and the weak learner should not print anything as this will cause race condition on the printing
+    //TODO find a way to have thread safe output?
     if(omp_get_max_threads()>1){
-      PLCHECK(learner1->verbosity==0);
-      PLCHECK(learner2->verbosity==0);
-      
-      PLCHECK(learner1->weak_learner_template->verbosity==0);
-      PLCHECK(learner2->weak_learner_template->verbosity==0);
+        learner1->verbosity=0;
+        learner2->verbosity=0;
+        learner1->weak_learner_template->verbosity=0;
+        learner2->weak_learner_template->verbosity=0;
     }
     
     EXTREME_MODULE_LOG<<"train() // start"<<endl;



From chrish at mail.berlios.de  Wed Mar 11 21:55:20 2009
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 11 Mar 2009 21:55:20 +0100
Subject: [Plearn-commits] r10015 - trunk/plearn/vmat
Message-ID: <200903112055.n2BKtKGa028382@sheep.berlios.de>

Author: chrish
Date: 2009-03-11 21:55:20 +0100 (Wed, 11 Mar 2009)
New Revision: 10015

Modified:
   trunk/plearn/vmat/PythonTableVMatrix.cc
Log:
Silence spurious warnings.

Modified: trunk/plearn/vmat/PythonTableVMatrix.cc
===================================================================
--- trunk/plearn/vmat/PythonTableVMatrix.cc	2009-03-11 20:42:36 UTC (rev 10014)
+++ trunk/plearn/vmat/PythonTableVMatrix.cc	2009-03-11 20:55:20 UTC (rev 10015)
@@ -58,7 +58,10 @@
 void PythonTableVMatrix::getNewRow(int i, const Vec& v) const
 {
     PLASSERT(the_table);
-    PyObject* row= PyObject_CallMethod(the_table, "getRow", "i", i);
+    // Casting away const is okay here, PyObject_CallMethod does not
+    // modify its arguments. XXX
+    PyObject* row= PyObject_CallMethod(the_table, const_cast<char*>("getRow"),
+                                       const_cast<char*>("i"), i);
     if(!row)
     {
         if (PyErr_Occurred()) PyErr_Print();
@@ -82,7 +85,9 @@
 void PythonTableVMatrix::build_()
 {
     if(!the_table) return;
-    PyObject* pywidth= PyObject_CallMethod(the_table, "width", 0);
+    // Casting away const is okay, PyObject_CallMethod does not modify its
+    // arguments.
+    PyObject* pywidth= PyObject_CallMethod(the_table, const_cast<char*>("width"), NULL);
     if(!pywidth)
     {
         if (PyErr_Occurred()) PyErr_Print();
@@ -91,7 +96,7 @@
     }
     width_= PythonObjectWrapper(pywidth);
     Py_DECREF(pywidth);
-    PyObject* pylength= PyObject_CallMethod(the_table, "length", 0);
+    PyObject* pylength= PyObject_CallMethod(the_table, const_cast<char*>("length"), NULL);
     if(!pylength)
     {
         if (PyErr_Occurred()) PyErr_Print();



From nouiz at mail.berlios.de  Wed Mar 11 21:55:21 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 11 Mar 2009 21:55:21 +0100
Subject: [Plearn-commits] r10016 - trunk/plearn/misc
Message-ID: <200903112055.n2BKtLY3028393@sheep.berlios.de>

Author: nouiz
Date: 2009-03-11 21:55:20 +0100 (Wed, 11 Mar 2009)
New Revision: 10016

Modified:
   trunk/plearn/misc/viewVMat.cc
Log:
in plearn vmat view, if we reload the vmat, when we quit we should not see the last vmat.


Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2009-03-11 20:55:20 UTC (rev 10015)
+++ trunk/plearn/misc/viewVMat.cc	2009-03-11 20:55:20 UTC (rev 10016)
@@ -656,6 +656,9 @@
                     endwin();
                     // And launch the new one.
                     viewVMat(new_vm, filename);
+                    if(string(c).empty())
+                        //if we reload, we should forget the last one
+                        key='q';
                 }
             }
             break;



From tihocan at mail.berlios.de  Thu Mar 12 15:26:32 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 12 Mar 2009 15:26:32 +0100
Subject: [Plearn-commits] r10017 - trunk/plearn/misc
Message-ID: <200903121426.n2CEQVnO028759@sheep.berlios.de>

Author: tihocan
Date: 2009-03-12 15:26:31 +0100 (Thu, 12 Mar 2009)
New Revision: 10017

Modified:
   trunk/plearn/misc/PTimer.cc
   trunk/plearn/misc/PTimer.h
Log:
- Proper object building mechanism in convenience constructor
- Typo fix: times -> time


Modified: trunk/plearn/misc/PTimer.cc
===================================================================
--- trunk/plearn/misc/PTimer.cc	2009-03-11 20:55:20 UTC (rev 10016)
+++ trunk/plearn/misc/PTimer.cc	2009-03-12 14:26:31 UTC (rev 10017)
@@ -53,21 +53,25 @@
     "\n"
     "Note that for advanced profiling, one should probably use the\n"
     "Profiler class instead.\n"
-    "For short duration and when the option use_times_fct is false\n"
-    "we will use the clock() fct that take the sum of all chield threads\n"
-    "Otherwise we use the times(0) fct that report the wall time.\n"
+    "For short duration or when the option use_time_fct is false we will use\n"
+    "the clock() function that takes the sum of all child threads. Otherwise\n"
+    "we use the time(0) function that reports the wall time.\n"
     );
 
 ////////////
 // PTimer //
 ////////////
 PTimer::PTimer()
-    :use_times_fct(false)
+    :use_time_fct(false)
 {}
 
-PTimer::PTimer(bool use_times_fct_)
-    :use_times_fct(use_times_fct_)
-{}
+PTimer::PTimer(bool use_time_fct_, bool call_build_):
+    inherited(call_build_),
+    use_time_fct(use_time_fct_)
+{
+    if (call_build_)
+        build_();
+}
 
 ///////////
 // build //
@@ -108,10 +112,10 @@
                                      OptionBase::learntoption,
         "Contains the current total times of all timers.");
 
-    declareOption(ol, "use_times_fct", &PTimer::use_times_fct,
+    declareOption(ol, "use_time_fct", &PTimer::use_time_fct,
                   OptionBase::buildoption,
-                  "If true, we will use the times(0) fct. So we will report the"
-                  " wall time. Usefull in multithread case.");
+        "If true, we will use the time(0) function. So we will report the\n"
+        "wall time. Useful in multithread case.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -202,7 +206,7 @@
     int timer_id = name_to_idx[timer_name];
     clock_t time_clock_t = clock() - start_clock_t[timer_id];
     long time_long = long(time(0)) - start_long[timer_id];
-    if (time_long > 1800 || use_times_fct)
+    if (time_long > 1800 || use_time_fct)
         // More than 30 mins: we use the approximate length.
         total_times[timer_id] += time_long;
     else

Modified: trunk/plearn/misc/PTimer.h
===================================================================
--- trunk/plearn/misc/PTimer.h	2009-03-11 20:55:20 UTC (rev 10016)
+++ trunk/plearn/misc/PTimer.h	2009-03-12 14:26:31 UTC (rev 10017)
@@ -63,13 +63,13 @@
 
     //! ### declare public option fields (such as build options) here
     //! Start your comments with Doxygen-compatible comments such as //!
-    bool use_times_fct;
+    bool use_time_fct;
 public:
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
     PTimer();
-    PTimer(bool use_times_fct_);
+    PTimer(bool use_time_fct_, bool call_build_ = true);
 
     // Your other public member functions go here
 



From nouiz at mail.berlios.de  Thu Mar 12 19:39:40 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 12 Mar 2009 19:39:40 +0100
Subject: [Plearn-commits] r10018 - trunk/plearn_learners/regressors
Message-ID: <200903121839.n2CIdeUw026399@sheep.berlios.de>

Author: nouiz
Date: 2009-03-12 19:39:39 +0100 (Thu, 12 Mar 2009)
New Revision: 10018

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
Log:
inline 2 fct and add a fct that I use to test one optimization.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-03-12 14:26:31 UTC (rev 10017)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-03-12 18:39:39 UTC (rev 10018)
@@ -237,9 +237,20 @@
     cout << " ws " << weights_sum;
     cout << " ts " << targets_sum;
     cout << " wts " << weighted_targets_sum;
-    cout << " wsts " << weighted_squared_targets_sum; 
+    cout << " wsts " << weighted_squared_targets_sum;
+    cout << " wts/ws " <<weighted_targets_sum/weights_sum;
+    cout << " wsts/ws "<<weighted_squared_targets_sum/weights_sum;
+    cout << " sqrt(wsts/ws) "<<sqrt(weighted_squared_targets_sum/weights_sum);
     cout << endl;
 }
+bool RegressionTreeLeave::uniqTarget(){
+    if(classname()=="RegressionTreeLeave"){
+        real wts_w = weighted_targets_sum/weights_sum;
+        real wsts_w= sqrt(weighted_squared_targets_sum/weights_sum);
+        return fast_is_equal(wts_w,wsts_w);
+    }else
+        PLERROR("In RegressionTreeLeave::uniqTarget subclass must reimplement it.");
+}
 
 void RegressionTreeLeave::verbose(string the_msg, int the_level)
 {

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-03-12 14:26:31 UTC (rev 10017)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-03-12 18:39:39 UTC (rev 10018)
@@ -101,8 +101,10 @@
     inline int           length()const{return length_;}
     virtual void         getOutputAndError(Vec& output, Vec& error)const;
     virtual void         printStats();
-    real                 getWeightsSum(){return weights_sum;}
-    real                 getTargetsSum(){return targets_sum;}
+    inline real          getWeightsSum(){return weights_sum;}
+    inline real          getTargetsSum(){return targets_sum;}
+    virtual bool         uniqTarget();
+
 private:
     void         build_();
     void         verbose(string msg, int level);



From nouiz at mail.berlios.de  Thu Mar 12 19:41:04 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 12 Mar 2009 19:41:04 +0100
Subject: [Plearn-commits] r10019 - trunk/plearn/vmat
Message-ID: <200903121841.n2CIf4vK026646@sheep.berlios.de>

Author: nouiz
Date: 2009-03-12 19:41:03 +0100 (Thu, 12 Mar 2009)
New Revision: 10019

Modified:
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
Log:
added jdate_is_missing as an alias for 2436935_is_missing. The name is more consistent with TextFileVMatrix.


Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2009-03-12 18:39:39 UTC (rev 10018)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2009-03-12 18:41:03 UTC (rev 10019)
@@ -117,7 +117,7 @@
     declareOption(ol, "missing_instructions", &MissingInstructionVMatrix::missing_instructions,
                   OptionBase::buildoption,
                   "The variable missing regeneration instructions in the form of pairs field : instruction.\n"
-                  "Supported instructions are skip, as_is, zero_is_missing, zero_or_neg_is_missing, 2436935_is_missing(01JAN1960 in julian day), present.\n"
+                  "Supported instructions are skip, as_is, zero_is_missing, zero_or_neg_is_missing, 2436935_is_missing(01JAN1960 in julian day), jdate_is_missing(idem as 2436935_is_missing), present.\n"
                   "If the instruction fieldname end with '*' we will extend it to all the source matrix fieldname that match the regex.\n"
                   "No other regex are supported");
     declareOption(ol, "default_instruction",
@@ -201,7 +201,8 @@
             ins[source_col] = "zero_is_missing";
         else if (missing_instructions[ins_col].second == "zero_or_neg_is_missing")
             ins[source_col] = "zero_or_neg_is_missing";
-        else if (missing_instructions[ins_col].second == "2436935_is_missing")
+        else if (missing_instructions[ins_col].second == "2436935_is_missing"
+                 || missing_instructions[ins_col].second == "jdate_is_missing")
             ins[source_col] = "2436935_is_missing";
         else if (missing_instructions[ins_col].second == "present")
             ins[source_col] = "present";



From nouiz at mail.berlios.de  Thu Mar 12 19:42:46 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 12 Mar 2009 19:42:46 +0100
Subject: [Plearn-commits] r10020 - trunk/plearn_learners/regressors
Message-ID: <200903121842.n2CIgjou026964@sheep.berlios.de>

Author: nouiz
Date: 2009-03-12 19:42:45 +0100 (Thu, 12 Mar 2009)
New Revision: 10020

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
fix help message and try to fix a bug in paralle computation when I think the compiler don't work correctly.


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-12 18:41:03 UTC (rev 10019)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-12 18:42:45 UTC (rev 10020)
@@ -58,7 +58,7 @@
                         "with the mean square error and a facto of the confidence funtion.\n"
                         "At each node expansion, it creates 3 nodes, one to hold any samples with a missing value on the\n"
                         "splitting attribute, one for the samples with values less than the value of the splitting attribute\n"
-                        "and one fr the others.\n"
+                        "and one for the others.\n"
     );
 
 RegressionTree::RegressionTree()     
@@ -237,8 +237,10 @@
         if (report_progress) pb->update(stage);
     }
     pb = NULL;
+#ifndef _OPENMP
     verbose("split_cols: "+tostring(split_cols),2);
     verbose("split_values: "+tostring(split_values),2);
+#endif
     if (compute_train_stats < 1){
         Profiler::pl_profile_end("RegressionTree::train");
         return;



From nouiz at mail.berlios.de  Thu Mar 12 20:04:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 12 Mar 2009 20:04:26 +0100
Subject: [Plearn-commits] r10021 - trunk/plearn_learners/online
Message-ID: <200903121904.n2CJ4Q3k030111@sheep.berlios.de>

Author: nouiz
Date: 2009-03-12 20:04:25 +0100 (Thu, 12 Mar 2009)
New Revision: 10021

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
fix compilation in float


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2009-03-12 18:42:45 UTC (rev 10020)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2009-03-12 19:04:25 UTC (rev 10021)
@@ -1855,7 +1855,7 @@
                 {
                     Vec expectation_c = greedy_target_expectations[i][c];
                     real p_c = greedy_target_layers[i]->expectation[c];
-                    multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+                    multiplyScaledAdd(expectation_c, real(1.), p_c, expectation);
                 }
             }
             else
@@ -2232,7 +2232,7 @@
             {
                 Vec expectation_c = greedy_target_expectations[i][c];
                 real p_c = greedy_target_layers[i]->expectation[c];
-                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+                multiplyScaledAdd(expectation_c, real(1.), p_c, expectation);
             }
         }
         else
@@ -2280,7 +2280,7 @@
             {
                 Vec expectation_c = greedy_target_expectations[n_layers-2][c];
                 real p_c = greedy_target_layers[n_layers-2]->expectation[c];
-                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+                multiplyScaledAdd(expectation_c, real(1.), p_c, expectation);
             }
         }
         else
@@ -2338,7 +2338,7 @@
 
                 // Update target connections, with gradient from p(h_l | h_l-1, y)
                 multiplyScaledAdd( greedy_target_activation_gradients[n_layers-2][c].toMat(layers[n_layers-1]->size,1),
-                                   1., -greedy_target_connections[n_layers-2]->learning_rate,
+                                   real(1.), -greedy_target_connections[n_layers-2]->learning_rate,
                                    greedy_target_connections[n_layers-2]->weights.column(c));
                 
                 greedy_target_probability_gradients[n_layers-2][c] = 
@@ -2365,7 +2365,7 @@
 
                 // Update target connections, with gradient from p(y | h_l-1 )
                 multiplyScaledAdd( greedy_target_activation_gradients[n_layers-2][c].toMat(layers[n_layers-1]->size,1),
-                                   1., -greedy_target_connections[n_layers-2]->learning_rate,
+                                   real(1.), -greedy_target_connections[n_layers-2]->learning_rate,
                                    greedy_target_connections[n_layers-2]->weights.column(c));
             }
 
@@ -2445,7 +2445,7 @@
 
                 // Update target connections, with gradient from p(h_l | h_l-1, y)
                 multiplyScaledAdd( greedy_target_activation_gradients[i-1][c].toMat(layers[i]->size,1),
-                                   1., -greedy_target_connections[i-1]->learning_rate,
+                                   real(1.), -greedy_target_connections[i-1]->learning_rate,
                                    greedy_target_connections[i-1]->weights.column(c));
                 
                 greedy_target_probability_gradients[i-1][c] = 
@@ -2472,7 +2472,7 @@
 
                 // Update target connections, with gradient from p(y | h_l-1 )
                 multiplyScaledAdd( greedy_target_activation_gradients[i-1][c].toMat(layers[i]->size,1),
-                                   1., -greedy_target_connections[i-1]->learning_rate,
+                                   real(1.), -greedy_target_connections[i-1]->learning_rate,
                                    greedy_target_connections[i-1]->weights.column(c));
             }
 
@@ -2968,7 +2968,7 @@
             {
                 Vec expectation_c = greedy_target_expectations[i][c];
                 real p_c = greedy_target_layers[i]->expectation[c];
-                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+                multiplyScaledAdd(expectation_c, real(1.), p_c, expectation);
             }
         }
         else
@@ -3047,7 +3047,7 @@
             {
                 Vec expectation_c = greedy_target_expectations[n_layers-2][c];
                 real p_c = greedy_target_layers[n_layers-2]->expectation[c];
-                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+                multiplyScaledAdd(expectation_c,real(1.), p_c, expectation);
             }
         }
         else



From lamblin at mail.berlios.de  Thu Mar 12 21:39:40 2009
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 12 Mar 2009 21:39:40 +0100
Subject: [Plearn-commits] r10022 - trunk/plearn_learners/generic
Message-ID: <200903122039.n2CKdeHV006830@sheep.berlios.de>

Author: lamblin
Date: 2009-03-12 21:39:35 +0100 (Thu, 12 Mar 2009)
New Revision: 10022

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
Fix segfault when setting a training set with targetsize = -1.


Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2009-03-12 19:04:25 UTC (rev 10021)
+++ trunk/plearn_learners/generic/NNet.cc	2009-03-12 20:39:35 UTC (rev 10022)
@@ -936,8 +936,13 @@
 /////////////////
 // fillWeights //
 /////////////////
-void NNet::fillWeights(const Var& weights, bool clear_first_row) {
-    if (initialization_method == "zero") {
+void NNet::fillWeights(const Var& weights, bool clear_first_row)
+{
+    if (!weights)
+        return;
+
+    if (initialization_method == "zero")
+    {
         weights->value->clear();
         return;
     }
@@ -1176,8 +1181,7 @@
         wout->matValue(0).clear();
     }
     else {
-        if (wout)
-            fillWeights(wout, true);
+        fillWeights(wout, true);
     }
 }
 
@@ -1277,16 +1281,25 @@
                 pb->update(i);
         }
     }
-    
+
     if(!train_stats)
         setTrainStatsCollector(new VecStatsCollector());
     // PLERROR("In NNet::train, you did not setTrainStatsCollector");
 
     int n_train = operate_on_bags ? n_training_bags
                                   : train_set->length();  
-    
-    if(input_to_output.isNull()) // Net has not been properly built yet (because build was called before the learner had a proper training set)
+
+    if(input_to_output.isNull())
+    {
+        // Net has not been properly built yet (because build was called before the learner had a proper training set)
         build();
+        if (input_to_output.isNull())
+            PLERROR(
+                "NNet::build was not able to properly build the network.\n"
+                "Please check that your variables have an appropriate value,\n"
+                "that your training set is correctly defined, that its sizes\n"
+                "are consistent, that its targetsize is not -1...");
+    }
 
     // number of samples seen by optimizer before each optimizer update
     int nsamples = batch_size>0 ? batch_size : n_train;



From nouiz at mail.berlios.de  Fri Mar 13 18:43:21 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 13 Mar 2009 18:43:21 +0100
Subject: [Plearn-commits] r10023 - trunk/plearn_learners/cgi
Message-ID: <200903131743.n2DHhLeG011525@sheep.berlios.de>

Author: nouiz
Date: 2009-03-13 18:43:15 +0100 (Fri, 13 Mar 2009)
New Revision: 10023

Modified:
   trunk/plearn_learners/cgi/ConfigParsing.cc
   trunk/plearn_learners/cgi/SecondIterationWrapper.cc
Log:
don't add config for var that we remove.


Modified: trunk/plearn_learners/cgi/ConfigParsing.cc
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-12 20:39:35 UTC (rev 10022)
+++ trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-13 17:43:15 UTC (rev 10023)
@@ -104,25 +104,27 @@
             f_csv << (r[1]) << endl;
         }
         string y = lowerstring(r[2]);//TODO check that this is an accepted command.
+        bool remove=false;
         if(y=="y" ||y=="yes"){//comment
             f_remove << (r[0]) << endl;
+            remove=true;
         }else if(y=="n" ||y=="no"||y==""){
         }else{
             PLERROR("Unknow value in column C:'%s'",r[2].c_str());
         }
-        if(!r[3].empty()){
+        if(!r[3].empty() && !remove){
             f_missing << (r[0]);
             f_missing << (" : ");
             f_missing << (r[3]);//TODO check that this is an accepted command.
             f_missing << endl;
         }
-        if(!r[4].empty()){
+        if(!r[4].empty() && !remove){
             f_imputation << (r[0]);
             f_imputation << (" : ");
             f_imputation << (r[4]);//TODO check that this is an accepted command.
             f_imputation << endl;
         }
-        if(!r[5].empty()){
+        if(!r[5].empty() && !remove){
             f_dichotomize <<r[0]<<" : ["<< (r[5]) << " ]"<<endl;
         }
 

Modified: trunk/plearn_learners/cgi/SecondIterationWrapper.cc
===================================================================
--- trunk/plearn_learners/cgi/SecondIterationWrapper.cc	2009-03-12 20:39:35 UTC (rev 10022)
+++ trunk/plearn_learners/cgi/SecondIterationWrapper.cc	2009-03-13 17:43:15 UTC (rev 10023)
@@ -314,8 +314,6 @@
 
 void SecondIterationWrapper::forget()
 {
-    PLASSERT( base_regressor );
-    base_regressor->forget();
 }
 
 int SecondIterationWrapper::outputsize() const
@@ -357,8 +355,8 @@
         real class_pred;
         if (outputv[0] <= 0.5) class_pred = 0.;
         else if (outputv[0] <= 1.5) class_pred = 1.0;
-        else if (outputv[0] <= 2.5) class_pred = 2.0;
-        else class_pred = 3.0;
+//        else if (outputv[0] <= 2.5) class_pred = 2.0;
+        else class_pred = 2.0;
         costsv[1] = pow((class_pred - targetv[0]), 2);
         costsv[2] = fabs(class_pred - targetv[0]);
         costsv[3] = class_pred == targetv[0]?0:1;



From nouiz at mail.berlios.de  Fri Mar 13 18:45:05 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 13 Mar 2009 18:45:05 +0100
Subject: [Plearn-commits] r10024 - trunk/plearn_learners/regressors
Message-ID: <200903131745.n2DHj546013085@sheep.berlios.de>

Author: nouiz
Date: 2009-03-13 18:45:04 +0100 (Fri, 13 Mar 2009)
New Revision: 10024

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
optimization of train time of RegressionTree of 20%. This is more usefull the more var you have.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-13 17:43:15 UTC (rev 10023)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-13 17:45:04 UTC (rev 10024)
@@ -66,7 +66,8 @@
     verbosity(0),
     next_id(0),
     do_sort_rows(true),
-    mem_tsource(true)
+    mem_tsource(true),
+    compact_reg_leave(-1)
 {
     build();
 }
@@ -82,7 +83,8 @@
     verbosity(verbosity_),
     next_id(0),
     do_sort_rows(do_sort_rows_),
-    mem_tsource(mem_tsource_)
+    mem_tsource(mem_tsource_),
+    compact_reg_leave(-1)
 {
     source = source_;
     tsource = tsource_;
@@ -99,7 +101,8 @@
     verbosity(verbosity_),
     next_id(0),
     do_sort_rows(do_sort_rows_),
-    mem_tsource(mem_tsource_)
+    mem_tsource(mem_tsource_),
+    compact_reg_leave(-1)
 {
     source = source_;
     build();
@@ -202,6 +205,7 @@
         }
     leave_register.resize(length());
     sortRows();
+    compact_reg.resize(length());
 }
 
 void RegressionTreeRegisters::reinitRegisters()
@@ -244,21 +248,39 @@
 
     int idx=0;
     int n=reg.length();
-    int i;
     RTR_type* preg = reg.data();
     RTR_type* ptsorted_row = tsorted_row[col];
     RTR_type_id* pleave_register = leave_register.data();
-    for( i=0;i<length() && n> idx;i++)
-    {
-        PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
-        int srow = ptsorted_row[i];
-        if ( pleave_register[srow] == leave_id){
-            PLASSERT(leave_register[srow] == leave_id);
-            PLASSERT(preg[idx]==reg[idx]);
-            preg[idx++]=srow;
+    if(compact_reg_leave==leave_id){
+        //compact_reg is used as an optimization.
+        //as it is more compact in memory then leave_register
+        //we are more memory friendly.
+        for(int i=0;i<length() && n> idx;i++){
+            PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
+            int srow = ptsorted_row[i];
+            if ( compact_reg[srow] ){
+                PLASSERT(leave_register[srow] == leave_id);
+                PLASSERT(preg[idx]==reg[idx]);
+                preg[idx++]=srow;
+            }
         }
+    }else{
+        for(uint i=0;i<compact_reg.size();i++)
+            compact_reg[i]=false;
+        for(int i=0;i<length() && n> idx;i++){
+            PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
+            int srow = ptsorted_row[i];
+            if ( pleave_register[srow] == leave_id){
+                PLASSERT(leave_register[srow] == leave_id);
+                PLASSERT(preg[idx]==reg[idx]);
+                preg[idx++]=srow;
+                compact_reg[srow]=true;
+            }
+        }
+        compact_reg_leave = leave_id;
     }
     PLASSERT(idx==reg->size());
+
 }
 
 void RegressionTreeRegisters::sortRows()

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-03-13 17:43:15 UTC (rev 10023)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-03-13 17:45:04 UTC (rev 10024)
@@ -88,6 +88,8 @@
 
     bool do_sort_rows;
     bool mem_tsource;
+    mutable vector<bool> compact_reg;
+    mutable int compact_reg_leave;
 
 public:
 



From nouiz at mail.berlios.de  Mon Mar 16 14:43:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Mar 2009 14:43:26 +0100
Subject: [Plearn-commits] r10025 - trunk/scripts
Message-ID: <200903161343.n2GDhQls029329@sheep.berlios.de>

Author: nouiz
Date: 2009-03-16 14:43:20 +0100 (Mon, 16 Mar 2009)
New Revision: 10025

Modified:
   trunk/scripts/dbidispatch
Log:
generate an error when we try to restart a job in the queue.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-03-13 17:45:04 UTC (rev 10024)
+++ trunk/scripts/dbidispatch	2009-03-16 13:43:20 UTC (rev 10025)
@@ -414,7 +414,10 @@
 for i in dbi_param:
     if i not in valid_dbi_param:
         print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end. It will be ignored."
-print "With the command to be expanded:"," ".join(command_argv),"\n\n"
+if dbi_param.get("restart",False):
+    print "With the command to be restarted:"," ".join(command_argv),"\n\n"
+else:
+    print "With the command to be expanded:"," ".join(command_argv),"\n\n"
 
 def generate_combination(repl,sep=" "):
     if repl == []:
@@ -480,11 +483,17 @@
     assert launch_cmd=="Condor"
     cmds=[]
     for arg in command_argv:
-        p=Popen("condor_history -l "+arg, shell=True, stdout=PIPE)
-        p.wait()
-        lines=p.stdout.readlines()
+        p1=Popen("condor_q -l "+arg, shell=True, stdout=PIPE)
+        p2=Popen("condor_history -l "+arg, shell=True, stdout=PIPE)
+        p1.wait();p2.wait()
+        lines=p1.stdout.readlines()
         for l in lines:
             if l.startswith("Arguments = "):
+                print "We don't accept to restart jobs in the queue:", arg
+                sys.exit(1)
+        lines=p2.stdout.readlines()
+        for l in lines:
+            if l.startswith("Arguments = "):
                 cmd=l[13:-2]
                 cmds.append(cmd.replace("'",""))
     commands=cmds



From nouiz at mail.berlios.de  Mon Mar 16 18:54:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Mar 2009 18:54:36 +0100
Subject: [Plearn-commits] r10026 - trunk/plearn_learners/regressors
Message-ID: <200903161754.n2GHsaQD017131@sheep.berlios.de>

Author: nouiz
Date: 2009-03-16 18:54:35 +0100 (Mon, 16 Mar 2009)
New Revision: 10026

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
optimization of 18% in my test by using pointeur.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-16 13:43:20 UTC (rev 10025)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-16 17:54:35 UTC (rev 10026)
@@ -224,20 +224,30 @@
     target.resize(reg.length());
     weight.resize(reg.length());
     value.resize(reg.length());
+    real * p = tsource.toMat()[col];
+    pair<real,real> * ptw = target_weight.data();
+    real * pt = target.data();
+    real * pw = weight.data();
+    real * pv = value.data();
     if(weightsize() <= 0){
         weight.fill(1.0 / length());
         for(int i=0;i<reg.length();i++){
-            target[i] = target_weight[int(reg[i])].first;
-            value[i]  = tsource->get(col, reg[i]);
+            PLASSERT(tsource->get(col, reg[i])==p[reg[i]]);
+            int idx = int(reg[i]);
+            pt[i] = ptw[idx].first;
+            pv[i] = p[idx];
         }
     } else {
         //It is better to do multiple pass for memory access.
         for(int i=0;i<reg.length();i++){
-            target[i] = target_weight[int(reg[i])].first;
-            weight[i] = target_weight[int(reg[i])].second;
+            int idx = int(reg[i]);
+            pt[i] = ptw[idx].first;
+            pw[i] = ptw[idx].second;
         }
-        for(int i=0;i<reg.length();i++)
-            value[i]  = tsource->get(col, reg[i]);
+        for(int i=0;i<reg.length();i++){
+            PLASSERT(tsource->get(col, reg[i])==p[reg[i]]);
+            pv[i] = p[reg[i]];
+        }
     }
 }
 
@@ -257,7 +267,7 @@
         //we are more memory friendly.
         for(int i=0;i<length() && n> idx;i++){
             PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
-            int srow = ptsorted_row[i];
+            RTR_type srow = ptsorted_row[i];
             if ( compact_reg[srow] ){
                 PLASSERT(leave_register[srow] == leave_id);
                 PLASSERT(preg[idx]==reg[idx]);
@@ -269,7 +279,7 @@
             compact_reg[i]=false;
         for(int i=0;i<length() && n> idx;i++){
             PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
-            int srow = ptsorted_row[i];
+            RTR_type srow = ptsorted_row[i];
             if ( pleave_register[srow] == leave_id){
                 PLASSERT(leave_register[srow] == leave_id);
                 PLASSERT(preg[idx]==reg[idx]);



From nouiz at mail.berlios.de  Mon Mar 16 19:03:23 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Mar 2009 19:03:23 +0100
Subject: [Plearn-commits] r10027 - trunk/plearn_learners/regressors
Message-ID: <200903161803.n2GI3NmN027949@sheep.berlios.de>

Author: nouiz
Date: 2009-03-16 19:03:22 +0100 (Mon, 16 Mar 2009)
New Revision: 10027

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
optimization.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-16 17:54:35 UTC (rev 10026)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-16 18:03:22 UTC (rev 10027)
@@ -229,24 +229,26 @@
     real * pt = target.data();
     real * pw = weight.data();
     real * pv = value.data();
+    RTR_type * preg = reg.data();
+
     if(weightsize() <= 0){
         weight.fill(1.0 / length());
         for(int i=0;i<reg.length();i++){
             PLASSERT(tsource->get(col, reg[i])==p[reg[i]]);
-            int idx = int(reg[i]);
+            int idx = int(preg[i]);
             pt[i] = ptw[idx].first;
             pv[i] = p[idx];
         }
     } else {
         //It is better to do multiple pass for memory access.
         for(int i=0;i<reg.length();i++){
-            int idx = int(reg[i]);
+            int idx = int(preg[i]);
             pt[i] = ptw[idx].first;
             pw[i] = ptw[idx].second;
         }
         for(int i=0;i<reg.length();i++){
             PLASSERT(tsource->get(col, reg[i])==p[reg[i]]);
-            pv[i] = p[reg[i]];
+            pv[i] = p[preg[i]];
         }
     }
 }



From nouiz at mail.berlios.de  Mon Mar 16 20:46:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Mar 2009 20:46:12 +0100
Subject: [Plearn-commits] r10028 - trunk/plearn_learners/regressors
Message-ID: <200903161946.n2GJkCpx008122@sheep.berlios.de>

Author: nouiz
Date: 2009-03-16 20:46:11 +0100 (Mon, 16 Mar 2009)
New Revision: 10028

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
remove useless warning.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-16 18:03:22 UTC (rev 10027)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-16 19:46:11 UTC (rev 10028)
@@ -328,7 +328,9 @@
 //     {
 //         sorted_row(each_train_sample_index).fill(each_train_sample_index);
 //     }
+#ifdef _OPENMP
 #pragma omp parallel for default(none) shared(pb)
+#endif
     for (int sample_dim = 0; sample_dim < inputsize(); sample_dim++)
     {
         sortEachDim(sample_dim);



From nouiz at mail.berlios.de  Mon Mar 16 22:48:07 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 16 Mar 2009 22:48:07 +0100
Subject: [Plearn-commits] r10029 - trunk/plearn_learners/regressors
Message-ID: <200903162148.n2GLm7p7015812@sheep.berlios.de>

Author: nouiz
Date: 2009-03-16 22:48:07 +0100 (Mon, 16 Mar 2009)
New Revision: 10029

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
code clean up.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-03-16 19:46:11 UTC (rev 10028)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-03-16 21:48:07 UTC (rev 10029)
@@ -239,7 +239,6 @@
     leave_output[0] = closest_value;
 }
 
-#define BY_ROW
 //#define RCMP
 void RegressionTreeNode::lookForBestSplit()
 {
@@ -315,26 +314,6 @@
         }
 
         missing_leave->getOutputAndError(tmp_vec, missing_error);
-
-#ifndef BY_ROW
-        //in case of missing value
-        if(candidate.size()==0){
-            PLCHECK(missing_leave->length()+right_leave->length()
-                    ==leave->length());
-            continue;
-        }
-        int row = candidate.pop();
-        while (candidate.size()>0)
-        {
-            int next_row = candidate.pop();
-            left_leave->removeRow(row, tmp_vec, left_error);
-            right_leave->addRow(row, tmp_vec, right_error);
-            compareSplit(col, train_set->get(next_row, col),
-                         train_set->get(row, col), left_error,
-                         right_error, missing_error);
-            row = next_row;
-        }
-#else
         tuple<real,real,int> ret=bestSplitInRow(col, candidate, left_error,
                                                 right_error, missing_error,
                                                 right_leave, left_leave,
@@ -355,7 +334,6 @@
         split_feature_value = get<1>(ret);
         split_balance = get<2>(ret);
         PLASSERT(fast_is_less(after_split_error,REAL_MAX)||split_col==-1);
-#endif
     }
     PLASSERT(fast_is_less(after_split_error,REAL_MAX)||split_col==-1);
 



From nouiz at mail.berlios.de  Tue Mar 17 20:22:21 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 17 Mar 2009 20:22:21 +0100
Subject: [Plearn-commits] r10030 - trunk/commands/PLearnCommands
Message-ID: <200903171922.n2HJML6U024843@sheep.berlios.de>

Author: nouiz
Date: 2009-03-17 20:22:20 +0100 (Tue, 17 Mar 2009)
New Revision: 10030

Modified:
   trunk/commands/PLearnCommands/plearn_main.cc
Log:
added the option --quiet


Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2009-03-16 21:48:07 UTC (rev 10029)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2009-03-17 19:22:20 UTC (rev 10030)
@@ -190,6 +190,7 @@
     // (verbosity_pos == -1)!!!
     int verbosity_pos                = findpos( command_line, "--verbosity"  );
     int verbosity_value_pos          = -1; // ... 
+    int quiet_pos                   = findpos( command_line, "--quiet" );
     VerbosityLevel verbosity_value   = VLEVEL_NORMAL;
 
     if ( verbosity_pos != -1 )
@@ -278,6 +279,7 @@
              c != global_calendar_pos        &&
              c != global_calendar_value_pos  &&
              c != servers_pos                &&
+             c != quiet_pos                  &&
              c != serversfile_pos            /*&&
              c != option_level_pos           &&
              c != option_level_value_pos*/
@@ -297,7 +299,7 @@
         }
     command_line.resize( cleaned ); // Truncating the end of the vector.
   
-    if (no_version_pos == -1){
+    if (no_version_pos == -1 && quiet_pos == -1){
         output_version( );
 #ifdef _OPENMP
         pout<<"Using OPENMP with "+tostring(omp_get_max_threads())+" threads."<<endl;
@@ -309,7 +311,7 @@
     if (windows_endl != -1)
         PStream::windows_endl = true;
 
-    if (enabled_modules_pos != -1)
+    if (enabled_modules_pos != -1 && quiet_pos == -1)
         perr << "Logging enabled for modules: "
              << join(PL_Log::instance().namedLogging(), ", ")
              << endl;



From islaja at mail.berlios.de  Tue Mar 17 21:23:31 2009
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Tue, 17 Mar 2009 21:23:31 +0100
Subject: [Plearn-commits] r10031 - trunk/plearn_learners/online
Message-ID: <200903172023.n2HKNVcv000892@sheep.berlios.de>

Author: islaja
Date: 2009-03-17 21:23:28 +0100 (Tue, 17 Mar 2009)
New Revision: 10031

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Fixed an error: added bias to layer's activation before calling fpropNLL. Only fpropNLL was using the wrong activation (consequently wrong expectation as well), since bpropNLL was using the expectation correctly computed by fprop. Then, despite the error, the gradients were correctly computed.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-03-17 19:22:20 UTC (rev 10030)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-03-17 20:23:28 UTC (rev 10031)
@@ -1484,6 +1484,7 @@
                                 layers[ index ]->expectation);
 
         layers[ index ]->activation << direct_and_reconstruction_activations;
+        layers[ index ]->activation += layers[ index ]->bias;
         //layers[ index ]->expectation_is_up_to_date = true;  // Won't work for certain RBMLayers
         layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
         train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
@@ -1511,6 +1512,7 @@
                                 layers[ index ]->expectation);
 
         layers[ index ]->activation << reconstruction_activations;
+        layers[ index ]->activation += layers[ index ]->bias;
         //layers[ index ]->expectation_is_up_to_date = true;
         layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
         real rec_err = layers[ index ]->fpropNLL(expectations[index]);
@@ -1526,6 +1528,7 @@
             layers[ index+1 ]->fprop( hidden_reconstruction_activations,
                 layers[ index+1 ]->expectation );
             layers[ index+1 ]->activation << hidden_reconstruction_activations;
+            layers[ index+1 ]->activation += layers[ index+1 ]->bias;
             //layers[ index+1 ]->expectation_is_up_to_date = true;
             layers[ index+1 ]->setExpectationByRef( layers[ index+1 ]->expectation );
             real hid_rec_err = layers[ index+1 ]->fpropNLL(expectations[index+1]);
@@ -1706,6 +1709,7 @@
 
     layers[ 0 ]->setExpectation( fine_tuning_reconstruction_expectations[ 0 ] );
     layers[ 0 ]->activation << fine_tuning_reconstruction_activations[0];
+    layers[ 0 ]->activation += layers[ 0 ]->bias;
     real rec_err = layers[ 0 ]->fpropNLL( input );
     train_costs[n_layers-1] = rec_err;
 
@@ -1999,6 +2003,7 @@
                               layers[ i-1 ]->expectation);
 
         layers[ i-1 ]->activation << reconstruction_activations;
+        layers[ i-1 ]->activation += layers[ i-1 ]->bias;
         //layers[ i-1 ]->expectation_is_up_to_date = true;
         layers[ i-1 ]->setExpectationByRef( layers[ i-1 ]->expectation );
         real rec_err = layers[ i-1 ]->fpropNLL( expectations[i-1] );
@@ -2337,6 +2342,7 @@
 
         layers[ i-1 ]->activations.resize(mbatch_size, layers[i-1]->size);
         layers[ i-1 ]->activations << reconstruction_activations_m;
+        layers[ i-1 ]->activations += layers[ i-1 ]->bias;
 
         Mat layer_exp = layers[i-1]->getExpectations();
         layers[ i-1 ]->fprop(reconstruction_activations_m,
@@ -2617,6 +2623,7 @@
                                 layers[ i ]->expectation);
 
             layers[ i ]->activation << reconstruction_activations;
+            layers[ i ]->activation += layers[ i ]->bias;
             //layers[ i ]->expectation_is_up_to_date = true;
             layers[ i ]->setExpectationByRef( layers[ i ]->expectation );
 
@@ -2651,6 +2658,8 @@
 
         layers[ currently_trained_layer-1 ]->activation <<
             reconstruction_activations;
+        layers[ currently_trained_layer-1 ]->activation += 
+            layers[ currently_trained_layer-1 ]->bias;
         //layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
         layers[ currently_trained_layer-1 ]->setExpectationByRef(
             layers[ currently_trained_layer-1 ]->expectation );
@@ -2668,6 +2677,8 @@
                 layers[ currently_trained_layer ]->expectation );
             layers[ currently_trained_layer ]->activation <<
                 hidden_reconstruction_activations;
+            layers[ currently_trained_layer ]->activation += 
+                layers[ currently_trained_layer ]->bias;
             //layers[ currently_trained_layer ]->expectation_is_up_to_date = true;
             layers[ currently_trained_layer ]->setExpectationByRef(
                 layers[ currently_trained_layer ]->expectation );



From nouiz at mail.berlios.de  Wed Mar 18 19:22:11 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Mar 2009 19:22:11 +0100
Subject: [Plearn-commits] r10032 - trunk/plearn_learners/regressors
Message-ID: <200903181822.n2IIMBNT000109@sheep.berlios.de>

Author: nouiz
Date: 2009-03-18 19:22:07 +0100 (Wed, 18 Mar 2009)
New Revision: 10032

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
-PP are not thread safe! So I remove their use in a section that is parallel.
-added macro RTR_target_t and RTR_weight_t to try to get some speed up from smaller memory foot print.
-desactivated one optimisation in RegressionTreeRegisters to use the variable compact_reg as this optimisation make stuff slower on boltzmann. This computer is closer to my target.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-17 20:23:28 UTC (rev 10031)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-18 18:22:07 UTC (rev 10032)
@@ -88,6 +88,7 @@
 {
     source = source_;
     tsource = tsource_;
+    tsource_mat = tsource.toMat();
     tsorted_row = tsorted_row_;
     build();
 }
@@ -187,6 +188,7 @@
             PP<MemoryVMatrixNoSave> tmp = new MemoryVMatrixNoSave(tsource);
             tsource = VMat(tmp);
         }
+        tsource_mat = tsource.toMat();
     }
     setMetaInfoFrom(source);
     weightsize_=1;
@@ -217,17 +219,18 @@
 }
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,
                                                   TVec<RTR_type> &reg,
-                                                  Vec &target,
-                                                  Vec &weight, Vec &value) const
+                                                  TVec<RTR_target_t> &target,
+                                                  TVec<RTR_weight_t> &weight,
+                                                  Vec &value) const
 {
     getAllRegisteredRow(leave_id,col,reg);
     target.resize(reg.length());
     weight.resize(reg.length());
     value.resize(reg.length());
-    real * p = tsource.toMat()[col];
-    pair<real,real> * ptw = target_weight.data();
-    real * pt = target.data();
-    real * pw = weight.data();
+    real * p = tsource_mat[col];
+    pair<RTR_target_t,RTR_weight_t> * ptw = target_weight.data();
+    RTR_target_t * pt = target.data();
+    RTR_weight_t * pw = weight.data();
     real * pv = value.data();
     RTR_type * preg = reg.data();
 
@@ -263,7 +266,7 @@
     RTR_type* preg = reg.data();
     RTR_type* ptsorted_row = tsorted_row[col];
     RTR_type_id* pleave_register = leave_register.data();
-    if(compact_reg_leave==leave_id){
+    if(false && compact_reg_leave==leave_id){
         //compact_reg is used as an optimization.
         //as it is more compact in memory then leave_register
         //we are more memory friendly.
@@ -277,8 +280,8 @@
             }
         }
     }else{
-        for(uint i=0;i<compact_reg.size();i++)
-            compact_reg[i]=false;
+//        for(uint i=0;i<compact_reg.size();i++)
+//            compact_reg[i]=false;
         for(int i=0;i<length() && n> idx;i++){
             PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
             RTR_type srow = ptsorted_row[i];
@@ -286,7 +289,7 @@
                 PLASSERT(leave_register[srow] == leave_id);
                 PLASSERT(preg[idx]==reg[idx]);
                 preg[idx++]=srow;
-                compact_reg[srow]=true;
+                //compact_reg[srow]=true;
             }
         }
         compact_reg_leave = leave_id;

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-03-17 20:23:28 UTC (rev 10031)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-03-18 18:22:07 UTC (rev 10032)
@@ -56,6 +56,15 @@
 #ifndef RTR_type_id
 #define RTR_type_id int16_t
 #endif
+//!The type for the target
+#ifndef RTR_target_t
+#define RTR_target_t real
+#endif
+//!The type for the weight
+#ifndef RTR_weight_t
+#define RTR_weight_t real
+#endif
+
 namespace PLearn {
 using namespace std;
 
@@ -81,9 +90,10 @@
     TMat<RTR_type> tsorted_row;
     TVec<RTR_type_id> leave_register;
     VMat tsource;
+    Mat tsource_mat;
     //we put it in pair instead of two vector to speed up
     //the getAllRegisteredRow(leave_id, col, reg, target, weight, value) fct
-    TVec<pair<real,real> > target_weight;
+    TVec<pair<RTR_target_t,RTR_weight_t> > target_weight;
     VMat source;
 
     bool do_sort_rows;
@@ -129,7 +139,7 @@
         next_id += 1;return next_id;}
     void         getAllRegisteredRow(RTR_type_id leave_id, int col, TVec<RTR_type> &reg)const;
     void         getAllRegisteredRow(RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
-                                     Vec &target, Vec &weight, Vec &value)const;
+                                     TVec<RTR_target_t> &target, TVec<RTR_weight_t> &weight, Vec &value)const;
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);
     inline virtual void put(int i, int j, real value)



From nouiz at mail.berlios.de  Wed Mar 18 19:45:47 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Mar 2009 19:45:47 +0100
Subject: [Plearn-commits] r10033 - trunk/plearn_learners/regressors
Message-ID: <200903181845.n2IIjlXI028621@sheep.berlios.de>

Author: nouiz
Date: 2009-03-18 19:45:45 +0100 (Wed, 18 Mar 2009)
New Revision: 10033

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
use correctly RTR_target_t RTR_weight_t.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-03-18 18:22:07 UTC (rev 10032)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-03-18 18:45:45 UTC (rev 10033)
@@ -246,8 +246,8 @@
         return;
     TVec<RTR_type> candidate(0, leave->length());//list of candidate row to split
     TVec<RTR_type> registered_row(leave->length());
-    Vec registered_target(0, leave->length()); 
-    Vec registered_weight(0, leave->length());
+    TVec<RTR_target_t> registered_target(0, leave->length()); 
+    TVec<RTR_weight_t> registered_weight(0, leave->length());
     Vec registered_value(0, leave->length());
     tmp_vec.resize(2);
     Vec left_error(3);
@@ -354,7 +354,7 @@
     PP<RegressionTreeLeave> right_leave,
     PP<RegressionTreeLeave> left_leave,
     PP<RegressionTreeRegisters> train_set,
-    Vec values, Vec targets, Vec weights
+    Vec values, TVec<RTR_target_t> targets, TVec<RTR_weight_t> weights
     )
 {
     int best_balance=INT_MAX;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2009-03-18 18:22:07 UTC (rev 10032)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2009-03-18 18:45:45 UTC (rev 10033)
@@ -136,7 +136,7 @@
                                                PP<RegressionTreeLeave> right_leave,
                                                PP<RegressionTreeLeave> left_leave,
                                                PP<RegressionTreeRegisters> train_set,
-                                               Vec values, Vec targets, Vec weights
+                                               Vec values, TVec<RTR_target_t> targets, TVec<RTR_weight_t> weights
         );
 };
 



From nouiz at mail.berlios.de  Wed Mar 18 20:55:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Mar 2009 20:55:36 +0100
Subject: [Plearn-commits] r10034 - in trunk: plearn/math
	plearn_learners/regressors
Message-ID: <200903181955.n2IJtaJY008174@sheep.berlios.de>

Author: nouiz
Date: 2009-03-18 20:55:35 +0100 (Wed, 18 Mar 2009)
New Revision: 10034

Modified:
   trunk/plearn/math/TMat_decl.h
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
-added fct TMat::copyColumnTo(int col,T*) 
I need this fct as PP are not threads safe and doing a RegressionTreeRegisters::getExample was doing seg fault for this reason. Making PP thread safe was too slow.
-Use this new fct to make RegressionTreeRegisters::getExample thread safe.


Modified: trunk/plearn/math/TMat_decl.h
===================================================================
--- trunk/plearn/math/TMat_decl.h	2009-03-18 18:45:45 UTC (rev 10033)
+++ trunk/plearn/math/TMat_decl.h	2009-03-18 19:55:35 UTC (rev 10034)
@@ -691,6 +691,17 @@
                 x[k] = row[j];
     }
 
+    //! copy a column to a C vector starting at x
+    //! This is fct is thead safe!
+    void copyColumnTo(int col, T* x) const
+    {
+        T* s = data()+col;
+        for(int i=0;i<length();i++){
+            *x=*s;
+            x++;
+            s+=mod();
+        }
+    }
     /*! ************
       Deep copying
       ************

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-18 18:45:45 UTC (rev 10033)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-18 19:55:35 UTC (rev 10034)
@@ -401,7 +401,7 @@
     if(weightsize()<0)
         PLERROR("In RegressionTreeRegisters::getExample, weightsize_ not defined for this vmat");
 #endif
-    tsource->getColumn(i,input);
+    tsource_mat.copyColumnTo(i,input.data());
 
     target[0]=target_weight[i].first;
     weight = target_weight[i].second;



From nouiz at mail.berlios.de  Wed Mar 18 21:47:57 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 18 Mar 2009 21:47:57 +0100
Subject: [Plearn-commits] r10035 - trunk/plearn_learners/regressors
Message-ID: <200903182047.n2IKlvvj015720@sheep.berlios.de>

Author: nouiz
Date: 2009-03-18 21:47:57 +0100 (Wed, 18 Mar 2009)
New Revision: 10035

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
added comment.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-18 19:55:35 UTC (rev 10034)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-18 20:47:57 UTC (rev 10035)
@@ -401,6 +401,8 @@
     if(weightsize()<0)
         PLERROR("In RegressionTreeRegisters::getExample, weightsize_ not defined for this vmat");
 #endif
+    //going by tsource is not thread safe as PP are not thread safe.
+    //so we use tsource_mat.copyColumnTo that is thread safe.
     tsource_mat.copyColumnTo(i,input.data());
 
     target[0]=target_weight[i].first;



From nouiz at mail.berlios.de  Thu Mar 19 15:02:24 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Mar 2009 15:02:24 +0100
Subject: [Plearn-commits] r10036 - trunk/plearn/base
Message-ID: <200903191402.n2JE2OuJ019720@sheep.berlios.de>

Author: nouiz
Date: 2009-03-19 15:02:24 +0100 (Thu, 19 Mar 2009)
New Revision: 10036

Modified:
   trunk/plearn/base/stringutils.cc
   trunk/plearn/base/stringutils.h
Log:
added two fct:

/*! splits a string into a list of substrings (using any occurence of
  the given delimiters as split point)
*/
vector<string> split_all(const string& s, const string& delimiters=" \t\n\r");

 /*! Split a string at deliminer while allowing a delimiter to be quoted so that it is not considered to be as a delimiter.
   The double_quote are only considered at the boundary of the field.
   The function should execute in O(n+k) where n is the number of character in s and k is the number of field in k.

vector<string> split_quoted_delimiter(const string& s, const string& delimiter,
                                      const string& double_quote);




Modified: trunk/plearn/base/stringutils.cc
===================================================================
--- trunk/plearn/base/stringutils.cc	2009-03-18 20:47:57 UTC (rev 10035)
+++ trunk/plearn/base/stringutils.cc	2009-03-19 14:02:24 UTC (rev 10036)
@@ -366,9 +366,12 @@
 
 vector<string> split_quoted_delimiter(const string& s, char delimiter,
                                       const string& double_quote){
-    int delim_size=double_quote.size();
-    if(delim_size==1)
+    int quote_size=double_quote.size();
+    if(quote_size==1)
         return split_quoted_delimiter(s,delimiter,double_quote[0]);
+    else if(quote_size==0)
+        return split(s,delimiter);
+    PLASSERT(double_quote.size()>0);
 
     vector<string> ret = split(s, delimiter);
     vector<string> ret2;
@@ -376,14 +379,14 @@
         bool bw=string_begins_with(ret[i],double_quote);
         bool ew=string_ends_with(ret[i],double_quote);
         if(bw && ew){
-            ret2.push_back(ret[i].substr(delim_size,
-                                         ret[i].size()-delim_size)); 
+            ret2.push_back(ret[i].substr(quote_size,
+                                         ret[i].size()-quote_size)); 
         }else if(bw){
-            string tmp=ret[i].substr(delim_size);
+            string tmp=ret[i].substr(quote_size);
             tmp+=delimiter;
             for(uint j=i+1;j<ret.size();j++){
                 if(string_ends_with(ret[j],double_quote)){
-                    tmp+=ret[j].substr(0,ret[j].size()-delim_size);
+                    tmp+=ret[j].substr(0,ret[j].size()-quote_size);
                     ret2.push_back(tmp);
                     i=j;
                     break;
@@ -425,9 +428,68 @@
             ret2.push_back(f);
     }
     return ret2;
-    
 }
 
+vector<string> split_quoted_delimiter(const string& s, const string& delimiters,
+                                      const string& double_quote)
+{
+    int quote_size=double_quote.size();
+    if(quote_size==1 && delimiters.size()==1)
+        return split_quoted_delimiter(s,delimiters[0],double_quote[0]);
+    else if(delimiters.size()==1 && quote_size==0)
+        return split(s,delimiters[0]);
+    PLASSERT(delimiters.size()>0);
+    vector<string> ret;
+
+    size_t startpos = 0;
+    size_t endpos = 0;
+
+    for(;;)
+    {
+        startpos = endpos;
+        bool quoted = string_begins_with(s.substr(startpos),double_quote);
+        if(quoted){
+            startpos+=quote_size;
+            endpos = s.find(double_quote,startpos);
+        }else
+            endpos = s.find_first_of(delimiters,startpos);
+        if(endpos==string::npos)
+        {
+            ret.push_back(s.substr(startpos));
+            break;
+        }
+        ret.push_back(s.substr(startpos,endpos-startpos));
+        if(quoted)
+            startpos+=quote_size;
+        else
+            endpos++;
+    }
+    return ret;
+}
+
+vector<string> split_all(const string& s, const string& delimiters)
+{
+    vector<string> result;
+
+    size_t startpos = 0;
+    size_t endpos = 0;
+
+    for(;;)
+    {
+        startpos = endpos;
+        endpos = s.find_first_of(delimiters,startpos);
+        if(endpos==string::npos)
+        {
+            result.push_back(s.substr(startpos));
+            break;
+        }
+        result.push_back(s.substr(startpos,endpos-startpos));
+        endpos++;
+    }
+
+    return result;
+}
+
 vector<string> split(const string& s, const string& delimiters, bool keep_delimiters)
 {
     vector<string> result;

Modified: trunk/plearn/base/stringutils.h
===================================================================
--- trunk/plearn/base/stringutils.h	2009-03-18 20:47:57 UTC (rev 10035)
+++ trunk/plearn/base/stringutils.h	2009-03-19 14:02:24 UTC (rev 10036)
@@ -169,6 +169,11 @@
 */
 vector<string> split(const string& s, const string& delimiters=" \t\n\r", bool keepdelimiters=false);
 
+/*! splits a string into a list of substrings (using any occurence of 
+  the given delimiters as split point)
+*/
+vector<string> split_all(const string& s, const string& delimiters=" \t\n\r");
+
 /*! Split a string at deliminer while allowing a delimiter to be quoted so that it is not considered to be as a delimiter.
   The double_quote are only considered at the boundary of the field.
   The function should execute in O(n+k) where n is the number of character in s and k is the number of field in k.
@@ -184,6 +189,9 @@
 vector<string> split_quoted_delimiter(const string& s, char delimiter,
                                       char double_quote);
 
+vector<string> split_quoted_delimiter(const string& s, const string& delimiters,
+                                      const string& double_quote);
+
 /*!     Split the string on the first occurence of a delimiter and returns 
   what was left of the delimitor and what was right of it.
   If no delimitor character is found, the original string is returned 



From nouiz at mail.berlios.de  Thu Mar 19 15:23:44 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Mar 2009 15:23:44 +0100
Subject: [Plearn-commits] r10037 - trunk/plearn/vmat
Message-ID: <200903191423.n2JENiiq021991@sheep.berlios.de>

Author: nouiz
Date: 2009-03-19 15:23:43 +0100 (Thu, 19 Mar 2009)
New Revision: 10037

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
allow many delimiter to be declared. We use any of them as a delimiter.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-19 14:02:24 UTC (rev 10036)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-19 14:23:43 UTC (rev 10037)
@@ -460,10 +460,6 @@
         buildVMatrixStringMapping();
 
     // Sanity checking
-    if (delimiter.size() != 1)
-        PLERROR("In TextFilesVMatrix::setMetaDataDir - the 'delimiter' option"
-                " '%s' must contain exactly one character",
-                delimiter.c_str());
 }
 
 
@@ -641,7 +637,7 @@
 
 TVec<string> TextFilesVMatrix::splitIntoFields(const string& raw_row) const
 {
-    return split_quoted_delimiter(removeblanks(raw_row), delimiter[0],quote_delimiter);
+    return split_quoted_delimiter(removeblanks(raw_row), delimiter,quote_delimiter);
 }
 
 TVec<string> TextFilesVMatrix::getTextFields(int i) const



From nouiz at mail.berlios.de  Thu Mar 19 15:31:20 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Mar 2009 15:31:20 +0100
Subject: [Plearn-commits] r10038 - trunk/plearn_learners/cgi
Message-ID: <200903191431.n2JEVK8O022667@sheep.berlios.de>

Author: nouiz
Date: 2009-03-19 15:31:20 +0100 (Thu, 19 Mar 2009)
New Revision: 10038

Modified:
   trunk/plearn_learners/cgi/ConfigParsing.cc
Log:
allow , and ; as delimiter in csv file.


Modified: trunk/plearn_learners/cgi/ConfigParsing.cc
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-19 14:23:43 UTC (rev 10037)
+++ trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-19 14:31:20 UTC (rev 10038)
@@ -74,7 +74,7 @@
     input.default_spec="char";
 //#auto_extend_map = 0  ;
     input.build_vmatrix_stringmap = 1  ;
-    input.delimiter = ","  ;//TODO ; or auto?
+    input.delimiter = ",;"  ;//TODO ; or auto?
     input.quote_delimiter = '"';
     input.skipheader.append(1);
     input.reorder_fieldspec_from_headers=1;



From nouiz at mail.berlios.de  Thu Mar 19 16:45:18 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Mar 2009 16:45:18 +0100
Subject: [Plearn-commits] r10039 - trunk/plearn_learners/meta
Message-ID: <200903191545.n2JFjIPt031092@sheep.berlios.de>

Author: nouiz
Date: 2009-03-19 16:45:18 +0100 (Thu, 19 Mar 2009)
New Revision: 10039

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
don't duplicate data in memory for no good reason.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-19 14:31:20 UTC (rev 10038)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-19 15:45:18 UTC (rev 10039)
@@ -608,7 +608,7 @@
                                                 t1->getTSortedRow(),
                                                 t1->getTSource(),
                                                 learner1->report_progress,
-                                                learner1->verbosity,false);
+                                                learner1->verbosity,false,false);
         }
         learner2->setTrainingSet(vmat2, call_forget);
     }



From nouiz at mail.berlios.de  Thu Mar 19 19:07:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Mar 2009 19:07:39 +0100
Subject: [Plearn-commits] r10040 - trunk/plearn_learners/regressors
Message-ID: <200903191807.n2JI7d30003291@sheep.berlios.de>

Author: nouiz
Date: 2009-03-19 19:07:38 +0100 (Thu, 19 Mar 2009)
New Revision: 10040

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
lower memory requirement.If mem_tsource is false, we would generate a full Mat to tsource_mat. That is not needed for test.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-19 15:45:18 UTC (rev 10039)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-19 18:07:38 UTC (rev 10040)
@@ -88,7 +88,8 @@
 {
     source = source_;
     tsource = tsource_;
-    tsource_mat = tsource.toMat();
+    if(tsource->classname()=="MemoryVMatrixNoSave")
+        tsource_mat = tsource.toMat();
     tsorted_row = tsorted_row_;
     build();
 }
@@ -188,7 +189,8 @@
             PP<MemoryVMatrixNoSave> tmp = new MemoryVMatrixNoSave(tsource);
             tsource = VMat(tmp);
         }
-        tsource_mat = tsource.toMat();
+        if(tsource->classname()=="MemoryVMatrixNoSave")
+            tsource_mat = tsource.toMat();
     }
     setMetaInfoFrom(source);
     weightsize_=1;
@@ -223,6 +225,8 @@
                                                   TVec<RTR_weight_t> &weight,
                                                   Vec &value) const
 {
+    PLASSERT(tsource_mat.length()==tsource.length());
+
     getAllRegisteredRow(leave_id,col,reg);
     target.resize(reg.length());
     weight.resize(reg.length());
@@ -260,6 +264,7 @@
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,
                                                   TVec<RTR_type> &reg) const
 {
+    PLASSERT(tsource_mat.length()==tsource.length());
 
     int idx=0;
     int n=reg.length();



From nouiz at mail.berlios.de  Thu Mar 19 20:43:10 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Mar 2009 20:43:10 +0100
Subject: [Plearn-commits] r10041 - trunk/plearn_learners/cgi
Message-ID: <200903191943.n2JJhAJw028302@sheep.berlios.de>

Author: nouiz
Date: 2009-03-19 20:43:08 +0100 (Thu, 19 Mar 2009)
New Revision: 10041

Modified:
   trunk/plearn_learners/cgi/ConfigParsing.cc
Log:
-regenerate the files only if they are not up to date
-some code cleanup.


Modified: trunk/plearn_learners/cgi/ConfigParsing.cc
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-19 18:07:38 UTC (rev 10040)
+++ trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-19 19:43:08 UTC (rev 10041)
@@ -60,21 +60,19 @@
 //! The actual implementation of the 'ConfigParsing' command
 void ConfigParsing::run(const vector<string>& args)
 {
-    // *** PLEASE COMPLETE HERE ****
-/*    args1 = conf/conf.all.csv;
-    args2 = conf/1convertCSV0709toPLearn.inc;
-    args3 = conf/3b_remove_col.inc;
-    args4 = conf/3fix_missing.inc;
-    args5 = conf/9dichotomize.inc;
-    args6 = conf/global_imputation_specifications.inc;
+/*    args0 = conf/conf.all.csv;
+    args1 = conf/1convertCSV0709toPLearn.inc;
+    args2 = conf/3b_remove_col.inc;
+    args3 = conf/3fix_missing.inc;
+    args4 = conf/9dichotomize.inc;
+    args5 = conf/global_imputation_specifications.inc;
 */
     PLCHECK(args.size()==6);
     TextFilesVMatrix input = TextFilesVMatrix();
     input.auto_build_map = 0  ;
     input.default_spec="char";
-//#auto_extend_map = 0  ;
     input.build_vmatrix_stringmap = 1  ;
-    input.delimiter = ",;"  ;//TODO ; or auto?
+    input.delimiter = ",;"  ;
     input.quote_delimiter = '"';
     input.skipheader.append(1);
     input.reorder_fieldspec_from_headers=1;
@@ -82,6 +80,16 @@
     input.partial_match=1;
     input.setMetaDataDir(args[0]+".metadatadir");
     input.build();
+    bool all_uptodate = true;
+    for(int i=1;i<=5;i++)
+        if(!input.isUpToDate(args[i])){
+            all_uptodate = false;
+            break;
+        }
+    if(all_uptodate){
+        pout << "All file are uptodate. We don't regenerate the.";
+        return;
+    }
     PStream f_csv = openFile(PPath(args[1]),PStream::raw_ascii,"w");
     PStream f_remove = openFile(args[2],PStream::raw_ascii,"w");
     PStream f_missing = openFile(args[3],PStream::raw_ascii,"w");
@@ -127,8 +135,6 @@
         if(!r[5].empty() && !remove){
             f_dichotomize <<r[0]<<" : ["<< (r[5]) << " ]"<<endl;
         }
-
-        
     }
     f_csv<<"$INCLUDE{conf/date_undef.inc}"<<endl;
     f_remove<<"$INCLUDE{conf/date_undef.inc}"<<endl;
@@ -136,8 +142,6 @@
     f_dichotomize<<"$INCLUDE{conf/date_undef.inc}"<<endl;
     f_imputation<<"$INCLUDE{conf/date_undef.inc}"<<endl;
 
-        
-
 }
 
 } // end of namespace PLearn



From nouiz at mail.berlios.de  Thu Mar 19 21:42:23 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 19 Mar 2009 21:42:23 +0100
Subject: [Plearn-commits] r10042 - in trunk: commands/PLearnCommands
	plearn/misc
Message-ID: <200903192042.n2JKgN7e004006@sheep.berlios.de>

Author: nouiz
Date: 2009-03-19 21:42:23 +0100 (Thu, 19 Mar 2009)
New Revision: 10042

Modified:
   trunk/commands/PLearnCommands/VMatCommand.cc
   trunk/plearn/misc/vmatmain.cc
Log:
added the option
vmat convert srt dst --update
we generate the destination only when the SOURCE file is newer than the destination  file or when the destination file is missing



Modified: trunk/commands/PLearnCommands/VMatCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatCommand.cc	2009-03-19 19:43:08 UTC (rev 10041)
+++ trunk/commands/PLearnCommands/VMatCommand.cc	2009-03-19 20:42:23 UTC (rev 10042)
@@ -80,6 +80,8 @@
         "       such as 0-18, or any combination thereof, e.g. 5,3,8-18,Date,74-85\n"
         "       If the option --mat_to_mem is specified, we load the original matrix into memory\n"
         "       If the option --save_vmat is specified, we save the source vmat in the destination metadatadir\n"
+        "       If the option --update is specified, we generate the <destination> only when the <source> file is newer\n"
+        "         then the destination file or when the destination file is missing\n"
         "       If .csv (Comma-Separated Value) is specified as the destination file, the \n"
         "       following additional options are also supported:\n"
         "         --skip-missings: if a row (after selecting the appropriate columns) contains\n"

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2009-03-19 19:43:08 UTC (rev 10041)
+++ trunk/plearn/misc/vmatmain.cc	2009-03-19 20:42:23 UTC (rev 10042)
@@ -660,7 +660,10 @@
          *           :: load the source vmat in memory before saving
          *     --save_vmat 
          *           :: if the source is a vmat, we serialize the constructed
-         *           ::object in the metadatadir of the destination
+         *           :: object in the metadatadir of the destination
+         *     --update
+         *           :: we generate the <destination> only when the <source> file is newer than
+         *           :: the destination  file or when the destination file is missing
          */
         TVec<string> columns;
         TVec<string> date_columns;
@@ -669,6 +672,7 @@
         string delimiter = ",";
         bool convert_date = false;
         bool save_vmat = false;
+        bool update = false;
         for (int i=4 ; i < argc && argv[i] ; ++i) {
             string curopt = removeblanks(argv[i]);
             if (curopt == "")
@@ -695,6 +699,8 @@
                 mat_to_mem = true;
             else if (curopt == "--save_vmat")
                 save_vmat = true;
+            else if (curopt == "--update")
+                update = true;
             else
                 PLWARNING("VMat convert: unrecognized option '%s'; ignoring it...",
                           curopt.c_str());
@@ -713,7 +719,9 @@
                       ext.c_str());
         if(mat_to_mem)
             vm.precompute();
-        if(ext==".amat")
+        if(update && vm->isUpToDate(destination))
+            pout << "The file is up to date. We don't regenerate it."<<endl;
+        else if(ext==".amat")
             // Save strings as strings so they are not lost.
             vm->saveAMAT(destination, true, false, true);
         else if(ext==".pmat")



From nouiz at mail.berlios.de  Fri Mar 20 17:58:41 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Mar 2009 17:58:41 +0100
Subject: [Plearn-commits] r10043 - trunk/plearn_learners/cgi
Message-ID: <200903201658.n2KGwf3C002858@sheep.berlios.de>

Author: nouiz
Date: 2009-03-20 17:58:41 +0100 (Fri, 20 Mar 2009)
New Revision: 10043

Modified:
   trunk/plearn_learners/cgi/ConfigParsing.cc
Log:
removed some include.


Modified: trunk/plearn_learners/cgi/ConfigParsing.cc
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-19 20:42:23 UTC (rev 10042)
+++ trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-20 16:58:41 UTC (rev 10043)
@@ -96,11 +96,6 @@
     PStream f_dichotomize = openFile(args[4],PStream::raw_ascii,"w");
     PStream f_imputation = openFile(args[5],PStream::raw_ascii,"w");
 
-    f_csv<<"$INCLUDE{conf/date.inc}"<<endl;
-    f_remove<<"$INCLUDE{conf/date.inc}"<<endl;
-    f_missing<<"$INCLUDE{conf/date.inc}"<<endl;
-    f_dichotomize<<"$INCLUDE{conf/date.inc}"<<endl;
-    f_imputation<<"$INCLUDE{conf/date.inc}"<<endl;
     for(int i=0;i<input.length();i++){
         TVec<string> r = input.getTextFields(i);
         char c = r[0][0];
@@ -136,12 +131,6 @@
             f_dichotomize <<r[0]<<" : ["<< (r[5]) << " ]"<<endl;
         }
     }
-    f_csv<<"$INCLUDE{conf/date_undef.inc}"<<endl;
-    f_remove<<"$INCLUDE{conf/date_undef.inc}"<<endl;
-    f_missing<<"$INCLUDE{conf/date_undef.inc}"<<endl;
-    f_dichotomize<<"$INCLUDE{conf/date_undef.inc}"<<endl;
-    f_imputation<<"$INCLUDE{conf/date_undef.inc}"<<endl;
-
 }
 
 } // end of namespace PLearn



From nouiz at mail.berlios.de  Fri Mar 20 19:39:33 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 20 Mar 2009 19:39:33 +0100
Subject: [Plearn-commits] r10044 - trunk/plearn/base
Message-ID: <200903201839.n2KIdX3X019990@sheep.berlios.de>

Author: nouiz
Date: 2009-03-20 19:39:33 +0100 (Fri, 20 Mar 2009)
New Revision: 10044

Modified:
   trunk/plearn/base/stringutils.cc
Log:
bugfixed new fct split_quoted_delimiter(const string& s, const string& delimiters,const string& double_quote)


Modified: trunk/plearn/base/stringutils.cc
===================================================================
--- trunk/plearn/base/stringutils.cc	2009-03-20 16:58:41 UTC (rev 10043)
+++ trunk/plearn/base/stringutils.cc	2009-03-20 18:39:33 UTC (rev 10044)
@@ -459,10 +459,12 @@
             break;
         }
         ret.push_back(s.substr(startpos,endpos-startpos));
-        if(quoted)
-            startpos+=quote_size;
-        else
+        if(quoted){
+            endpos+=quote_size+1;
+        }else
             endpos++;
+        if(endpos>s.size())
+            break;
     }
     return ret;
 }



From nouiz at mail.berlios.de  Mon Mar 23 16:23:57 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Mar 2009 16:23:57 +0100
Subject: [Plearn-commits] r10045 - trunk/plearn_learners/meta
Message-ID: <200903231523.n2NFNviY009524@sheep.berlios.de>

Author: nouiz
Date: 2009-03-23 16:23:57 +0100 (Mon, 23 Mar 2009)
New Revision: 10045

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
In the learner MultiClassAdaBoost
-renamed some variable to be more clear.
-added some documentation
-added some test cost


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-20 18:39:33 UTC (rev 10044)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-23 15:23:57 UTC (rev 10045)
@@ -67,9 +67,9 @@
     done_warn_once_target_gt_2(false),
     timer(new PTimer(true)),
     time_sum(0),
-    time_sum_rtr(0),
+    time_sum_ft(0),
     time_last_stage(0),
-    time_last_stage_rtr(0),
+    time_last_stage_ft(0),
     last_stage(0),
     forward_sub_learner_test_costs(false),
     forward_test(0)
@@ -123,7 +123,7 @@
                   OptionBase::buildoption,
                   "if 0, default test. If 1 forward the test fct to the sub"
                   " learner. If 2, determine at each stage what is the faster"
-                  " based on past  test time.\n");
+                  " based on past test time.\n");
 
     declareOption(ol, "train_time",
                   &MultiClassAdaBoost::train_time, 
@@ -162,24 +162,34 @@
     declareOption(ol, "time_sum",
                   &MultiClassAdaBoost::time_sum, 
                   OptionBase::learntoption|OptionBase::nosave,
-                  "The time spend in test() during the last stage. If test() is called multiple time for the same stage this is the sum of the time.");
-    declareOption(ol, "time_sum_rtr",
-                  &MultiClassAdaBoost::time_sum_rtr, 
+                  "The time spend in test() during the last stage if"
+                  " forward_test==2 and we use the inhereted::test fct."
+                  " If test() is called multiple time for the same stage"
+                  " this is the sum of the time.");
+    declareOption(ol, "time_sum_ft",
+                  &MultiClassAdaBoost::time_sum_ft, 
                   OptionBase::learntoption|OptionBase::nosave,
-                  "The time spend in test() during the last stage. If test() is called multiple time for the same stage this is the sum of the time.");
+                  "The time spend in test() during the last stage if"
+                  " forward_test is 1 or 2 and we forward the test to the"
+                  " subleaner. If test() is called multiple time for the same"
+                  " stage this is the sum of the time.");
     declareOption(ol, "time_last_stage",
                   &MultiClassAdaBoost::time_last_stage, 
                   OptionBase::learntoption|OptionBase::nosave,
-                  "");
-    declareOption(ol, "time_last_stage_rtr",
-                  &MultiClassAdaBoost::time_last_stage_rtr, 
+                  "This is the last value of time_sum in the last stage.");
+    declareOption(ol, "time_last_stage_ft",
+                  &MultiClassAdaBoost::time_last_stage_ft, 
                   OptionBase::learntoption|OptionBase::nosave,
-                  "");
+                  "This is the last value of time_sum_ft in the last stage.");
     declareOption(ol, "last_stage",
                   &MultiClassAdaBoost::last_stage, 
                   OptionBase::learntoption |OptionBase::nosave,
-                  "The stage at witch time_sum was used");
- }
+                  "The stage at witch time_sum or time_sum_ft was used");
+    declareOption(ol, "last_stage",
+                  &MultiClassAdaBoost::last_stage, 
+                  OptionBase::learntoption |OptionBase::nosave,
+                  "The stage at witch time_sum or time_sum_ft was used");
+}
 
 void MultiClassAdaBoost::build_()
 {
@@ -423,6 +433,11 @@
         costs[8]=total_train_time;
         costs[9]=test_time;
         costs[10]=total_test_time;
+        costs[11]=time_sum;
+        costs[12]=time_sum_ft;
+        costs[13]=time_last_stage;
+        costs[14]=time_last_stage_ft;
+        costs[15]=last_stage;
     }else{
         costs[7]=costs[8]=costs[9]=costs[10]=MISSING_VALUE;
     }
@@ -469,8 +484,14 @@
     costs[8]=total_train_time;
     costs[9]=test_time;
     costs[10]=total_test_time;
+    costs[11]=time_sum;
+    costs[12]=time_sum_ft;
+    costs[13]=time_last_stage;
+    costs[14]=time_last_stage_ft;
+    costs[15]=last_stage;
+
     if(forward_sub_learner_test_costs){
-        costs.resize(7+4);
+        costs.resize(7+4+5);
 	PLASSERT(sub_costs1.size()==learner1->nTestCosts() || sub_costs1.size()==0);
 	PLASSERT(sub_costs2.size()==learner2->nTestCosts() || sub_costs2.size()==0);
 
@@ -519,6 +540,12 @@
     names.append("total_train_time");
     names.append("test_time");
     names.append("total_test_time");
+    names.append("time_sum");
+    names.append("time_sum_ft");
+    names.append("time_last_stage");
+    names.append("time_last_stage_ft");
+    names.append("last_stage");
+
     if(forward_sub_learner_test_costs){
         TVec<string> subcosts=learner1->getTestCostNames();
         for(int i=0;i<subcosts.length();i++){
@@ -637,13 +664,13 @@
         time_last_stage=time_sum;
         time_sum=0;
     }
-    if(last_stage<stage && time_sum_rtr>0){
-        time_last_stage_rtr=time_sum_rtr;
-        time_sum_rtr=0;
+    if(last_stage<stage && time_sum_ft>0){
+            time_last_stage_ft=time_sum_ft;
+        time_sum_ft=0;
     }
 
-    if(forward_test==2 && time_last_stage<time_last_stage_rtr){
-        EXTREME_MODULE_LOG<<"inherited start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+    if(forward_test==2 && time_last_stage<time_last_stage_ft){
+        EXTREME_MODULE_LOG<<"inherited start time_sum="<<time_sum<<" time_sum_ft="<<time_sum_ft<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_ft=" <<time_last_stage_ft<<endl;
         timer->resetTimer("MultiClassAdaBoost::test() current");
         timer->startTimer("MultiClassAdaBoost::test() current");
         PLCHECK(last_stage<=stage);
@@ -653,10 +680,10 @@
         Profiler::pl_profile_end("MultiClassAdaBoost::test()");
         time_sum += timer->getTimer("MultiClassAdaBoost::test() current");
         last_stage=stage;
-        EXTREME_MODULE_LOG<<"inherited end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+        EXTREME_MODULE_LOG<<"inherited end time_sum="<<time_sum<<" time_sum_ft="<<time_sum_ft<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_ft=" <<time_last_stage_ft<<endl;
         return;
     }
-    EXTREME_MODULE_LOG<<"start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+    EXTREME_MODULE_LOG<<"start time_sum="<<time_sum<<" time_sum_ft="<<time_sum_ft<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_ft=" <<time_last_stage_ft<<endl;
     timer->resetTimer("MultiClassAdaBoost::test() current");
     timer->startTimer("MultiClassAdaBoost::test() current");
     //Profiler::pl_profile_start("MultiClassAdaBoost::test() part1");//cheap
@@ -700,9 +727,6 @@
         PLCHECK(((PP<ProcessingVMatrix>)testset2)->source==testset);
     }
 
-    if (test_stats){
-        
-    }
     //Profiler::pl_profile_end("MultiClassAdaBoost::test() part1");//cheap
     Profiler::pl_profile_start("MultiClassAdaBoost::test() subtest");
     learner1->test(testset1,test_stats1,testoutputs1,testcosts1);
@@ -803,11 +827,11 @@
     timer->stopTimer("MultiClassAdaBoost::test()");
     Profiler::pl_profile_end("MultiClassAdaBoost::test()");
     
-    time_sum_rtr +=timer->getTimer("MultiClassAdaBoost::test() current");
+    time_sum_ft +=timer->getTimer("MultiClassAdaBoost::test() current");
 
 
     last_stage=stage;
-    EXTREME_MODULE_LOG<<"end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+    EXTREME_MODULE_LOG<<"end time_sum="<<time_sum<<" time_sum_ft="<<time_sum_ft<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_ft=" <<time_last_stage_ft<<endl;
 
 }
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-03-20 18:39:33 UTC (rev 10044)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-03-23 15:23:57 UTC (rev 10045)
@@ -90,10 +90,12 @@
 
     PP<PTimer> timer;
 
+    //Variable used when forward_test==2.
+    //They are used to determine if inherited::test is faster then forwarding the test fct.
     mutable real time_sum;
-    mutable real time_sum_rtr;
+    mutable real time_sum_ft;
     mutable real time_last_stage;
-    mutable real time_last_stage_rtr;
+    mutable real time_last_stage_ft;
     mutable int last_stage;
 
 public:



From nouiz at mail.berlios.de  Mon Mar 23 18:24:41 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Mar 2009 18:24:41 +0100
Subject: [Plearn-commits] r10046 - in trunk/plearn_learners/meta: .
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata
Message-ID: <200903231724.n2NHOfqS024757@sheep.berlios.de>

Author: nouiz
Date: 2009-03-23 18:24:40 +0100 (Mon, 23 Mar 2009)
New Revision: 10046

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
Log:
bugfix in the last commit and updated the test.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-23 15:23:57 UTC (rev 10045)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-23 17:24:40 UTC (rev 10046)
@@ -156,7 +156,7 @@
 
     declareOption(ol, "done_warn_once_target_gt_2",
                   &MultiClassAdaBoost::done_warn_once_target_gt_2,
-                  OptionBase::learntoption,
+                  OptionBase::learntoption|OptionBase::nosave,
                   "Used to keep track if we have done the warning or not.");
 
     declareOption(ol, "time_sum",
@@ -442,7 +442,7 @@
         costs[7]=costs[8]=costs[9]=costs[10]=MISSING_VALUE;
     }
     if(forward_sub_learner_test_costs){
-        costs.resize(7+4);
+        costs.resize(7+4+5);
         subcosts1+=subcosts2;
         costs.append(subcosts1);
     }

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-03-23 15:23:57 UTC (rev 10045)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-03-23 17:24:40 UTC (rev 10046)
@@ -924,11 +924,8 @@
 finalized = 0  )
 ;
 forward_test = 0 ;
-train_time = 0 ;
-total_train_time = 0 ;
-test_time = 0 ;
-total_test_time = 0 ;
-time_costs = 0  )
+time_costs = 0 ;
+warn_once_target_gt_2 = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -998,6 +995,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *5  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2009-03-23 15:23:57 UTC (rev 10045)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2009-03-23 17:24:40 UTC (rev 10046)
@@ -9,6 +9,11 @@
 total_train_time	0
 test_time	0
 total_test_time	0
+time_sum	0
+time_sum_ft	0
+time_last_stage	0
+time_last_stage_ft	0
+last_stage	0
 sum_sublearner.binary_class_error	0
 sum_sublearner.exp_neg_margin	0
 sum_sublearner.class_error	0

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2009-03-23 15:23:57 UTC (rev 10045)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2009-03-23 17:24:40 UTC (rev 10046)
@@ -9,6 +9,11 @@
 total_train_time	0
 test_time	0
 total_test_time	0
+time_sum	0
+time_sum_ft	0
+time_last_stage	0
+time_last_stage_ft	0
+last_stage	0
 sum_sublearner.binary_class_error	0
 sum_sublearner.exp_neg_margin	0
 sum_sublearner.class_error	0

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-03-23 15:23:57 UTC (rev 10045)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-03-23 17:24:40 UTC (rev 10046)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL9903"
+__REVISION__ = "PL10045"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/test_cost_names.txt	2009-03-23 15:23:57 UTC (rev 10045)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/test_cost_names.txt	2009-03-23 17:24:40 UTC (rev 10046)
@@ -9,6 +9,11 @@
 total_train_time
 test_time
 total_test_time
+time_sum
+time_sum_ft
+time_last_stage
+time_last_stage_ft
+last_stage
 sum_sublearner.binary_class_error
 sum_sublearner.exp_neg_margin
 sum_sublearner.class_error

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-03-23 15:23:57 UTC (rev 10045)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-03-23 17:24:40 UTC (rev 10046)
@@ -313,11 +313,8 @@
 finalized = 0  )
 ;
 forward_test = 0 ;
-train_time = 0 ;
-total_train_time = 0 ;
-test_time = 0 ;
-total_test_time = 0 ;
-time_costs = 0  )
+time_costs = 0 ;
+warn_once_target_gt_2 = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -388,6 +385,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *6  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;



From nouiz at mail.berlios.de  Mon Mar 23 18:57:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Mar 2009 18:57:39 +0100
Subject: [Plearn-commits] r10047 - in trunk/plearn_learners/meta: .
	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
Message-ID: <200903231757.n2NHvdKC017378@sheep.berlios.de>

Author: nouiz
Date: 2009-03-23 18:57:35 +0100 (Mon, 23 Mar 2009)
New Revision: 10047

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
Log:
another bugfix.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-23 17:24:40 UTC (rev 10046)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-23 17:57:35 UTC (rev 10047)
@@ -440,6 +440,7 @@
         costs[15]=last_stage;
     }else{
         costs[7]=costs[8]=costs[9]=costs[10]=MISSING_VALUE;
+        costs[11]=costs[12]=costs[13]=costs[14]=costs[15]=MISSING_VALUE;
     }
     if(forward_sub_learner_test_costs){
         costs.resize(7+4+5);

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)



From nouiz at mail.berlios.de  Mon Mar 23 19:06:55 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Mar 2009 19:06:55 +0100
Subject: [Plearn-commits] r10048 - trunk/plearn_learners/meta
Message-ID: <200903231806.n2NI6tGB024677@sheep.berlios.de>

Author: nouiz
Date: 2009-03-23 19:06:53 +0100 (Mon, 23 Mar 2009)
New Revision: 10048

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
-Made the test() fct of MultiClassAdaBoost parallel
-Made the option forward_test==2 of MultiClassAdaBoost be more resistent to the sublearner timing of the test() fct. In some case the first time it is executed, it take more then then the following one. So we don't consider the first.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-23 17:57:35 UTC (rev 10047)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-03-23 18:06:53 UTC (rev 10048)
@@ -71,6 +71,7 @@
     time_last_stage(0),
     time_last_stage_ft(0),
     last_stage(0),
+    nb_sequential_ft(0),
     forward_sub_learner_test_costs(false),
     forward_test(0)
 /* ### Initialize all fields to their default value here */
@@ -189,7 +190,13 @@
                   &MultiClassAdaBoost::last_stage, 
                   OptionBase::learntoption |OptionBase::nosave,
                   "The stage at witch time_sum or time_sum_ft was used");
-}
+    declareOption(ol, "nb_sequential_ft",
+                  &MultiClassAdaBoost::nb_sequential_ft, 
+                  OptionBase::learntoption |OptionBase::nosave,
+                  "The number of sequential time that we forward the test()"
+                  " fct. We must do that as the first time we forward it, the"
+                  " time is higher then the following ones.");
+ }
 
 void MultiClassAdaBoost::build_()
 {
@@ -244,8 +251,10 @@
     deepCopyField(output2,           copies);
     deepCopyField(subcosts1,         copies);
     deepCopyField(subcosts2,         copies);
+    deepCopyField(timer,             copies);
     deepCopyField(learner1,          copies);
     deepCopyField(learner2,          copies);
+
     //not needed as we only read it.
     //deepCopyField(learner_template,  copies);
 }
@@ -666,7 +675,9 @@
         time_sum=0;
     }
     if(last_stage<stage && time_sum_ft>0){
+        if(nb_sequential_ft>0)
             time_last_stage_ft=time_sum_ft;
+        nb_sequential_ft++;
         time_sum_ft=0;
     }
 
@@ -681,6 +692,7 @@
         Profiler::pl_profile_end("MultiClassAdaBoost::test()");
         time_sum += timer->getTimer("MultiClassAdaBoost::test() current");
         last_stage=stage;
+        nb_sequential_ft = 0;
         EXTREME_MODULE_LOG<<"inherited end time_sum="<<time_sum<<" time_sum_ft="<<time_sum_ft<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_ft=" <<time_last_stage_ft<<endl;
         return;
     }
@@ -730,8 +742,18 @@
 
     //Profiler::pl_profile_end("MultiClassAdaBoost::test() part1");//cheap
     Profiler::pl_profile_start("MultiClassAdaBoost::test() subtest");
+#ifdef _OPENMP
+#pragma omp parallel sections
+{
+#pragma omp section 
     learner1->test(testset1,test_stats1,testoutputs1,testcosts1);
+#pragma omp section 
     learner2->test(testset2,test_stats2,testoutputs2,testcosts2);
+}
+#else
+    learner1->test(testset1,test_stats1,testoutputs1,testcosts1);
+    learner2->test(testset2,test_stats2,testoutputs2,testcosts2);
+#endif
     Profiler::pl_profile_end("MultiClassAdaBoost::test() subtest");
 
     VMat my_outputs = 0;

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-03-23 17:57:35 UTC (rev 10047)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-03-23 18:06:53 UTC (rev 10048)
@@ -97,6 +97,7 @@
     mutable real time_last_stage;
     mutable real time_last_stage_ft;
     mutable int last_stage;
+    mutable int nb_sequential_ft;
 
 public:
     //#####  Public Build Options  ############################################



From nouiz at mail.berlios.de  Mon Mar 23 19:37:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Mar 2009 19:37:39 +0100
Subject: [Plearn-commits] r10049 - trunk/plearn/vmat
Message-ID: <200903231837.n2NIbdhS005846@sheep.berlios.de>

Author: nouiz
Date: 2009-03-23 19:37:37 +0100 (Mon, 23 Mar 2009)
New Revision: 10049

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
when we load old stats of a VMatrix from a file, if the lenght()!=width() we where generating an error. Now we generate a Warning and recompute the stats.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2009-03-23 18:06:53 UTC (rev 10048)
+++ trunk/plearn/vmat/VMatrix.cc	2009-03-23 18:37:37 UTC (rev 10049)
@@ -1640,12 +1640,15 @@
     try{
         if (uptodate){
             PLearn::load(statsfile, stats);
-            if(stats.length()!=width())
-                PLERROR("In VMatrix::getPrecomputedStatsFromFile() for class %s -"
-                        " bad file %s. Delete it to have it recreated.",
+            if(stats.length()!=width()){
+                uptodate=false;
+                PLWARNING("In VMatrix::getPrecomputedStatsFromFile() for class"
+                          " %s - The file %s don't have the good number of"
+                          " stats. We regenerate it.",
                         classname().c_str(), statsfile.c_str());
-        } else
-        {
+            }
+        }
+        if(!uptodate){
             VMat vm = const_cast<VMatrix*>(this);
             stats = PLearn::computeStats(vm, maxnvalues, progress_bar);
             if(!metadatadir.isEmpty())



From nouiz at mail.berlios.de  Mon Mar 23 21:31:07 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 23 Mar 2009 21:31:07 +0100
Subject: [Plearn-commits] r10050 - trunk/python_modules/plearn/pytest
Message-ID: <200903232031.n2NKV7Yh019288@sheep.berlios.de>

Author: nouiz
Date: 2009-03-23 21:31:07 +0100 (Mon, 23 Mar 2009)
New Revision: 10050

Modified:
   trunk/python_modules/plearn/pytest/modes.py
Log:
added a show option for --force, -f


Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2009-03-23 18:37:37 UTC (rev 10049)
+++ trunk/python_modules/plearn/pytest/modes.py	2009-03-23 20:31:07 UTC (rev 10050)
@@ -777,7 +777,7 @@
         ogroups = super(results, cls).option_groups(parser)
 
         ogroups[1].add_option(
-            '--force', default=False,
+            '-f', '--force', default=False,
             action="store_true",
             help="If True, new results will be generated without prompting, even "
             "if results already are under version control." )



From nouiz at mail.berlios.de  Tue Mar 24 15:21:17 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 24 Mar 2009 15:21:17 +0100
Subject: [Plearn-commits] r10051 - trunk/plearn_learners/cgi
Message-ID: <200903241421.n2OELHad013056@sheep.berlios.de>

Author: nouiz
Date: 2009-03-24 15:21:17 +0100 (Tue, 24 Mar 2009)
New Revision: 10051

Modified:
   trunk/plearn_learners/cgi/StabilisationLearner.cc
   trunk/plearn_learners/cgi/StabilisationLearner.h
Log:
added name to the output columns.


Modified: trunk/plearn_learners/cgi/StabilisationLearner.cc
===================================================================
--- trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-03-23 20:31:07 UTC (rev 10050)
+++ trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-03-24 14:21:17 UTC (rev 10051)
@@ -152,6 +152,12 @@
     return names;
 }
 
+TVec<string> StabilisationLearner::getOutputNames() const
+{
+    TVec<string> names(1);
+    names[0]="predicted_class";
+    return names;
+}
 
 } // end of namespace PLearn
 

Modified: trunk/plearn_learners/cgi/StabilisationLearner.h
===================================================================
--- trunk/plearn_learners/cgi/StabilisationLearner.h	2009-03-23 20:31:07 UTC (rev 10050)
+++ trunk/plearn_learners/cgi/StabilisationLearner.h	2009-03-24 14:21:17 UTC (rev 10051)
@@ -106,8 +106,8 @@
     //! and for which it updates the VecStatsCollector train_stats.
     virtual TVec<std::string> getTrainCostNames() const;
 
+    virtual TVec<std::string> getOutputNames() const;
 
-
     //#####  PLearn::Object Protocol  #########################################
 
     PLEARN_DECLARE_OBJECT(StabilisationLearner);



From chrish at mail.berlios.de  Tue Mar 24 21:05:23 2009
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 24 Mar 2009 21:05:23 +0100
Subject: [Plearn-commits] r10052 - trunk
Message-ID: <200903242005.n2OK5NA2024962@sheep.berlios.de>

Author: chrish
Date: 2009-03-24 21:05:22 +0100 (Tue, 24 Mar 2009)
New Revision: 10052

Modified:
   trunk/pymake.config.model
Log:
Update ApSTAT config for new numpy version installed locally.

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-03-24 14:21:17 UTC (rev 10051)
+++ trunk/pymake.config.model	2009-03-24 20:05:22 UTC (rev 10052)
@@ -346,9 +346,10 @@
         python_version = '2.5'
         optionargs += [ 'python%s' % python_version.replace('.', '') ]
         python_lib_root = '/usr/lib'
-        numpy_site_packages = '-L/usr/lib/python' + python_version + '/site-packages/numarray'
+        numpy_site_packages = '-L/usr/local/lib/python' + python_version + '/site-packages/numpy'
         python_includedirs = [ '/usr/include/python' + python_version]
-        numpy_includedirs = python_includedirs
+        numpy_includedirs = ['/usr/local/lib/python2.5/site-packages/numpy/core/include',
+                             '/usr/local/stow/python2.5-numpy/lib/python2.5/site-packages/numpy/numarray/numpy'] + python_includedirs
     elif domain_name.endswith('iro.umontreal.ca'):
         optionargs += [ pyoption ]
         python_version = pyver



From nouiz at mail.berlios.de  Wed Mar 25 15:57:32 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Mar 2009 15:57:32 +0100
Subject: [Plearn-commits] r10053 - trunk/commands/PLearnCommands
Message-ID: <200903251457.n2PEvWQN013199@sheep.berlios.de>

Author: nouiz
Date: 2009-03-25 15:57:32 +0100 (Wed, 25 Mar 2009)
New Revision: 10053

Modified:
   trunk/commands/PLearnCommands/VMatCommand.cc
Log:
added option --precision to: vmat cat <dataset>... [--precision=N] [<optional_vpl_filtering_code>]


Modified: trunk/commands/PLearnCommands/VMatCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatCommand.cc	2009-03-24 20:05:22 UTC (rev 10052)
+++ trunk/commands/PLearnCommands/VMatCommand.cc	2009-03-25 14:57:32 UTC (rev 10053)
@@ -62,7 +62,7 @@
         "       To display statistics for that field \n"
         "   or: vmat bbox <dataset> [<extra_percent>] \n"
         "       To display the data bounding box (i.e., for each field, its min and max, possibly extended by +-extra_percent ex: 0.10 for +-10% of the data range )\n"
-        "   or: vmat cat <dataset>... [<optional_vpl_filtering_code>]\n"
+        "   or: vmat cat <dataset>... [--precision=N] [<optional_vpl_filtering_code>]\n"
         "       To display the dataset \n"
         "   or: vmat sascat <dataset.vmat> <dataset.txt>\n"
         "       To output in <dataset.txt> the dataset in SAS-like tab-separated format with field names on the first line\n"



From nouiz at mail.berlios.de  Wed Mar 25 16:14:53 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Mar 2009 16:14:53 +0100
Subject: [Plearn-commits] r10054 - trunk/plearn/misc
Message-ID: <200903251514.n2PFErHu015624@sheep.berlios.de>

Author: nouiz
Date: 2009-03-25 16:14:51 +0100 (Wed, 25 Mar 2009)
New Revision: 10054

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
missing file in last commit.


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2009-03-25 14:57:32 UTC (rev 10053)
+++ trunk/plearn/misc/vmatmain.cc	2009-03-25 15:14:51 UTC (rev 10054)
@@ -997,16 +997,26 @@
     else if(command=="cat")
     {
         if(argc < 3)
-            PLERROR("'vmat cat' must be used that way : vmat cat FILE... [vplFilteringCode]");
+            PLERROR("'vmat cat' must be used that way : vmat cat FILE... [--precision=N] [vplFilteringCode]");
         string code;
         int nb_file=argc-2;
-        if(argc>=4)
-        {
-            if(!isfile(argv[argc-1])){
+        int precision = -1;
+        for (int i=argc-1 ; i >=3 && argv[i] ; i--) {
+            string curopt = removeblanks(argv[i]);
+            if(curopt.substr(0,12) == "--precision="){
+                precision = toint(curopt.substr(12));
+                nb_file--;
+            }else if(!isfile(argv[argc-1])){
                 code=argv[argc-1];
                 nb_file--;
             }
         }
+        if(precision>0){
+            char tmpbuf[100];
+            snprintf(tmpbuf,100,"%%#.%df",precision);
+            pout.setDoubleFormat(tmpbuf);
+            pout.setFloatFormat(tmpbuf);
+        }
         for(int file=0;file<nb_file;file++)
         {
             string dbname=argv[file+2];



From nouiz at mail.berlios.de  Wed Mar 25 18:19:51 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Mar 2009 18:19:51 +0100
Subject: [Plearn-commits] r10055 - in trunk/plearn_learners: hyper testers
Message-ID: <200903251719.n2PHJplf001882@sheep.berlios.de>

Author: nouiz
Date: 2009-03-25 18:19:51 +0100 (Wed, 25 Mar 2009)
New Revision: 10055

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperLearner.h
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
modified how we reload an auto_saved learner to don't make an assignment to this.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2009-03-25 15:14:51 UTC (rev 10054)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2009-03-25 17:19:51 UTC (rev 10055)
@@ -66,8 +66,6 @@
     "They are accessible in the higher level PTester as: \n"
     "for ex: E[test1.E[mse]]");
 
-bool HyperLearner::reloading = false;
-
 TVec<string> HyperLearner::getTrainCostNames() const
 {
     if (strategy.size() > 0)
@@ -139,10 +137,7 @@
 
     declareOption(ol, "reloaded", &HyperLearner::reloaded,
                   OptionBase::learntoption|OptionBase::nosave,
-        "Used internally to avoid reloading a file, since the build function\n"
-        "may be called many times after the expdir is set,in particular in\n"
-        "PLearn::HyperLearner::setTrainingSet.");
-
+                  "Used to give a warning.");
     inherited::declareOptions(ol);
 
     // Hide some unused options.
@@ -306,7 +301,6 @@
     for(int commandnum=0; commandnum<strategy.length(); commandnum++)
         strategy[commandnum]->setHyperLearner(this);
 
-    auto_load();
 }
 
 /////////
@@ -374,45 +368,16 @@
     PLearn::save(tmp, this);
 
 #ifdef BOUNDCHECK
-    HyperLearner n;
+    HyperLearner *n = new HyperLearner();
     PLearn::load(tmp,n);
-    PLCHECK(PLearn::diff(&n,this));
+    PLCHECK(PLearn::diff(n,this));
+    delete n;
 #endif
 
     mvforce(tmp,f);
     Profiler::pl_profile_end("HyperLearner::auto_save");
 }
 
-///////////////
-// auto_load //
-///////////////
-void HyperLearner::auto_load()
-{
-    // Reload the saved HyperLearner. Note that becase the boolean 'reloading'
-    // is static, it means we can currently reload only one single
-    // HyperLearner at a time. It may be interesting in the future to change
-    // this reload mechanism to let us reload multiple (chained) HyperLearners.
-    if(expdir.isEmpty()){
-        if(verbosity>1)
-            DBG_MODULE_LOG<<"auto_load() - no expdir. Can't reload."<<endl;
-        return;
-    }
-    PPath f = expdir/"hyper_learner_auto_save.psave";
-    bool isf=isfile(f);
-    if(stage==0 && !reloading && !reloaded && isf){
-        Profiler::pl_profile_start("HyperLearner::auto_load");
-        if(verbosity>0)
-            PLWARNING("In HyperLearner::auto_load() - reloading from file %s",f.c_str());
-        reloading = true;
-        PLearn::load(f,*this);
-        reloading = false;
-        reloaded = true;
-        Profiler::pl_profile_end("HyperLearner::auto_load");
-    }
-    else if(!isf && verbosity>1)
-        PLWARNING("In HyperLearner::auto_load() - no file to reload.");
-}
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/hyper/HyperLearner.h
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.h	2009-03-25 15:14:51 UTC (rev 10054)
+++ trunk/plearn_learners/hyper/HyperLearner.h	2009-03-25 17:19:51 UTC (rev 10055)
@@ -75,8 +75,7 @@
 
     bool provide_strategy_expdir; //!< should each strategy step be provided a directory expdir/Step#
     bool save_final_learner; //!< should final learner be saved in expdir/final_learner.psave
-    bool reloaded; //!< Need to don't reload each time build is called as it can be called many time.
-    static bool reloading;
+    bool reloaded; //!< needed for a warning
     // HyperLearner methods
 
     //! if true, we finalize the learner after training.
@@ -116,9 +115,6 @@
     //! Save the current HyperLearner in its expdir
     void auto_save();
 
-    //! Load the previously saved HyperLearner in its expdir if available
-    void auto_load();
-
 }; // class HyperLearner
 
 DECLARE_OBJECT_PTR(HyperLearner);

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2009-03-25 15:14:51 UTC (rev 10054)
+++ trunk/plearn_learners/testers/PTester.cc	2009-03-25 17:19:51 UTC (rev 10055)
@@ -47,6 +47,7 @@
 #include <plearn/vmat/MemoryVMatrix.h>
 #include <assert.h>
 #include <plearn/base/RemoteDeclareMethod.h>
+#include <plearn_learners/hyper/HyperLearner.h>
 
 #include <plearn/misc/PLearnService.h>
 
@@ -85,6 +86,7 @@
 // PTester //
 /////////////
 PTester::PTester():
+       reloaded(false),
        need_to_save_test_names(false),
        provide_learner_expdir(false),
        report_stats(true),
@@ -354,6 +356,30 @@
     if (PLMPI::rank!=0)
         expdir = "";
 #endif
+
+    if(!reloaded && learner && learner->classname()=="HyperLearner"){
+        if(expdir.isEmpty()){
+            PLWARNING("PTester::build_() - no expdir. Can't reload.");
+            return;
+        }
+        PPath f = expdir/"Split0"/"LearnerExpdir"/"hyper_learner_auto_save.psave";
+        bool isf=isfile(f);
+        if(!reloaded && isf){
+            if(splitter->nsplits()!=1){
+                PLERROR("In PTester::build_() - The auto_save function only work when their is one split.");
+                //TODO: this only work if we have only one split
+            }
+            Profiler::pl_profile_start("PTester::auto_load");
+            PLWARNING("In PTester::build_() - reloading from file %s",f.c_str());
+            HyperLearner *l = new HyperLearner();
+            PLearn::load(f,l);
+            l->reloaded=true;
+            learner=l;
+            reloaded = true;
+            Profiler::pl_profile_end("PTester::auto_load");
+        }
+    }
+
     statnames_processed.resize(statnames.length());
     statnames_processed << statnames;
     if (statmask) {

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2009-03-25 15:14:51 UTC (rev 10054)
+++ trunk/plearn_learners/testers/PTester.h	2009-03-25 17:19:51 UTC (rev 10055)
@@ -60,7 +60,7 @@
     //! It is private because it is safer to access stats from getStatNames,
     //! since the 'statmask' option may modify the stats.
     TVec<string> statnames;
-
+    bool reloaded;
 protected:
 
     //! The 'real' statnames: these are obtained from 'statnames' by a



From nouiz at mail.berlios.de  Wed Mar 25 18:42:43 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Mar 2009 18:42:43 +0100
Subject: [Plearn-commits] r10056 - trunk/plearn/vmat
Message-ID: <200903251742.n2PHghOI013028@sheep.berlios.de>

Author: nouiz
Date: 2009-03-25 18:42:42 +0100 (Wed, 25 Mar 2009)
New Revision: 10056

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
print warning and log in a more compact way.


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2009-03-25 17:19:51 UTC (rev 10055)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2009-03-25 17:42:42 UTC (rev 10056)
@@ -293,6 +293,7 @@
     if (min_non_missing_threshold > 0){
         int min_non_missing =
             int(round(min_non_missing_threshold * the_train_source->length()));
+        TVec<int> have_missing;
         for (int i = 0; i < is; i++){
             if (stats[i].nnonmissing() >= min_non_missing 
                 && stats[i].nnonmissing() > 0)
@@ -303,13 +304,21 @@
                           source->fieldName(i).c_str(),
                           int(stats[i].nmissing()),
                           int(stats[i].n()));
-            if (info_var_with_missing && stats[i].nmissing() > 0)
-                MODULE_LOG<<"INFO: In build_() var '"
-                          <<source->fieldName(i).c_str()
-                          <<"' have missing value: "
-                          <<stats[i].nmissing()
-                          <<"/"<< stats[i].n()<<"."<<endl;
+            if (info_var_with_missing && stats[i].nmissing() > 0){
+                have_missing.append(i);
+            }
         }
+        if(have_missing.length()>0){
+            string s="INFO: In build_() variable with missing value (var,nb_missing/nb_value): ";
+            for(int i=0;i<have_missing.length();i++){
+                int ii = have_missing[i];
+                s+=" ("+source->fieldName(ii)
+                    +","+tostring(stats[ii].nmissing())
+                    +"/"+ tostring(stats[ii].n())+")";
+
+            }
+            MODULE_LOG<<s<<endl;
+        }
                 
     } else
         for (int i = 0; i < is; i++)
@@ -327,12 +336,12 @@
                 const_indices.append(i);
         }
         if(warn_removed_var && const_indices.length()>0){
-            NORMAL_LOG<<" WARNING: In VariableDeletionVMatrix::build_() - The following tuple (variable, constant value) indicate variable that are removed because they are constant: " <<endl;
+            string s = " WARNING: In VariableDeletionVMatrix::build_() - The following tuple (variable, constant value) indicate variable that are removed because they are constant: \n";
             for(int i=0;i<const_indices.length();i++){
                 StatsCollector stat = stats[i];
-                NORMAL_LOG<<"("<<source->fieldName(i)<<","<<stat.min()<<"),";
+                s+=" ("+source->fieldName(i)+","+tostring(stat.min())+")";
             }
-            NORMAL_LOG<<endl;
+            NORMAL_LOG<<s<<endl;
         }
         indices.resize(final_indices.length());
         indices << final_indices; 



From nouiz at mail.berlios.de  Wed Mar 25 20:48:52 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Mar 2009 20:48:52 +0100
Subject: [Plearn-commits] r10057 - trunk/doc
Message-ID: <200903251948.n2PJmq0Q031361@sheep.berlios.de>

Author: nouiz
Date: 2009-03-25 20:48:52 +0100 (Wed, 25 Mar 2009)
New Revision: 10057

Modified:
   trunk/doc/machine_learning.tex
Log:
fixed link in the doc.


Modified: trunk/doc/machine_learning.tex
===================================================================
--- trunk/doc/machine_learning.tex	2009-03-25 17:42:42 UTC (rev 10056)
+++ trunk/doc/machine_learning.tex	2009-03-25 19:48:52 UTC (rev 10057)
@@ -89,7 +89,7 @@
 \chapter{Boltzmann Machines and Deep Belief Networks}
 
 The equations can be seen on
-\url{http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Neurones/DBNEquations}
+\url{http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DBNEquations}
 if you have an account on LISA's TWiki.
 
 All the code files are located in {\tt



From nouiz at mail.berlios.de  Wed Mar 25 20:50:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Mar 2009 20:50:12 +0100
Subject: [Plearn-commits] r10058 - trunk/doc
Message-ID: <200903251950.n2PJoCHD031532@sheep.berlios.de>

Author: nouiz
Date: 2009-03-25 20:50:11 +0100 (Wed, 25 Mar 2009)
New Revision: 10058

Modified:
   trunk/doc/machine_learning.tex
Log:
updated text with the url.


Modified: trunk/doc/machine_learning.tex
===================================================================
--- trunk/doc/machine_learning.tex	2009-03-25 19:48:52 UTC (rev 10057)
+++ trunk/doc/machine_learning.tex	2009-03-25 19:50:11 UTC (rev 10058)
@@ -89,8 +89,7 @@
 \chapter{Boltzmann Machines and Deep Belief Networks}
 
 The equations can be seen on
-\url{http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DBNEquations}
-if you have an account on LISA's TWiki.
+\url{http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DBNEquations}.
 
 All the code files are located in {\tt
 \$PLEARNDIR/plearn\_learners/online}.



From nouiz at mail.berlios.de  Wed Mar 25 21:30:30 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 25 Mar 2009 21:30:30 +0100
Subject: [Plearn-commits] r10059 - trunk/plearn/vmat
Message-ID: <200903252030.n2PKUUOu004241@sheep.berlios.de>

Author: nouiz
Date: 2009-03-25 21:30:28 +0100 (Wed, 25 Mar 2009)
New Revision: 10059

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
fixed warning and changed indices name to be more consistent in the file.


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2009-03-25 19:50:11 UTC (rev 10058)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2009-03-25 20:30:28 UTC (rev 10059)
@@ -310,11 +310,11 @@
         }
         if(have_missing.length()>0){
             string s="INFO: In build_() variable with missing value (var,nb_missing/nb_value): ";
-            for(int i=0;i<have_missing.length();i++){
-                int ii = have_missing[i];
-                s+=" ("+source->fieldName(ii)
-                    +","+tostring(stats[ii].nmissing())
-                    +"/"+ tostring(stats[ii].n())+")";
+            for(int k=0;k<have_missing.length();k++){
+                int i = have_missing[k];
+                s+=" ("+source->fieldName(i)
+                    +","+tostring(stats[i].nmissing())
+                    +"/"+ tostring(stats[i].n())+")";
 
             }
             MODULE_LOG<<s<<endl;
@@ -337,7 +337,8 @@
         }
         if(warn_removed_var && const_indices.length()>0){
             string s = " WARNING: In VariableDeletionVMatrix::build_() - The following tuple (variable, constant value) indicate variable that are removed because they are constant: \n";
-            for(int i=0;i<const_indices.length();i++){
+            for(int k=0;k<const_indices.length();k++){
+                int i = const_indices[k];
                 StatsCollector stat = stats[i];
                 s+=" ("+source->fieldName(i)+","+tostring(stat.min())+")";
             }



From nouiz at mail.berlios.de  Thu Mar 26 16:09:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Mar 2009 16:09:39 +0100
Subject: [Plearn-commits] r10060 - in trunk/plearn_learners:
 meta/test/AdaBoost
 meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir
 meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0
 meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir
 meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0
 meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir
 meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0
 meta/test/MultiClassAdaBoost
 meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir
 meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
 regressors regressors/test/RegressionTree
 regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
 regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
 regressors/test/Re!
 gressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir
 regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir
Message-ID: <200903261509.n2QF9dqx018529@sheep.berlios.de>

Author: nouiz
Date: 2009-03-26 16:09:37 +0100 (Thu, 26 Mar 2009)
New Revision: 10060

Modified:
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/AdaBoost/PL_AdaBoost_template.pyplearn
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_fast.pyplearn
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_outputs.pyplearn
Log:
-Modified RegressionTree so that the output is completly determined by the leave.
  -The Tree just pass it around. 
  -This will allow other type of leave in the futur.
  -Updated the tests


Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -260,7 +260,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *9 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -282,7 +283,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *11 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -304,7 +306,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -332,7 +335,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -354,7 +358,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -382,7 +387,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -414,7 +420,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *21 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -436,7 +443,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -464,7 +472,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -526,7 +535,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -548,7 +558,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *30 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -570,7 +581,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -598,7 +610,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -628,7 +641,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *36 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -650,7 +664,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *38 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -672,7 +687,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -700,7 +716,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -730,7 +747,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -792,7 +810,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *47 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -814,7 +833,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *49 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -836,7 +856,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *51 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -858,7 +879,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -886,7 +908,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *55 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -908,7 +931,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -936,7 +960,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -968,7 +993,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -998,7 +1024,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1058,7 +1085,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *66 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1080,7 +1108,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *68 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1102,7 +1131,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *70 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1124,7 +1154,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1152,7 +1183,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1182,7 +1214,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1212,7 +1245,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *78 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1234,7 +1268,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1262,7 +1297,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1324,7 +1360,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *85 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1346,7 +1383,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1374,7 +1412,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *89 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1396,7 +1435,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1424,7 +1464,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *93 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1446,7 +1487,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1474,7 +1516,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *97 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1496,7 +1539,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1524,7 +1568,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1585,7 +1630,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -1709,6 +1755,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *5  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/experiment.plearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/experiment.plearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -23,7 +23,7 @@
             weak_learner_template = *3 -> RegressionTree(
                 complexity_penalty_factor = 0.0,
                 compute_train_stats = 0,
-                leave_template = *2 -> RegressionTreeLeave( ),
+                leave_template = *2 -> RegressionTreeLeave( output_confidence_target = 1 ),
                 loss_function_weight = 1,
                 maximum_number_of_nodes = 5,
                 missing_is_valid = 0,

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2009-03-26 15:09:37 UTC (rev 10060)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9866"
+__REVISION__ = "PL10047"
 conf                                          = False
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -68,7 +68,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -193,6 +194,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *6  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -260,7 +260,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *9 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -282,7 +283,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *11 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -304,7 +306,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -332,7 +335,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -354,7 +358,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -382,7 +387,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -414,7 +420,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *21 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -436,7 +443,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -464,7 +472,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -526,7 +535,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -548,7 +558,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *30 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -570,7 +581,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -598,7 +610,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -628,7 +641,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *36 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -650,7 +664,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *38 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -672,7 +687,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -700,7 +716,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -730,7 +747,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -792,7 +810,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *47 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -814,7 +833,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *49 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -836,7 +856,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *51 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -858,7 +879,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -886,7 +908,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *55 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -908,7 +931,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -936,7 +960,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -968,7 +993,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -998,7 +1024,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1058,7 +1085,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *66 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1080,7 +1108,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *68 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1102,7 +1131,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *70 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1124,7 +1154,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1152,7 +1183,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1182,7 +1214,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1212,7 +1245,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *78 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1234,7 +1268,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1262,7 +1297,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1324,7 +1360,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *85 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1346,7 +1383,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1374,7 +1412,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *89 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1396,7 +1435,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1424,7 +1464,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *93 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1446,7 +1487,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1474,7 +1516,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *97 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1496,7 +1539,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1524,7 +1568,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1585,7 +1630,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -1709,6 +1755,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *5  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/experiment.plearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/experiment.plearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -23,7 +23,7 @@
             weak_learner_template = *3 -> RegressionTree(
                 complexity_penalty_factor = 0.0,
                 compute_train_stats = 0,
-                leave_template = *2 -> RegressionTreeLeave( ),
+                leave_template = *2 -> RegressionTreeLeave( output_confidence_target = 1 ),
                 loss_function_weight = 1,
                 maximum_number_of_nodes = 5,
                 missing_is_valid = 0,

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2009-03-26 15:09:37 UTC (rev 10060)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9866"
+__REVISION__ = "PL10047"
 conf                                          = True
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -68,7 +68,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -193,6 +194,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *6  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -260,7 +260,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *9 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -282,7 +283,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *11 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -304,7 +306,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -332,7 +335,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -354,7 +358,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -382,7 +387,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -414,7 +420,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *21 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -436,7 +443,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -464,7 +472,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -526,7 +535,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -548,7 +558,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *30 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -570,7 +581,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -598,7 +610,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -628,7 +641,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *36 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -650,7 +664,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *38 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -672,7 +687,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -700,7 +716,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -730,7 +747,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -792,7 +810,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *47 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -814,7 +833,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *49 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -836,7 +856,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *51 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -858,7 +879,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -886,7 +908,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *55 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -908,7 +931,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -936,7 +960,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -968,7 +993,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -998,7 +1024,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1058,7 +1085,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *66 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1080,7 +1108,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *68 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1102,7 +1131,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *70 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1124,7 +1154,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1152,7 +1183,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1182,7 +1214,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1212,7 +1245,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *78 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1234,7 +1268,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1262,7 +1297,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1324,7 +1360,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *85 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1346,7 +1383,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1374,7 +1412,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *89 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1396,7 +1435,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1424,7 +1464,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *93 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1446,7 +1487,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1474,7 +1516,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *97 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -1496,7 +1539,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1524,7 +1568,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -1585,7 +1630,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -1709,6 +1755,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *5  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/experiment.plearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/experiment.plearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -23,7 +23,7 @@
             weak_learner_template = *3 -> RegressionTree(
                 complexity_penalty_factor = 0.0,
                 compute_train_stats = 0,
-                leave_template = *2 -> RegressionTreeLeave( ),
+                leave_template = *2 -> RegressionTreeLeave( output_confidence_target = 1 ),
                 loss_function_weight = 1,
                 maximum_number_of_nodes = 5,
                 missing_is_valid = 0,

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2009-03-26 15:09:37 UTC (rev 10060)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9866"
+__REVISION__ = "PL10047"
 conf                                          = False
 pseudo                                        = True

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -68,7 +68,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -193,6 +194,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *6  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/PL_AdaBoost_template.pyplearn
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/PL_AdaBoost_template.pyplearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/AdaBoost/PL_AdaBoost_template.pyplearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -66,7 +66,7 @@
         complexity_penalty_factor = 0.0,
         verbosity = 2,
         report_progress = 0,
-        leave_template = pl.RegressionTreeLeave( )
+        leave_template = pl.RegressionTreeLeave(output_confidence_target = 1)
         )
 )
 

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -277,7 +277,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *10 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -299,7 +300,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *12 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -321,7 +323,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -349,7 +352,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *16 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -371,7 +375,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -399,7 +404,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -431,7 +437,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -486,7 +493,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -572,7 +580,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -594,7 +603,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *30 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -616,7 +626,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -644,7 +655,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *34 ->RegressionTreeNode(
 missing_is_valid = 0 ;
@@ -666,7 +678,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -694,7 +707,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -726,7 +740,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
 ;
 left_node = *0 ;
 left_leave = *0 ;
@@ -781,7 +796,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -867,7 +883,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -18,7 +18,7 @@
                     complexity_penalty_factor = 0.0,
                     compute_train_stats = 0,
                     forget_when_training_set_changes = 1,
-                    leave_template = *2 -> RegressionTreeLeave( ),
+                    leave_template = *2 -> RegressionTreeLeave( output_confidence_target = 1 ),
                     loss_function_weight = 1,
                     maximum_number_of_nodes = 3,
                     missing_is_valid = 0,

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-03-26 15:09:37 UTC (rev 10060)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL10045"
+__REVISION__ = "PL10047"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -85,7 +85,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -170,7 +171,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -256,7 +258,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -20,7 +20,7 @@
             verbosity = 2,
             report_progress = 1,
             forget_when_training_set_changes = 1,
-            leave_template = pl.RegressionTreeLeave( )
+            leave_template = pl.RegressionTreeLeave(output_confidence_target = 1)
             ),
         test_minibatch_size=plargs.tms,
         weight_by_resampling=0,

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-26 15:09:37 UTC (rev 10060)
@@ -61,14 +61,14 @@
                         "and one for the others.\n"
     );
 
+bool RegressionTree::output_confidence_target = false;
+
 RegressionTree::RegressionTree()     
     : missing_is_valid(false),
       loss_function_weight(1.0),
       maximum_number_of_nodes(400),
       compute_train_stats(1),
-      complexity_penalty_factor(0.0),
-      output_confidence_target(false)
-
+      complexity_penalty_factor(0.0)
 {
 }
 
@@ -93,14 +93,11 @@
                   "If the error inprovement for the next split is less than the result, the algorithm proceed to an early stop."
                   "(When set to 0.0, the default value, it has no impact).");
 
-    declareOption(ol, "output_confidence_target",
+    declareStaticOption(ol, "output_confidence_target",
                   &RegressionTree::output_confidence_target,
                   OptionBase::buildoption,
-                  "If false the output size is 1 and contain only the predicted"
-                  " target. Else output size is 2 and contain also the"
-                  " confidence\n");
+                  "to reload old learner.");
 
-
     declareOption(ol, "multiclass_outputs", &RegressionTree::multiclass_outputs, OptionBase::buildoption,
                   "A vector of possible output values when solving a multiclass problem.\n"
                   "When making a prediction, the tree will adjust the output value of each leave to the closest value provided in this vector.");
@@ -152,7 +149,7 @@
     deepCopyField(first_leave, copies);
     deepCopyField(split_cols, copies);
     deepCopyField(split_values, copies);
-    deepCopyField(tmp_vec, copies);
+    //deepCopyField(tmp_vec, copies); not needed as we don't use it.
     
 }
 
@@ -196,7 +193,6 @@
                     weightsize);
     }
 
-    tmp_vec.resize(2);
     nodes = new TVec<PP<RegressionTreeNode> >();
     tmp_computeCostsFromOutput.resize(outputsize());
     
@@ -367,14 +363,6 @@
     return node; 
 }
 
-int RegressionTree::outputsize() const
-{
-    if(output_confidence_target)
-        return 2;
-    else
-        return 1;
-}
-
 TVec<string> RegressionTree::getTrainCostNames() const
 {
     TVec<string> return_msg(5);
@@ -407,24 +395,13 @@
 }
 void RegressionTree::computeOutput(const Vec& inputv, Vec& outputv) const
 {
-    if(!output_confidence_target){
-        computeOutputAndNodes(inputv, tmp_vec);
-        outputv[0]=tmp_vec[0];
-    }
-    else
-        computeOutputAndNodes(inputv, outputv);
-        
+    computeOutputAndNodes(inputv, outputv);
 }
 
 void RegressionTree::computeOutputAndNodes(const Vec& inputv, Vec& outputv,
                                            TVec<PP<RegressionTreeNode> >* nodes) const
 {
-    if(!output_confidence_target){
-        root->computeOutputAndNodes(inputv, tmp_vec, nodes);
-        outputv[0]=tmp_vec[0];
-    }
-    else
-        root->computeOutputAndNodes(inputv, outputv, nodes);
+    root->computeOutputAndNodes(inputv, outputv, nodes);
     return;
 }
 
@@ -436,13 +413,9 @@
     PLASSERT(nodes);
     nodes->resize(0);
 
-    computeOutputAndNodes(input, tmp_vec, nodes);
-    if(!output_confidence_target)
-        output[0]=tmp_vec[0];
-    else
-        output<<tmp_vec;
+    computeOutputAndNodes(input, output, nodes);
 
-    computeCostsFromOutputsAndNodes(input, tmp_vec, target, *nodes, costs);
+    computeCostsFromOutputsAndNodes(input, output, target, *nodes, costs);
 }
 
 void RegressionTree::computeCostsFromOutputsAndNodes(const Vec& input,

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2009-03-26 15:09:37 UTC (rev 10060)
@@ -46,10 +46,10 @@
 
 #include <plearn_learners/generic/PLearner.h>
 #include "RegressionTreeRegisters.h"
+#include "RegressionTreeLeave.h"
 namespace PLearn {
 using namespace std;
 class RegressionTreeQueue;
-class RegressionTreeLeave;
 class RegressionTreeNode;
 
 class RegressionTree: public PLearner
@@ -63,16 +63,15 @@
   Build options: they have to be set before training
 */
 
-    bool  missing_is_valid;
+    bool missing_is_valid;
     real loss_function_weight;
     int maximum_number_of_nodes;
     int compute_train_stats;   
     real complexity_penalty_factor;
-    bool output_confidence_target;
     Vec multiclass_outputs;
     PP<RegressionTreeLeave> leave_template;    
     PP<RegressionTreeRegisters> sorted_train_set;
-  
+    static bool output_confidence_target; //to reload old computer
 /*
   Learnt options: they are sized and initialized if need be, at stage 0
 */
@@ -107,7 +106,7 @@
     virtual void         train();
     virtual void         finalize();
     virtual void         forget();
-    virtual int          outputsize() const;
+    virtual int          outputsize() const {return leave_template->outputsize();}
     virtual TVec<string> getTrainCostNames() const;
     virtual TVec<string> getTestCostNames() const;
     PP<RegressionTreeRegisters> getSortedTrainingSet() const;

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-03-26 15:09:37 UTC (rev 10060)
@@ -53,6 +53,7 @@
 
 int RegressionTreeLeave::verbosity = 0;
 Vec RegressionTreeLeave::dummy_vec;
+bool RegressionTreeLeave::output_confidence_target = false;
 
 RegressionTreeLeave::RegressionTreeLeave():
     missing_leave(false),
@@ -98,6 +99,13 @@
     declareOption(ol, "loss_function_factor", &RegressionTreeLeave::loss_function_factor, OptionBase::learntoption,
                   "2 / pow(loss_function_weight, 2.0).\n");
 
+    declareStaticOption(ol, "output_confidence_target",
+                  &RegressionTreeLeave::output_confidence_target,
+                  OptionBase::buildoption,
+                  "If false the output size is 1 and contain only the predicted"
+                  " target. Else output size is 2 and contain also the"
+                  " confidence\n");
+
     declareStaticOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
     declareStaticOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
@@ -194,12 +202,13 @@
 
 void RegressionTreeLeave::getOutputAndError(Vec& output, Vec& error)const
 {
+    real conf = 0;
     if(length_>0){
         output[0] = weighted_targets_sum / weights_sum;
         if (missing_leave != true)
         {
             //we put the most frequent case first as an optimisation
-            output[1] = 1.0;
+            conf = 1.0;
             error[0] = ((weights_sum * output[0] * output[0]) - 
                         (2.0 * weighted_targets_sum * output[0]) + weighted_squared_targets_sum)
                 * loss_function_factor;
@@ -211,17 +220,15 @@
         }
         else
         {
-            output[1] = 0.0;
             error[0] = 0.0;
             error[1] = weights_sum;
             error[2] = 0.0;
         }
     }else{
-        output[0]=MISSING_VALUE;
-        output[1] = 0.0;
+        output[0] = MISSING_VALUE;
         error.clear();
-        return;
     }
+    if(output_confidence_target) output[1] = conf;
 }
 
 void RegressionTreeLeave::printStats()
@@ -231,7 +238,8 @@
     Vec error(3);
     getOutputAndError(output,error);
     cout << " o0 " << output[0];
-    cout << " o1 " << output[1];
+    if(output_confidence_target)
+        cout << " o1 " << output[1];
     cout << " e0 " << error[0];
     cout << " e1 " << error[1];
     cout << " ws " << weights_sum;

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-03-26 15:09:37 UTC (rev 10060)
@@ -77,7 +77,7 @@
     real weighted_targets_sum;
     real weighted_squared_targets_sum;
     real loss_function_factor;
- 
+    static bool output_confidence_target;
 public:
     RegressionTreeLeave();
     virtual              ~RegressionTreeLeave();
@@ -86,6 +86,8 @@
     static  void         declareOptions(OptionList& ol);
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
+    virtual int          outputsize() const
+    {return output_confidence_target?2:1;}
     void         initLeave(PP<RegressionTreeRegisters> the_train_set, RTR_type_id the_id, bool the_missing_leave = false);
     virtual void         initStats();
     virtual void         addRow(int row);

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-03-26 15:09:37 UTC (rev 10060)
@@ -216,6 +216,7 @@
         error.clear();
         return;
     }
+    real conf = 0;
     int mc_winer = 0;
     //index of the max. Is their an optimized version?
     for (int mc_ind = 1; mc_ind < multiclass_outputs.length(); mc_ind++)
@@ -226,14 +227,13 @@
     output[0] = multiclass_outputs[mc_winer];
     if (missing_leave)
     {
-        output[1] = 0.0;
         error[0] = 0.0;
         error[1] = weights_sum;
         error[2] = 0.0;
     }
     else
     {
-        output[1] = multiclass_weights_sum[mc_winer] / weights_sum;;
+        conf = multiclass_weights_sum[mc_winer] / weights_sum;;
         error[0] = 0.0;
         if (objective_function == "l1")
         {
@@ -261,8 +261,9 @@
                 error[2] = weights_sum * l2_loss_function_factor; 
             else error[2] = error[0];
         }
-        error[1] = (1.0 - output[1]) * length_;
+        error[1] = (1.0 - conf) * length_;
     }
+    if(output_confidence_target) output[1] = conf;
 }
 
 void RegressionTreeMulticlassLeave::printStats()

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-03-26 15:09:37 UTC (rev 10060)
@@ -188,6 +188,7 @@
         return;
     }
     int mc_winer = 0;
+    real conf = 0;
     //index of the max. Is their an optimized version?
     for (int mc_ind = 1; mc_ind < nb_class; mc_ind++)
     {
@@ -197,14 +198,13 @@
     output[0] = mc_winer;
     if (missing_leave)
     {
-        output[1] = 0.0;
         error[0] = 0.0;
         error[1] = weights_sum;
         error[2] = 0.0;
     }
     else
     {
-        output[1] = multiclass_weights_sum[mc_winer] / weights_sum;
+        conf = multiclass_weights_sum[mc_winer] / weights_sum;
         error[0] = 0.0;
         if (objective_function == "l1")
         {
@@ -227,8 +227,9 @@
         if (error[0] > weights_sum * loss_function_factor)
             error[2] = weights_sum * loss_function_factor;
         else error[2] = error[0];
-        error[1] = (1.0 - output[1]) * length_;
+        error[1] = (1.0 - conf) * length_;
     }
+    if(output_confidence_target) output[1] = conf;
 }
 
 void RegressionTreeMulticlassLeaveFast::printStats()

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-03-26 15:09:37 UTC (rev 10060)
@@ -214,7 +214,7 @@
     right_leave = ::PLearn::deepCopy(leave_template);
     right_leave->initLeave(the_train_set, right_leave_id);
 
-    leave_output.resize(2);
+    leave_output.resize(leave_template->outputsize());
     leave_error.resize(3);
 
     leave->getOutputAndError(leave_output,leave_error);
@@ -496,8 +496,7 @@
         nodes->append(this);
     if (!left_node)
     {
-        outputv[0] = leave_output[0];
-        outputv[1] = leave_output[1];
+        outputv << leave_output;
         return;
     }
     if (is_missing(inputv[split_col]))

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -19,7 +19,7 @@
             complexity_penalty_factor = 0.0,
             compute_train_stats = 1,
             forget_when_training_set_changes = 1,
-            leave_template = *3 -> RegressionTreeLeave( ),
+            leave_template = *3 -> RegressionTreeLeave( output_confidence_target = 1 ),
             loss_function_weight = 1,
             maximum_number_of_nodes = 50,
             nstages = 10,

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2009-03-26 15:09:37 UTC (rev 10060)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL9855"
+__REVISION__ = "PL10047"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -27,7 +27,7 @@
 targetsize = 1 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = "" ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat.metadata/SubVMatrix_istart=0_jstart=0_length=200_width=5.metadata/" ;
 source = *2   )
 ;
 splitter = *3 ->FractionSplitter(
@@ -75,7 +75,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -98,7 +99,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -115,6 +117,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -168,6 +171,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *7  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;
@@ -185,7 +189,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -202,6 +207,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -28,7 +28,8 @@
                 multiclass_outputs = [
                     0,
                     1
-                    ]
+                    ],
+                output_confidence_target = 1
                 ),
             loss_function_weight = 1,
             maximum_number_of_nodes = 50,

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2009-03-26 15:09:37 UTC (rev 10060)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9855"
+__REVISION__ = "PL10047"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -90,7 +90,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -113,7 +114,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -130,6 +132,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -183,6 +186,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *8  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;
@@ -200,7 +204,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -217,6 +222,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/experiment.plearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/experiment.plearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -24,7 +24,10 @@
             complexity_penalty_factor = 0.0,
             compute_train_stats = 1,
             forget_when_training_set_changes = 1,
-            leave_template = *4 -> RegressionTreeMulticlassLeaveFast( nb_class = 2 ),
+            leave_template = *4 -> RegressionTreeMulticlassLeaveFast(
+                nb_class = 2,
+                output_confidence_target = 1
+                ),
             loss_function_weight = 1,
             maximum_number_of_nodes = 50,
             nstages = 10,

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt	2009-03-26 15:09:37 UTC (rev 10060)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9955"
+__REVISION__ = "PL10047"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/tester.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -85,6 +85,7 @@
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 1 ;
+output_confidence_target = 1 ;
 nb_class = 2 ;
 objective_function = "l1" ;
 multiclass_weights_sum = []

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/experiment.plearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/experiment.plearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -8,7 +8,7 @@
             complexity_penalty_factor = 0.0,
             compute_train_stats = 1,
             forget_when_training_set_changes = 1,
-            leave_template = *2 -> RegressionTreeLeave( ),
+            leave_template = *2 -> RegressionTreeLeave( output_confidence_target = 1 ),
             loss_function_weight = 1,
             maximum_number_of_nodes = 50,
             multiclass_outputs = [

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt	2009-03-26 15:09:37 UTC (rev 10060)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL9855"
+__REVISION__ = "PL10047"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class.vmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/tester.psave	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/tester.psave	2009-03-26 15:09:37 UTC (rev 10060)
@@ -57,7 +57,8 @@
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1  )
+loss_function_factor = 1 ;
+output_confidence_target = 1  )
 ;
 root = *0 ;
 priority_queue = *0 ;
@@ -80,7 +81,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -97,6 +99,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -150,6 +153,7 @@
 ] ;
 provide_strategy_expdir = 1 ;
 save_final_learner = 0 ;
+finalize_learner = 0 ;
 learner = *6  ;
 provide_learner_expdir = 1 ;
 expdir_append = "" ;
@@ -167,7 +171,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -184,6 +189,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -39,7 +39,7 @@
         ,report_progress = 1
         ,forget_when_training_set_changes = 1
 #        ,conf_rated_adaboost = 0
-        ,leave_template = pl.RegressionTreeLeave( )
+        ,leave_template = pl.RegressionTreeLeave(output_confidence_target = 1)
         ),
     tester = pl.PTester(
     splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,0.75), (0,.75), (0.75,1) ])),

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -35,7 +35,7 @@
         ,report_progress = 1
         ,forget_when_training_set_changes = 1
 #        ,conf_rated_adaboost = 0
-        ,leave_template = pl.RegressionTreeMulticlassLeave(multiclass_outputs=[ 0, 1 ] )
+        ,leave_template = pl.RegressionTreeMulticlassLeave(multiclass_outputs=[ 0, 1 ],output_confidence_target = 1 )
         ),
     tester = pl.PTester(
     splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,200), (0,200), (200,1) ])),

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_fast.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_fast.pyplearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_fast.pyplearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -35,7 +35,7 @@
         ,report_progress = 1
         ,forget_when_training_set_changes = 1
 #        ,conf_rated_adaboost = 0
-        ,leave_template = pl.RegressionTreeMulticlassLeaveFast(nb_class=2 )
+        ,leave_template = pl.RegressionTreeMulticlassLeaveFast(nb_class=2, output_confidence_target = 1)
         ),
     tester = pl.PTester(
     splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,200), (0,200), (200,1) ])),

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_outputs.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_outputs.pyplearn	2009-03-25 20:30:28 UTC (rev 10059)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_outputs.pyplearn	2009-03-26 15:09:37 UTC (rev 10060)
@@ -30,7 +30,7 @@
         ,report_progress = 1
         ,forget_when_training_set_changes = 1
 #        ,conf_rated_adaboost = 0
-        ,leave_template = pl.RegressionTreeLeave( )
+        ,leave_template = pl.RegressionTreeLeave(output_confidence_target = 1)
         ),
     tester = pl.PTester(
     splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,0.75), (0,.75), (0.75,1) ])),



From nouiz at mail.berlios.de  Thu Mar 26 18:37:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Mar 2009 18:37:45 +0100
Subject: [Plearn-commits] r10061 - trunk/plearn_learners/cgi
Message-ID: <200903261737.n2QHbja6010542@sheep.berlios.de>

Author: nouiz
Date: 2009-03-26 18:37:44 +0100 (Thu, 26 Mar 2009)
New Revision: 10061

Modified:
   trunk/plearn_learners/cgi/ConfigParsing.cc
Log:
better information printing.


Modified: trunk/plearn_learners/cgi/ConfigParsing.cc
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-26 15:09:37 UTC (rev 10060)
+++ trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-26 17:37:44 UTC (rev 10061)
@@ -40,6 +40,7 @@
 #include "ConfigParsing.h"
 #include <plearn/vmat/TextFilesVMatrix.h>
 #include <plearn/io/openFile.h>
+#include <plearn/io/pl_log.h>
 #include <plearn/base/stringutils.h>
 
 namespace PLearn {
@@ -87,7 +88,7 @@
             break;
         }
     if(all_uptodate){
-        pout << "All file are uptodate. We don't regenerate the.";
+        NORMAL_LOG << "All config file are uptodate. We don't regenerate them."<<endl;
         return;
     }
     PStream f_csv = openFile(PPath(args[1]),PStream::raw_ascii,"w");



From nouiz at mail.berlios.de  Thu Mar 26 18:38:54 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Mar 2009 18:38:54 +0100
Subject: [Plearn-commits] r10062 - trunk/plearn/vmat
Message-ID: <200903261738.n2QHcsAt011164@sheep.berlios.de>

Author: nouiz
Date: 2009-03-26 18:38:47 +0100 (Thu, 26 Mar 2009)
New Revision: 10062

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
if their is multiple delimiter, we select one of them based on the two first line.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-26 17:37:44 UTC (rev 10061)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-26 17:38:47 UTC (rev 10062)
@@ -43,6 +43,7 @@
 #include <plearn/base/stringutils.h>
 #include <plearn/io/load_and_save.h>
 #include <plearn/io/fileutils.h>
+#define PL_LOG_MODULE_NAME "TextFilesVMatrix"
 #include <plearn/io/pl_log.h>
 
 namespace PLearn {
@@ -131,8 +132,8 @@
     int lineno = 0;
     for(unsigned char fileno=0; fileno<txtfiles.length(); fileno++)
     {
-        FILE* f = txtfiles[(int)fileno];
-        fseek(f,0,SEEK_SET);
+        FILE* fi = txtfiles[(int)fileno];
+        fseek(fi,0,SEEK_SET);
 
         int nskip = 0; // number of header lines to skip
         if(!skipheader.isEmpty())
@@ -141,18 +142,18 @@
         // read the data rows and build the index
         for(;;)
         {
-            long pos_long = ftell(f);
+            long pos_long = ftell(fi);
             if (pos_long > INT_MAX)
                 PLERROR("In TextFilesVMatrix::buildIdx - 'pos_long' cannot be "
                         "more than %d", INT_MAX);
             int pos = int(pos_long);
-            if(!fgets(buf, sizeof(buf), f))
+            if(!fgets(buf, sizeof(buf), fi))
                 break;
 
 #ifdef CYGWIN_FGETS_BUGFIX
             // Bugfix for CYGWIN carriage return bug.
             // Should be safe to enable in all case, but need to be tester more widely.
-            long new_pos = ftell(f);
+            long new_pos = ftell(fi);
             long lbuf = long(strlen(buf));
             if (lbuf+pos != new_pos)
                 if(lbuf+1+pos==new_pos && buf[lbuf-1]=='\n' && buf[lbuf-2]!='\r')
@@ -162,15 +163,15 @@
                     //So if their is only a \n, we are a caractere too far.
                     //if dos end of lines, return \n as end of lines in the strings and put the pos correctly.
                     
-                    fseek(f,-1,SEEK_CUR);
+                    fseek(fi,-1,SEEK_CUR);
                     
 		    //if unix end of lines
-                    if(fgetc(f)!='\n')
+                    if(fgetc(fi)!='\n')
                     	fseek(f,-1,SEEK_CUR);
                 }
                 //in the eof case?
                 else if(lbuf-1+pos==new_pos && buf[lbuf-1]=='\n' && buf[lbuf-2]!='\r')
-                    fseek(f,+1,SEEK_CUR);
+                    fseek(fi,+1,SEEK_CUR);
                 else
                     PLERROR("In TextFilesVMatrix::buildId - The number of characters read "
                             "does not match the position in the file.");
@@ -226,6 +227,50 @@
     TVec<string> fnames_header;//field names take in the header of source file
     char buf[50000];
 
+
+    //select witch delimiter we will use for all the files.
+    if(delimiter.size()>1){
+        FILE* f = txtfiles[0];
+        fseek(f,0,SEEK_SET);
+        if(!fgets(buf, sizeof(buf), f))
+            PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth() - "
+                    "Couldn't read the fields names from file '%s'",
+                    txtfilenames[0].c_str());
+
+        string s1 = string(buf);
+        if(!fgets(buf, sizeof(buf), f))
+            PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth() - "
+                    "Couldn't read the fields names from file '%s'",
+                    txtfilenames[0].c_str());
+        string s2 = string(buf);
+        TVec<int> nbs1(delimiter.size());
+        TVec<int> nbs2(delimiter.size());
+        
+        string old_delimiter = delimiter;
+        for(uint i=0;i<old_delimiter.size();i++){
+            delimiter = old_delimiter[i];
+            TVec<string> fields1 = splitIntoFields(s1);
+            TVec<string> fields2 = splitIntoFields(s2);
+            nbs1[i]=fields1.size();
+            nbs2[i]=fields2.size();
+        }
+        delimiter=old_delimiter;
+        for(uint i=0;i<old_delimiter.size();i++){
+            if(nbs1[i]==nbs2[i]&& nbs1[i]>0){
+                delimiter = old_delimiter[i];
+            }
+        }
+        MODULE_LOG << "Selected delimiter: <" << delimiter << ">" << endl;
+        if(delimiter.size()!=1){
+            PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth() - We can't"
+                    " automatically determine the delimiter to use as the two"
+                    " first row don't have a common delimiter with the same"
+                    " number of occurence. nbs1=%s, nbs2=%s",
+                    tostring(nbs1).c_str(),tostring(nbs2).c_str());
+        }
+    }
+    PLCHECK(delimiter.size()==1);
+
     //read the fieldnames from the files.
     for(int i=0; i<txtfiles.size(); i++){
         FILE* f = txtfiles[i];
@@ -921,7 +966,9 @@
                   "Delimiter to use to split the fields.  Common delimiters are:\n"
                   "- \"\\t\" : used for SAS files (the default)\n"
                   "- \",\"  : used for CSV files\n"
-                  "- \";\"  : used for a variant of CSV files");
+                  "- \";\"  : used for a variant of CSV files\n"
+                  "If more then 1 delimiter, we will select one based on the"
+                  " first two line\n");
 
     declareOption(ol, "quote_delimiter", &TextFilesVMatrix::quote_delimiter, OptionBase::buildoption,
         "The escape character to indicate the delimiter is not considered.\n"



From nouiz at mail.berlios.de  Thu Mar 26 19:14:18 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Mar 2009 19:14:18 +0100
Subject: [Plearn-commits] r10063 - trunk/python_modules/plearn/pytest
Message-ID: <200903261814.n2QIEIgG016628@sheep.berlios.de>

Author: nouiz
Date: 2009-03-26 19:14:17 +0100 (Thu, 26 Mar 2009)
New Revision: 10063

Modified:
   trunk/python_modules/plearn/pytest/modes.py
   trunk/python_modules/plearn/pytest/programs.py
Log:
print the path to the compile log when it failed.


Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2009-03-26 17:38:47 UTC (rev 10062)
+++ trunk/python_modules/plearn/pytest/modes.py	2009-03-26 18:14:17 UTC (rev 10063)
@@ -745,6 +745,13 @@
                     else:
                         logging.debug(e)
                         test.setStatus("SKIPPED", core.traceback(e))
+        l=[]
+        for (test_name, test) in test_instances:
+            if not test.compilationSucceeded():
+                f=test.program.getCompilationLogPath()
+                if f not in l:
+                    l+=[f]
+                    logging.info("The failed compile log %s"%f)
 
 class compile(RoutineBasedMode):
     RoutineType = CompilationRoutine

Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2009-03-26 17:38:47 UTC (rev 10062)
+++ trunk/python_modules/plearn/pytest/programs.py	2009-03-26 18:14:17 UTC (rev 10063)
@@ -133,7 +133,7 @@
             if self.compiler is None:
                 self.compiler = Program.default_compiler
             self.__attempted_to_compile = False
-            self.__log_file_path = internal_exec_path+'.log'
+            self.__log_file_path = self.getCompilationLogPath()
         logging.debug("Program instance: %s\n"%repr(self))
 
     def handleCompileOptions(self):        
@@ -308,6 +308,9 @@
     def getName(self):
         return self.name
         
+    def getCompilationLogPath(self):
+        return self.getInternalExecPath()+'.log'
+
     def getInternalExecPath(self, candidate=None):
         if hasattr(self, '_internal_exec_path'):
             assert candidate is None, 'candidate is not None:'+repr(candidate)



From nouiz at mail.berlios.de  Thu Mar 26 19:22:16 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Mar 2009 19:22:16 +0100
Subject: [Plearn-commits] r10064 - trunk/python_modules/plearn/pytest
Message-ID: <200903261822.n2QIMG1n025677@sheep.berlios.de>

Author: nouiz
Date: 2009-03-26 19:22:16 +0100 (Thu, 26 Mar 2009)
New Revision: 10064

Modified:
   trunk/python_modules/plearn/pytest/modes.py
Log:
fix in case we crashed during the confirm command.


Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2009-03-26 18:14:17 UTC (rev 10063)
+++ trunk/python_modules/plearn/pytest/modes.py	2009-03-26 18:22:16 UTC (rev 10064)
@@ -421,6 +421,7 @@
                     if os.path.exists(svn_results):
                         expected_results = test.resultsDirectory(tests.EXPECTED_RESULTS)
                         confirmed_results = expected_results+'.confirmed'
+                        shutil.rmtree(confirmed_results)#in case we crashed
                         os.rename(expected_results, confirmed_results)
                         os.rename(svn_results, expected_results)
                         self.migrate_results_trees(expected_results, confirmed_results)



From nouiz at mail.berlios.de  Thu Mar 26 20:44:24 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 26 Mar 2009 20:44:24 +0100
Subject: [Plearn-commits] r10065 - trunk/plearn/vmat
Message-ID: <200903261944.n2QJiOg4027874@sheep.berlios.de>

Author: nouiz
Date: 2009-03-26 20:44:24 +0100 (Thu, 26 Mar 2009)
New Revision: 10065

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
bugfix and remove warning.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-26 18:22:16 UTC (rev 10064)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-03-26 19:44:24 UTC (rev 10065)
@@ -155,7 +155,7 @@
             // Should be safe to enable in all case, but need to be tester more widely.
             long new_pos = ftell(fi);
             long lbuf = long(strlen(buf));
-            if (lbuf+pos != new_pos)
+            if (lbuf+pos != new_pos){
                 if(lbuf+1+pos==new_pos && buf[lbuf-1]=='\n' && buf[lbuf-2]!='\r')
                 {
                     //bug under windows. fgets return the good string if unix end of lines, but
@@ -167,7 +167,7 @@
                     
 		    //if unix end of lines
                     if(fgetc(fi)!='\n')
-                    	fseek(f,-1,SEEK_CUR);
+                    	fseek(fi,-1,SEEK_CUR);
                 }
                 //in the eof case?
                 else if(lbuf-1+pos==new_pos && buf[lbuf-1]=='\n' && buf[lbuf-2]!='\r')
@@ -175,6 +175,7 @@
                 else
                     PLERROR("In TextFilesVMatrix::buildId - The number of characters read "
                             "does not match the position in the file.");
+            }
 #endif
 
             buf[sizeof(buf)-1] = '\0';         // ensure null-terminated



From nouiz at mail.berlios.de  Fri Mar 27 16:16:18 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 27 Mar 2009 16:16:18 +0100
Subject: [Plearn-commits] r10066 - in trunk: commands
 plearn_learners/regressors plearn_learners/regressors/test/RegressionTree
 plearn_learners/regressors/test/RegressionTree/.pytest
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/Learner!
 Expdir/Strat0/Trials0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stat!
 s.pmat.metadata plearn_learners/regressors/test/RegressionTree! /.pytest
Message-ID: <200903271516.n2RFGIaE023087@sheep.berlios.de>

Author: nouiz
Date: 2009-03-27 16:16:14 +0100 (Fri, 27 Mar 2009)
New Revision: 10066

Added:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/train_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_prob.pyplearn
Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/test/RegressionTree/pytest.config
Log:
Added a class RegressionTreeMulticlassLeaveProb that output the prob of each class instead of only the winner and added test.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/commands/plearn_noblas_inc.h	2009-03-27 15:16:14 UTC (rev 10066)
@@ -191,6 +191,8 @@
 #include <plearn_learners/regressors/RegressionTree.h>
 #include <plearn_learners/regressors/RegressionTreeMulticlassLeave.h>
 #include <plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h>
+#include <plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h>
+
 // Unsupervised
 #include <plearn_learners/unsupervised/UniformizeLearner.h>
 

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-27 15:16:14 UTC (rev 10066)
@@ -426,7 +426,7 @@
 {
     costs.clear();
     costs[0] = pow((output[0] - target[0]), 2);
-    if(output.length()>1) costs[1] = output[1];
+    if(leave_template->output_confidence_target) costs[1] = output[1];
     else costs[1] = MISSING_VALUE;
     costs[2] = 1.0 - (l2_loss_function_factor * costs[0]);
     costs[3] = 1.0 - (l1_loss_function_factor * abs(output[0] - target[0]));

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-03-27 15:16:14 UTC (rev 10066)
@@ -58,26 +58,25 @@
     real loss_function_weight;
     static int  verbosity;
 
-protected:
-
 /*
   Build options: they have to be set before building
 */
-
     RTR_type_id  id;
     PP<RegressionTreeRegisters> train_set;
+    static bool output_confidence_target;
+
+protected:
  
 /*
   Learnt options: they are sized and initialized if need be, in initLeave(...)
 */
-
     int  length_;
     real weights_sum;
     real targets_sum;
     real weighted_targets_sum;
     real weighted_squared_targets_sum;
     real loss_function_factor;
-    static bool output_confidence_target;
+
 public:
     RegressionTreeLeave();
     virtual              ~RegressionTreeLeave();

Copied: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc (from rev 9969, trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc)
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-02-27 20:40:12 UTC (rev 9969)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,264 @@
+// -*- C++ -*-
+
+// RegressionTreeMulticlassLeaveProb.cc
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* ********************************************************************************    
+ * $Id: RegressionTreeMulticlassLeaveProb.cc, v 1.0 2004/07/19 10:00:00 Bengio/Kegl/Godbout    *
+ * This file is part of the PLearn library.                                     *
+ ******************************************************************************** */
+
+#include "RegressionTreeMulticlassLeaveProb.h"
+#include "RegressionTreeRegisters.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(RegressionTreeMulticlassLeaveProb,
+                        "Object to represent the leaves of a regression tree.",
+                        "It maintains the necessary statistics to compute the output and the train error\n"
+                        "of the samples in the leave.\n"
+    );
+
+RegressionTreeMulticlassLeaveProb::RegressionTreeMulticlassLeaveProb()
+    : nb_class(-1),
+      objective_function("l1")
+{
+    build();
+}
+
+RegressionTreeMulticlassLeaveProb::~RegressionTreeMulticlassLeaveProb()
+{
+}
+
+void RegressionTreeMulticlassLeaveProb::declareOptions(OptionList& ol)
+{ 
+    inherited::declareOptions(ol);
+
+    declareOption(ol, "nb_class", 
+                  &RegressionTreeMulticlassLeaveProb::nb_class,
+                  OptionBase::buildoption,
+                  "The number of class. Should be numbered from 0 to nb_class -1.\n"
+        );
+
+    declareOption(ol, "objective_function",
+                  &RegressionTreeMulticlassLeaveProb::objective_function,
+                  OptionBase::buildoption,
+                  "The function to be used to compute the leave error.\n"
+                  "Current supported values are l1 and l2 (default is l1).");
+      
+    declareOption(ol, "multiclass_weights_sum",
+                  &RegressionTreeMulticlassLeaveProb::multiclass_weights_sum,
+                  OptionBase::learntoption,
+                  "A vector to count the weight sum of each possible output "
+                  "for the sample in this leave.\n");
+    redeclareOption(ol, "loss_function_factor",
+                  &RegressionTreeMulticlassLeaveProb::loss_function_factor,
+                  OptionBase::learntoption,
+                  "The loss fct factor. Depend of the objective_function.\n");
+}
+
+void RegressionTreeMulticlassLeaveProb::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(objective_function, copies);
+    deepCopyField(multiclass_weights_sum, copies);
+}
+
+void RegressionTreeMulticlassLeaveProb::build()
+{
+    inherited::build();
+    build_();
+}
+
+void RegressionTreeMulticlassLeaveProb::build_()
+{
+}
+
+void RegressionTreeMulticlassLeaveProb::initStats()
+{
+    length_ = 0;
+    weights_sum = 0.0;
+    if (loss_function_weight != 0.0)
+    {
+        if(objective_function == "l1")
+            loss_function_factor = 2.0 / loss_function_weight;
+        else
+            loss_function_factor = 2.0 / pow(loss_function_weight, 2);
+    }
+    else
+    {
+        loss_function_factor = 1.0;
+    }
+    multiclass_weights_sum.resize(nb_class);
+    multiclass_weights_sum.fill(0);
+}
+
+void RegressionTreeMulticlassLeaveProb::addRow(int row)
+{
+    real weight = train_set->getWeight(row);
+    real target = train_set->getTarget(row);
+    RegressionTreeMulticlassLeaveProb::addRow(row, target, weight);
+}
+
+void RegressionTreeMulticlassLeaveProb::addRow(int row, real target, real weight,
+                                 Vec outputv, Vec errorv)
+{
+    RegressionTreeMulticlassLeaveProb::addRow(row, target, weight);
+    RegressionTreeMulticlassLeaveProb::getOutputAndError(outputv,errorv);
+}
+
+void RegressionTreeMulticlassLeaveProb::addRow(int row, real target, real weight)
+{
+    length_ += 1;
+    weights_sum += weight;
+    multiclass_weights_sum[int(target)] += weight;
+}
+
+void RegressionTreeMulticlassLeaveProb::addRow(int row, Vec outputv, Vec errorv)
+{
+    RegressionTreeMulticlassLeaveProb::addRow(row);
+    RegressionTreeMulticlassLeaveProb::getOutputAndError(outputv,errorv);    
+}
+
+void RegressionTreeMulticlassLeaveProb::removeRow(int row, Vec outputv, Vec errorv)
+{
+    real weight = train_set->getWeight(row);
+    real target = train_set->getTarget(row);
+    RegressionTreeMulticlassLeaveProb::removeRow(row,target,weight,outputv,errorv);
+}
+
+void RegressionTreeMulticlassLeaveProb::removeRow(int row, real target, real weight,
+                                 Vec outputv, Vec errorv){
+    RegressionTreeMulticlassLeaveProb::removeRow(row,target,weight);
+    RegressionTreeMulticlassLeaveProb::getOutputAndError(outputv,errorv);
+}
+
+void RegressionTreeMulticlassLeaveProb::removeRow(int row, real target, real weight)
+{
+    length_ -= 1;
+    weights_sum -= weight;
+    PLASSERT(length_>=0);
+    PLASSERT(weights_sum>=0);
+    PLASSERT(length_>0 || weights_sum==0);
+    multiclass_weights_sum[int(target)] -= weight;
+}
+
+void RegressionTreeMulticlassLeaveProb::getOutputAndError(Vec& output, Vec& error)const
+{
+#ifdef BOUNDCHECK
+    if(nb_class<=0)
+        PLERROR("In RegressionTreeMulticlassLeaveProb::getOutputAndError() -"
+                " nb_class must be set.");
+#endif
+    if(length_==0){        
+        output.fill(MISSING_VALUE);
+        error.clear();
+        return;
+    }
+    int mc_winer = 0;
+    real conf = 0;
+    //index of the max. Is their an optimized version?
+    output[1] = multiclass_weights_sum[0] / weights_sum;
+    for (int mc_ind = 1; mc_ind < nb_class; mc_ind++)
+    {
+        output[mc_ind+1]=multiclass_weights_sum[mc_ind] / weights_sum;
+        if (multiclass_weights_sum[mc_ind] > multiclass_weights_sum[mc_winer])
+            mc_winer = mc_ind;
+    }
+    output[0] = mc_winer;
+    if (missing_leave)
+    {
+        error[0] = 0.0;
+        error[1] = weights_sum;
+        error[2] = 0.0;
+    }
+    else
+    {
+        conf = multiclass_weights_sum[mc_winer] / weights_sum;
+        error[0] = 0.0;
+        if (objective_function == "l1")
+        {
+            for (int mc_ind = 0; mc_ind < nb_class;mc_ind++)
+            {
+                error[0] += abs(mc_winer - mc_ind) 
+                    * multiclass_weights_sum[mc_ind];
+            }
+        }
+        else
+        {
+            for (int mc_ind = 0; mc_ind < nb_class;mc_ind++)
+            {
+                error[0] += pow(mc_winer - mc_ind, 2.) 
+                    * multiclass_weights_sum[mc_ind];
+            }
+        }
+        error[0] *= loss_function_factor * length_ / weights_sum;
+        if (error[0] < 1E-10) error[0] = 0.0;
+        if (error[0] > weights_sum * loss_function_factor)
+            error[2] = weights_sum * loss_function_factor;
+        else error[2] = error[0];
+        error[1] = (1.0 - conf) * length_;
+    }
+}
+
+void RegressionTreeMulticlassLeaveProb::printStats()
+{
+    cout << " l " << length_;
+    Vec output(2);
+    Vec error(3);
+    getOutputAndError(output,error);
+    cout << " o0 " << output[0];
+    cout << " o1 " << output[1];
+    cout << " e0 " << error[0];
+    cout << " e1 " << error[1];
+    cout << " ws " << weights_sum;
+    cout << endl;
+    cout << " mws " << multiclass_weights_sum << endl;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h (from rev 9967, trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h)
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h	2009-02-27 20:21:29 UTC (rev 9967)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,110 @@
+// -*- C++ -*-
+
+// RegressionTreeMulticlassLeaveProb.h
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *********************************************************************************   
+ * $Id: RegressionTreeMulticlassLeaveProb.h, v 1.0 2004/07/19 10:00:00 Bengio/Kegl/Godbout     *
+ * This file is part of the PLearn library.                                      *
+ ********************************************************************************* */
+
+#ifndef RegressionTreeMulticlassLeaveProb_INC
+#define RegressionTreeMulticlassLeaveProb_INC
+
+#include "RegressionTreeLeave.h"
+
+namespace PLearn {
+using namespace std;
+
+class RegressionTreeMulticlassLeaveProb: public RegressionTreeLeave
+{
+    typedef RegressionTreeLeave inherited;
+  
+private:
+
+/*
+  Build options: they have to be set before building
+*/
+
+    int nb_class;
+    string objective_function;   
+    
+/*
+  Learnt options: they are sized and initialized if need be, in initLeave(...)
+*/
+
+    Vec multiclass_weights_sum;
+ 
+public:
+    RegressionTreeMulticlassLeaveProb();
+    virtual              ~RegressionTreeMulticlassLeaveProb();
+    PLEARN_DECLARE_OBJECT(RegressionTreeMulticlassLeaveProb);
+
+    static  void         declareOptions(OptionList& ol);
+    virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
+    virtual void         build();
+    virtual int          outputsize() const {return nb_class+1;}
+    void         initStats();
+    void         addRow(int row);
+    void         addRow(int row, real target, real weight);
+    void         addRow(int row, Vec outputv, Vec errorv);
+    void         addRow(int row, real target, real weight, Vec outputv, Vec errorv);
+    void         removeRow(int row, real target, real weight);
+    void         removeRow(int row, Vec outputv, Vec errorv);
+    void         removeRow(int row, real target, real weight, Vec outputv, Vec errorv);
+    void         getOutputAndError(Vec& output, Vec& error)const;
+    void         printStats();
+  
+private:
+    void         build_();
+};
+
+DECLARE_OBJECT_PTR(RegressionTreeMulticlassLeaveProb);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/RUN.log	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/RUN.log	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,11 @@
+HyperLearner: starting the optimization
+split_cols: []
+
+split_values: []
+
+split_cols: 1 
+split_values: 0.547790627008831965 
+split_cols: 1 0 
+split_values: 0.547790627008831965 1.88430442629138994 
+split_cols: 1 0 0 
+split_values: 0.547790627008831965 1.88430442629138994 3.07754902034128275 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 3177 ;
+sumsquare_ = 3177 ;
+sumcube_ = 3177 ;
+sumfourth_ = 3177 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 6831 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -6354 ;
+sumsquare_ = 12708 ;
+sumcube_ = -25416 ;
+sumfourth_ = 50832 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -6354 ;
+sumsquare_ = 12708 ;
+sumcube_ = -25416 ;
+sumfourth_ = 50832 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 3177 ;
+sumsquare_ = 3177 ;
+sumcube_ = 3177 ;
+sumfourth_ = 3177 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0 ;
+max_ = 0 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 121 ;
+sumsquare_ = 121 ;
+sumcube_ = 121 ;
+sumfourth_ = 121 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -121 ;
+sumsquare_ = 121 ;
+sumcube_ = -121 ;
+sumfourth_ = 121 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1145 ;
+sumsquare_ = 1145 ;
+sumcube_ = 1145 ;
+sumfourth_ = 1145 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 6831 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2290 ;
+sumsquare_ = 4580 ;
+sumcube_ = -9160 ;
+sumfourth_ = 18320 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2290 ;
+sumsquare_ = 4580 ;
+sumcube_ = -9160 ;
+sumfourth_ = 18320 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1145 ;
+sumsquare_ = 1145 ;
+sumcube_ = 1145 ;
+sumfourth_ = 1145 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 3312 ;
+sumsquare_ = 3312 ;
+sumcube_ = 3312 ;
+sumfourth_ = 3312 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 3311 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -3312 ;
+sumsquare_ = 3312 ;
+sumcube_ = -3312 ;
+sumfourth_ = 3312 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 3311 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 121 ;
+sumsquare_ = 121 ;
+sumcube_ = 121 ;
+sumfourth_ = 121 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -121 ;
+sumsquare_ = 121 ;
+sumcube_ = -121 ;
+sumfourth_ = 121 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 141 ;
+sumsquare_ = 181 ;
+sumcube_ = 261 ;
+sumfourth_ = 421 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 199 ;
+agemax_ = 190 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -20 ;
+sumsquare_ = 20 ;
+sumcube_ = -20 ;
+sumfourth_ = 20 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1229 ;
+sumsquare_ = 1229 ;
+sumcube_ = 1229 ;
+sumfourth_ = 1229 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 6831 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2458 ;
+sumsquare_ = 4916 ;
+sumcube_ = -9832 ;
+sumfourth_ = 19664 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2458 ;
+sumsquare_ = 4916 ;
+sumcube_ = -9832 ;
+sumfourth_ = 19664 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1229 ;
+sumsquare_ = 1229 ;
+sumcube_ = 1229 ;
+sumfourth_ = 1229 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 4464 ;
+sumsquare_ = 6768 ;
+sumcube_ = 11376 ;
+sumfourth_ = 20592 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 6830 ;
+agemax_ = 3266 ;
+first_ = 0 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1152 ;
+sumsquare_ = 1152 ;
+sumcube_ = -1152 ;
+sumfourth_ = 1152 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 3266 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 141 ;
+sumsquare_ = 181 ;
+sumcube_ = 261 ;
+sumfourth_ = 421 ;
+min_ = 0 ;
+max_ = 2 ;
+agmemin_ = 199 ;
+agemax_ = 190 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -20 ;
+sumsquare_ = 20 ;
+sumcube_ = -20 ;
+sumfourth_ = 20 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 199 ;
+agemax_ = 140 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 140 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 6831 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 5040 ;
+sumsquare_ = 9648 ;
+sumcube_ = 22320 ;
+sumfourth_ = 58032 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 6830 ;
+agemax_ = 3254 ;
+first_ = 0 ;
+last_ = 3 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -576 ;
+sumsquare_ = 576 ;
+sumcube_ = -576 ;
+sumfourth_ = 576 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 3254 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 199 ;
+agemax_ = 140 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 140 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,11 @@
+_trial_	0
+_objective_	0
+nstages	0
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,7 @@
+mse	0
+base_confidence	0
+base_reward_l2	0
+base_reward_l1	0
+class_error	0
+SPLIT_VAR_x1	0
+SPLIT_VAR_y2	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,3 @@
+out0	0
+out1	0
+out2	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 200 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 199 ;
+agemax_ = 140 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 140 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,7 @@
+mse	0
+base_confidence	0
+base_reward_l2	0
+base_reward_l1	0
+class_error	0
+SPLIT_VAR_x1	0
+SPLIT_VAR_y2	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,3 @@
+out0	0
+out1	0
+out2	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,171 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 7 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" "SPLIT_VAR_x1" "SPLIT_VAR_y2" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 7 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 6831 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 5040 ;
+sumsquare_ = 9648 ;
+sumcube_ = 22320 ;
+sumfourth_ = 58032 ;
+min_ = 0 ;
+max_ = 3 ;
+agmemin_ = 6830 ;
+agemax_ = 3254 ;
+first_ = 0 ;
+last_ = 3 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -576 ;
+sumsquare_ = 576 ;
+sumcube_ = -576 ;
+sumfourth_ = 576 ;
+min_ = 1 ;
+max_ = 2 ;
+agmemin_ = 3254 ;
+agemax_ = 6830 ;
+first_ = 2 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/train_stats.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/train_stats.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,192 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 8 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.195000000000000007 ;
+max_ = 0.195000000000000007 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.195000000000000007 ;
+last_ = 0.195000000000000007 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 1 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.609999999999999987 ;
+max_ = 0.609999999999999987 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.609999999999999987 ;
+last_ = 0.609999999999999987 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.609999999999999987 ;
+max_ = 0.609999999999999987 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.609999999999999987 ;
+last_ = 0.609999999999999987 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.0985214463475333063 ;
+max_ = 0.0985214463475333063 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.0985214463475333063 ;
+last_ = 0.0985214463475333063 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 1 ;
+nnonmissing_ = 0 ;
+sumsquarew_ = 0 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = nan ;
+max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = nan ;
+last_ = nan ;
+binary_ = -1 ;
+integer_ = -1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.802957107304933415 ;
+max_ = 0.802957107304933415 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.802957107304933415 ;
+last_ = 0.802957107304933415 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.802957107304933415 ;
+max_ = 0.802957107304933415 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.802957107304933415 ;
+last_ = 0.802957107304933415 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/experiment.plearn	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/experiment.plearn	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,116 @@
+*12 -> PTester(
+    dataset = *3 -> ConcatRowsVMatrix(
+        inputsize = 2,
+        sources = [
+            *1 -> AutoVMatrix(
+                inputsize = 2,
+                specification = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat",
+                targetsize = 1
+                ),
+            *2 -> AutoVMatrix(
+                inputsize = 2,
+                specification = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat",
+                targetsize = 1
+                )
+            ],
+        targetsize = 1,
+        weightsize = 0
+        ),
+    expdir = "expdir",
+    learner = *10 -> HyperLearner(
+        dont_restart_upon_change = [ "nstages" ],
+        forget_when_training_set_changes = 0,
+        learner = *5 -> RegressionTree(
+            complexity_penalty_factor = 0.0,
+            compute_train_stats = 1,
+            forget_when_training_set_changes = 1,
+            leave_template = *4 -> RegressionTreeMulticlassLeaveProb( nb_class = 2 ),
+            loss_function_weight = 1,
+            maximum_number_of_nodes = 50,
+            nstages = 10,
+            output_confidence_target = 1,
+            report_progress = 1,
+            verbosity = 2
+            ),
+        nstages = 1,
+        option_fields = [ "nstages" ],
+        provide_learner_expdir = 1,
+        provide_strategy_expdir = 1,
+        report_progress = 1,
+        save_final_learner = 0,
+        strategy = [
+            *7 -> HyperOptimize(
+                oracle = *6 -> EarlyStoppingOracle(
+                    max_degradation = 3.40282e+38,
+                    max_degraded_steps = 120,
+                    max_value = 3.40282e+38,
+                    min_improvement = -3.40282e+38,
+                    min_n_steps = 2,
+                    min_value = -3.40282e+38,
+                    option = "nstages",
+                    range = [
+                        1,
+                        5,
+                        1
+                        ],
+                    relative_max_degradation = -1,
+                    relative_min_improvement = -1
+                    ),
+                provide_tester_expdir = 1,
+                which_cost = "E[test2.E[class_error]]"
+                )
+            ],
+        tester = *9 -> PTester(
+            provide_learner_expdir = 1,
+            report_stats = 1,
+            save_data_sets = 0,
+            save_initial_learners = 0,
+            save_initial_tester = 0,
+            save_learners = 0,
+            save_test_confidence = 0,
+            save_test_costs = 0,
+            save_test_names = 0,
+            save_test_outputs = 0,
+            splitter = *8 -> FractionSplitter(
+                splits = 1 3 [
+                        (0, 200),
+                        (0, 200),
+                        (200, 1)
+                        ]
+                ),
+            statnames = [
+                "E[test1.E[class_error]]",
+                "E[test1.E[base_confidence]]",
+                "E[test1.E[base_reward_l2]]",
+                "E[test1.E[base_reward_l1]]",
+                "E[test2.E[class_error]]",
+                "E[test2.E[base_confidence]]",
+                "E[test2.E[base_reward_l2]]",
+                "E[test2.E[base_reward_l1]]"
+                ]
+            ),
+        verbosity = 1
+        ),
+    provide_learner_expdir = 1,
+    save_learners = 0,
+    save_test_confidence = 0,
+    save_test_costs = 1,
+    save_test_outputs = 1,
+    splitter = *11 -> FractionSplitter(
+        splits = 1 3 [
+                (0, 1),
+                (0, 200),
+                (200, 1)
+                ]
+        ),
+    statnames = [
+        "E[test1.E[class_error]]",
+        "E[test1.E[base_confidence]]",
+        "E[test1.E[base_reward_l2]]",
+        "E[test1.E[base_reward_l1]]",
+        "E[test2.E[class_error]]",
+        "E[test2.E[base_confidence]]",
+        "E[test2.E[base_reward_l2]]",
+        "E[test2.E[base_reward_l1]]"
+        ]
+    )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/global_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,3 @@
+__REVISION__ = "PL10065"
+datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
+datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/sizes	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/split_stats.pmat.metadata/sizes	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/test_cost_names.txt	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/test_cost_names.txt	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,7 @@
+mse
+base_confidence
+base_reward_l2
+base_reward_l1
+class_error
+SPLIT_VAR_x1
+SPLIT_VAR_y2

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/tester.psave	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/tester.psave	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,231 @@
+PTester(
+expdir = "PYTEST__PL_RegressionTree_MultiClassProb__RESULTS:expdir/" ;
+dataset = *1 ->ConcatRowsVMatrix(
+sources = 2 [ *2 ->AutoVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 200 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat.metadata/" ;
+fieldinfos = 3 [ "x1" 0 "y2" 0 "class" 0 ]  )
+*3 ->AutoVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 6831 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat.metadata/" ;
+fieldinfos = 3 [ "x1" 0 "x2" 0 "class" 0 ]  )
+] ;
+fill_missing = 0 ;
+fully_check_mappings = 0 ;
+only_common_fields = 0 ;
+writable = 0 ;
+length = 7031 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "" ;
+fieldinfos = 3 [ "x1" 0 "y2" 0 "class" 0 ]  )
+;
+splitter = *4 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 1 )	(0 , 200 )	(200 , 1 )	
+]
+;
+one_is_absolute = 0  )
+;
+statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *5 ->HyperLearner(
+tester = *6 ->PTester(
+expdir = "" ;
+dataset = *0 ;
+splitter = *7 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 200 )	(0 , 200 )	(200 , 1 )	
+]
+;
+one_is_absolute = 0  )
+;
+statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *8 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 50 ;
+compute_train_stats = 1 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 1 ;
+multiclass_outputs = []
+;
+leave_template = *9 ->RegressionTreeMulticlassLeaveProb(
+id = 0 ;
+missing_leave = 0 ;
+loss_function_weight = 0 ;
+verbosity = 0 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output_confidence_target = 0 ;
+nb_class = 2 ;
+objective_function = "l1" ;
+multiclass_weights_sum = []
+ )
+;
+root = *0 ;
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
+;
+split_values = []
+;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 0 ;
+n_examples = -1 ;
+inputsize = -1 ;
+targetsize = -1 ;
+weightsize = -1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 10 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 0 ;
+save_stat_collectors = 1 ;
+save_split_stats = 1 ;
+save_learners = 0 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 0 ;
+call_forget_in_run = 1 ;
+save_test_costs = 0 ;
+save_test_names = 0 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+finalize_learner = 0 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 0 ;
+enforce_clean_expdir = 1 ;
+redirect_stdout = 0 ;
+redirect_stderr = 0  )
+;
+option_fields = 1 [ "nstages" ] ;
+dont_restart_upon_change = 1 [ "nstages" ] ;
+strategy = 1 [ *10 ->HyperOptimize(
+which_cost = "E[test2.E[class_error]]" ;
+min_n_trials = 0 ;
+oracle = *11 ->EarlyStoppingOracle(
+option = "nstages" ;
+values = []
+;
+range = 3 [ 1 5 1 ] ;
+min_value = -3.40282000000000014e+38 ;
+max_value = 3.40282000000000014e+38 ;
+max_degradation = 3.40282000000000014e+38 ;
+relative_max_degradation = -1 ;
+min_improvement = -3.40282000000000014e+38 ;
+relative_min_improvement = -1 ;
+max_degraded_steps = 120 ;
+min_n_steps = 2 ;
+nreturned = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_step = -1 ;
+met_early_stopping = 0  )
+;
+provide_tester_expdir = 1 ;
+sub_strategy = []
+;
+rerun_after_sub = 0 ;
+provide_sub_expdir = 1 ;
+save_best_learner = 0 ;
+splitter = *0 ;
+auto_save = 0 ;
+auto_save_diff_time = 10800 ;
+auto_save_test = 0 ;
+best_objective = 1.79769313486231571e+308 ;
+best_results = []
+;
+best_learner = *0 ;
+trialnum = 0 ;
+option_vals = []
+;
+verbosity = 0  )
+] ;
+provide_strategy_expdir = 1 ;
+save_final_learner = 0 ;
+finalize_learner = 0 ;
+learner = *8  ;
+provide_learner_expdir = 1 ;
+expdir_append = "" ;
+forward_nstages = 0 ;
+random_gen = *0 ;
+stage = 0 ;
+n_examples = -1 ;
+inputsize = -1 ;
+targetsize = -1 ;
+weightsize = -1 ;
+forget_when_training_set_changes = 0 ;
+nstages = 1 ;
+report_progress = 1 ;
+verbosity = 1 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 1 ;
+save_stat_collectors = 1 ;
+save_split_stats = 1 ;
+save_learners = 0 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 1 ;
+call_forget_in_run = 1 ;
+save_test_costs = 1 ;
+save_test_names = 1 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+finalize_learner = 0 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 0 ;
+enforce_clean_expdir = 1 ;
+redirect_stdout = 0 ;
+redirect_stderr = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/train_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/train_cost_names.txt	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/train_cost_names.txt	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]
+E[test1.E[base_confidence]]
+E[test1.E[base_reward_l2]]
+E[test1.E[base_reward_l1]]
+E[test2.E[class_error]]
+E[test2.E[base_confidence]]
+E[test2.E[base_reward_l2]]
+E[test2.E[base_reward_l1]]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/pytest.config	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/pytest.config	2009-03-27 15:16:14 UTC (rev 10066)
@@ -74,6 +74,22 @@
     
 """
 Test(
+    name = "PL_RegressionTree_MultiClassProb",
+    description = "Exercise basic functionality of RegressionTree with RegressionTreeMultiClassLeaveProb",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "regression_tree_multiclass_prob.pyplearn",
+    resources = [ "regression_tree_multiclass_prob.pyplearn" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None
+    )
+
+Test(
     name = "PL_RegressionTree_MultiClassFast",
     description = "Exercise basic functionality of RegressionTree with RegressionTreeMultiClassLeaveFast",
     category = "General",

Added: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_prob.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_prob.pyplearn	2009-03-26 19:44:24 UTC (rev 10065)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass_prob.pyplearn	2009-03-27 15:16:14 UTC (rev 10066)
@@ -0,0 +1,96 @@
+import os.path
+from plearn.pyplearn import *
+
+plarg_defaults.datatrain  = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat"
+plarg_defaults.datatest   = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat"
+
+dataset = pl.ConcatRowsVMatrix(
+sources = [ pl.AutoVMatrix(specification=plargs.datatrain,inputsize=2,targetsize=1),
+	pl.AutoVMatrix(specification=plargs.datatest,inputsize=2,targetsize=1) ],
+inputsize=2,
+targetsize=1,
+weightsize=0
+)
+
+learner = pl.HyperLearner(
+    option_fields = [ "nstages" ],
+    dont_restart_upon_change = [ "nstages" ] ,
+    provide_strategy_expdir = 1 ,
+    save_final_learner = 0 ,
+    provide_learner_expdir = 1 ,
+    forget_when_training_set_changes = 0 ,
+    nstages = 1 ,
+    report_progress = 1 ,
+    verbosity = 1 ,
+    learner = pl.RegressionTree(
+        nstages = 10
+        ,loss_function_weight = 1
+#        ,missing_is_valid = 0
+#        ,multiclass_outputs = []
+        ,maximum_number_of_nodes = 50
+        ,compute_train_stats = 1
+        ,complexity_penalty_factor = 0.0
+        ,output_confidence_target = 1
+        ,verbosity = 2
+        ,report_progress = 1
+        ,forget_when_training_set_changes = 1
+#        ,conf_rated_adaboost = 0
+        ,leave_template = pl.RegressionTreeMulticlassLeaveProb(nb_class=2)
+        ),
+    tester = pl.PTester(
+    splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,200), (0,200), (200,1) ])),
+    statnames = [ #'E[train.E[class_error]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[class_error]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]',
+                 'E[test2.E[class_error]]',  'E[test2.E[base_confidence]]',  'E[test2.E[base_reward_l2]]',  'E[test2.E[base_reward_l1]]'],
+    save_test_outputs = 0 ,
+    report_stats = 1  ,
+    save_initial_tester = 0 ,
+    save_learners = 0 ,
+    save_initial_learners = 0  ,
+    save_data_sets = 0  ,
+    save_test_costs = 0  ,
+    provide_learner_expdir = 1  ,
+    save_test_confidence = 0  ,
+    save_test_names = 0,
+    ),
+    strategy = [
+
+    pl.HyperOptimize(
+    which_cost = "E[test2.E[class_error]]" ,
+    provide_tester_expdir = 1 ,
+    oracle = pl.EarlyStoppingOracle(
+    option = "nstages" ,
+    range = [ 1, 5, 1 ],
+    min_value = -3.40282e+38 ,
+    max_value = 3.40282e+38 ,
+    max_degradation = 3.40282e+38 ,
+    relative_max_degradation = -1 ,
+    min_improvement = -3.40282e+38 ,
+    relative_min_improvement = -1 ,
+    max_degraded_steps = 120 ,
+    min_n_steps = 2 
+    )  # end of EarlyStoppingOracle
+    )  # end of sub_strategy.HyperOptimize
+    ]  # end of HyperLearner strategy
+    )
+splitter = pl.FractionSplitter(
+    splits = TMat(1,3, [ (0,1), (0,200), (200,1) ])
+    )
+tester = pl.PTester(
+    expdir = plargs.expdir,
+    dataset = dataset,
+    splitter = splitter,
+    learner = learner,
+    statnames = [#'E[train.E[class_error]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[class_error]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]',
+                 'E[test2.E[class_error]]',  'E[test2.E[base_confidence]]',  'E[test2.E[base_reward_l2]]',  'E[test2.E[base_reward_l1]]'],
+    provide_learner_expdir = 1,
+    save_test_costs = 1,
+    save_test_outputs = 1,
+    save_test_confidence = 0,
+    save_learners = 0
+
+    )
+
+def main():
+    return tester



From nouiz at mail.berlios.de  Fri Mar 27 16:50:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 27 Mar 2009 16:50:26 +0100
Subject: [Plearn-commits] r10067 - trunk/python_modules/plearn/pytest
Message-ID: <200903271550.n2RFoQZu027883@sheep.berlios.de>

Author: nouiz
Date: 2009-03-27 16:50:26 +0100 (Fri, 27 Mar 2009)
New Revision: 10067

Modified:
   trunk/python_modules/plearn/pytest/modes.py
Log:
don't delete dir if it don't exist.


Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2009-03-27 15:16:14 UTC (rev 10066)
+++ trunk/python_modules/plearn/pytest/modes.py	2009-03-27 15:50:26 UTC (rev 10067)
@@ -421,7 +421,8 @@
                     if os.path.exists(svn_results):
                         expected_results = test.resultsDirectory(tests.EXPECTED_RESULTS)
                         confirmed_results = expected_results+'.confirmed'
-                        shutil.rmtree(confirmed_results)#in case we crashed
+                        if os.path.exists(confirmed_results):
+                            shutil.rmtree(confirmed_results)#in case we crashed
                         os.rename(expected_results, confirmed_results)
                         os.rename(svn_results, expected_results)
                         self.migrate_results_trees(expected_results, confirmed_results)



From nouiz at mail.berlios.de  Fri Mar 27 16:53:01 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 27 Mar 2009 16:53:01 +0100
Subject: [Plearn-commits] r10068 - in trunk/plearn_learners:
 meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir
 meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0
 meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir
 meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0
 meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir
 meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0
 meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir
 meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
 regressors
 regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
 regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
 regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir
 r!
 egressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir
 regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir
Message-ID: <200903271553.n2RFr1Yh028296@sheep.berlios.de>

Author: nouiz
Date: 2009-03-27 16:53:00 +0100 (Fri, 27 Mar 2009)
New Revision: 10068

Modified:
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt
Log:
Don't remove the leave_template when we finilize the learner as we need it to reload the learner.


Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2009-03-27 15:53:00 UTC (rev 10068)
@@ -239,8 +239,20 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *7 ->RegressionTreeNode(
+leave_template = *7 ->RegressionTreeLeave(
+id = 0 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
+;
+root = *8 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -250,7 +262,7 @@
 split_feature_value = 0.188677163392186542 ;
 after_split_error = 0.165982151128678812 ;
 missing_node = *0 ;
-missing_leave = *8 ->RegressionTreeLeave(
+missing_leave = *9 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -263,7 +275,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *9 ->RegressionTreeNode(
+left_node = *10 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -273,7 +285,7 @@
 split_feature_value = 8.88178419700125232e-16 ;
 after_split_error = 0.0733752620545073397 ;
 missing_node = *0 ;
-missing_leave = *10 ->RegressionTreeLeave(
+missing_leave = *11 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -286,7 +298,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *11 ->RegressionTreeNode(
+left_node = *12 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -296,7 +308,7 @@
 split_feature_value = 0.185353117285409069 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *12 ->RegressionTreeLeave(
+missing_leave = *13 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -315,7 +327,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *13 ->RegressionTreeNode(
+right_node = *14 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -325,7 +337,7 @@
 split_feature_value = 3.44335671087492301e-13 ;
 after_split_error = 0.0528301886792452852 ;
 missing_node = *0 ;
-missing_leave = *14 ->RegressionTreeLeave(
+missing_leave = *15 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -338,7 +350,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *15 ->RegressionTreeNode(
+left_node = *16 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -348,7 +360,7 @@
 split_feature_value = 2.63677968348474678e-15 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *16 ->RegressionTreeLeave(
+missing_leave = *17 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -367,7 +379,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *17 ->RegressionTreeNode(
+right_node = *18 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -377,7 +389,7 @@
 split_feature_value = 0.0166672042033834122 ;
 after_split_error = 0.0452830188679245321 ;
 missing_node = *0 ;
-missing_leave = *18 ->RegressionTreeLeave(
+missing_leave = *19 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -400,7 +412,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *19 ->RegressionTreeNode(
+right_node = *20 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -410,7 +422,7 @@
 split_feature_value = 0.170219082142077621 ;
 after_split_error = 0.0448775543115166875 ;
 missing_node = *0 ;
-missing_leave = *20 ->RegressionTreeLeave(
+missing_leave = *21 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -423,7 +435,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *21 ->RegressionTreeNode(
+left_node = *22 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -433,7 +445,7 @@
 split_feature_value = 0.141699018667860999 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *22 ->RegressionTreeLeave(
+missing_leave = *23 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -452,7 +464,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *23 ->RegressionTreeNode(
+right_node = *24 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -462,7 +474,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0332075471698113078 ;
 missing_node = *0 ;
-missing_leave = *24 ->RegressionTreeLeave(
+missing_leave = *25 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -506,7 +518,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*25 ->RegressionTree(
+*26 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -514,8 +526,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *26 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *27 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -525,7 +537,7 @@
 split_feature_value = 0.482293993618237549 ;
 after_split_error = 0.260686628807433929 ;
 missing_node = *0 ;
-missing_leave = *27 ->RegressionTreeLeave(
+missing_leave = *28 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -538,7 +550,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *28 ->RegressionTreeNode(
+left_node = *29 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -548,7 +560,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.0130872483221476481 ;
 missing_node = *0 ;
-missing_leave = *29 ->RegressionTreeLeave(
+missing_leave = *30 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -561,7 +573,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *30 ->RegressionTreeNode(
+left_node = *31 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -571,7 +583,7 @@
 split_feature_value = 0.414475818795681961 ;
 after_split_error = 0.0120805369127516774 ;
 missing_node = *0 ;
-missing_leave = *31 ->RegressionTreeLeave(
+missing_leave = *32 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -590,7 +602,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *32 ->RegressionTreeNode(
+right_node = *33 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -600,7 +612,7 @@
 split_feature_value = 1.66171551518878857e-09 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *33 ->RegressionTreeLeave(
+missing_leave = *34 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -621,7 +633,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *34 ->RegressionTreeNode(
+right_node = *35 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -631,7 +643,7 @@
 split_feature_value = 0.999999999538717876 ;
 after_split_error = 0.145761307736665707 ;
 missing_node = *0 ;
-missing_leave = *35 ->RegressionTreeLeave(
+missing_leave = *36 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -644,7 +656,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *36 ->RegressionTreeNode(
+left_node = *37 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -654,7 +666,7 @@
 split_feature_value = 0.725372806051693186 ;
 after_split_error = 0.033557046979865772 ;
 missing_node = *0 ;
-missing_leave = *37 ->RegressionTreeLeave(
+missing_leave = *38 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -667,7 +679,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *38 ->RegressionTreeNode(
+left_node = *39 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -677,7 +689,7 @@
 split_feature_value = 0.00609705032829710447 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *39 ->RegressionTreeLeave(
+missing_leave = *40 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -696,7 +708,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *40 ->RegressionTreeNode(
+right_node = *41 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -706,7 +718,7 @@
 split_feature_value = 0.999935792696207582 ;
 after_split_error = 0.0100671140939597309 ;
 missing_node = *0 ;
-missing_leave = *41 ->RegressionTreeLeave(
+missing_leave = *42 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -727,7 +739,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *42 ->RegressionTreeNode(
+right_node = *43 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -737,7 +749,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0132194427496440392 ;
 missing_node = *0 ;
-missing_leave = *43 ->RegressionTreeLeave(
+missing_leave = *44 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -781,7 +793,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*44 ->RegressionTree(
+*45 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -789,8 +801,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *45 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *46 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -800,7 +812,7 @@
 split_feature_value = 0.995245802370935517 ;
 after_split_error = 0.376402584155050013 ;
 missing_node = *0 ;
-missing_leave = *46 ->RegressionTreeLeave(
+missing_leave = *47 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -813,7 +825,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *47 ->RegressionTreeNode(
+left_node = *48 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -823,7 +835,7 @@
 split_feature_value = 0.994286124455373455 ;
 after_split_error = 0.31500238638171546 ;
 missing_node = *0 ;
-missing_leave = *48 ->RegressionTreeLeave(
+missing_leave = *49 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -836,7 +848,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *49 ->RegressionTreeNode(
+left_node = *50 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -846,7 +858,7 @@
 split_feature_value = 8.32667268468867405e-16 ;
 after_split_error = 0.24550237059167368 ;
 missing_node = *0 ;
-missing_leave = *50 ->RegressionTreeLeave(
+missing_leave = *51 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -859,7 +871,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *51 ->RegressionTreeNode(
+left_node = *52 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -869,7 +881,7 @@
 split_feature_value = 0.180062798802320762 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *52 ->RegressionTreeLeave(
+missing_leave = *53 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -888,7 +900,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *53 ->RegressionTreeNode(
+right_node = *54 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -898,7 +910,7 @@
 split_feature_value = 0.242853945270868343 ;
 after_split_error = 0.19876061032130396 ;
 missing_node = *0 ;
-missing_leave = *54 ->RegressionTreeLeave(
+missing_leave = *55 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -911,7 +923,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *55 ->RegressionTreeNode(
+left_node = *56 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -921,7 +933,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0107349298100743173 ;
 missing_node = *0 ;
-missing_leave = *56 ->RegressionTreeLeave(
+missing_leave = *57 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -940,7 +952,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *57 ->RegressionTreeNode(
+right_node = *58 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -950,7 +962,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.165519772456188596 ;
 missing_node = *0 ;
-missing_leave = *58 ->RegressionTreeLeave(
+missing_leave = *59 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -973,7 +985,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *59 ->RegressionTreeNode(
+right_node = *60 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -983,7 +995,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *60 ->RegressionTreeLeave(
+missing_leave = *61 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1004,7 +1016,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *61 ->RegressionTreeNode(
+right_node = *62 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1014,7 +1026,7 @@
 split_feature_value = 0.893509913423101043 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *62 ->RegressionTreeLeave(
+missing_leave = *63 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1056,7 +1068,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*63 ->RegressionTree(
+*64 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1064,8 +1076,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *64 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *65 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1075,7 +1087,7 @@
 split_feature_value = 0.575210824891620121 ;
 after_split_error = 0.29407734793231588 ;
 missing_node = *0 ;
-missing_leave = *65 ->RegressionTreeLeave(
+missing_leave = *66 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1088,7 +1100,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *66 ->RegressionTreeNode(
+left_node = *67 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1098,7 +1110,7 @@
 split_feature_value = 0.861099804177139827 ;
 after_split_error = 0.181076457428609006 ;
 missing_node = *0 ;
-missing_leave = *67 ->RegressionTreeLeave(
+missing_leave = *68 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1111,7 +1123,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *68 ->RegressionTreeNode(
+left_node = *69 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1121,7 +1133,7 @@
 split_feature_value = 0.591195353843563365 ;
 after_split_error = 0.158960974397215959 ;
 missing_node = *0 ;
-missing_leave = *69 ->RegressionTreeLeave(
+missing_leave = *70 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1134,7 +1146,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *70 ->RegressionTreeNode(
+left_node = *71 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1144,7 +1156,7 @@
 split_feature_value = 0.187798288368589333 ;
 after_split_error = 0.111558538404175941 ;
 missing_node = *0 ;
-missing_leave = *71 ->RegressionTreeLeave(
+missing_leave = *72 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1163,7 +1175,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *72 ->RegressionTreeNode(
+right_node = *73 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1173,7 +1185,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0131695580600227936 ;
 missing_node = *0 ;
-missing_leave = *73 ->RegressionTreeLeave(
+missing_leave = *74 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1194,7 +1206,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *74 ->RegressionTreeNode(
+right_node = *75 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1204,7 +1216,7 @@
 split_feature_value = 0.999999941905052481 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *75 ->RegressionTreeLeave(
+missing_leave = *76 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1225,7 +1237,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *76 ->RegressionTreeNode(
+right_node = *77 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1235,7 +1247,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0130600399056774452 ;
 missing_node = *0 ;
-missing_leave = *77 ->RegressionTreeLeave(
+missing_leave = *78 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1248,7 +1260,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *78 ->RegressionTreeNode(
+left_node = *79 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1258,7 +1270,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *79 ->RegressionTreeLeave(
+missing_leave = *80 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1277,7 +1289,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *80 ->RegressionTreeNode(
+right_node = *81 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1287,7 +1299,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0123042505592841078 ;
 missing_node = *0 ;
-missing_leave = *81 ->RegressionTreeLeave(
+missing_leave = *82 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1331,7 +1343,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*82 ->RegressionTree(
+*83 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1339,8 +1351,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *83 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *84 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1350,7 +1362,7 @@
 split_feature_value = 0.00132212183813046336 ;
 after_split_error = 0.430920430920432751 ;
 missing_node = *0 ;
-missing_leave = *84 ->RegressionTreeLeave(
+missing_leave = *85 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1363,7 +1375,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *85 ->RegressionTreeNode(
+left_node = *86 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1373,7 +1385,7 @@
 split_feature_value = 0.0480606101777627803 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *86 ->RegressionTreeLeave(
+missing_leave = *87 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1392,7 +1404,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *87 ->RegressionTreeNode(
+right_node = *88 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1402,7 +1414,7 @@
 split_feature_value = 0.0173550052261667587 ;
 after_split_error = 0.387008857597093914 ;
 missing_node = *0 ;
-missing_leave = *88 ->RegressionTreeLeave(
+missing_leave = *89 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1415,7 +1427,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *89 ->RegressionTreeNode(
+left_node = *90 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1425,7 +1437,7 @@
 split_feature_value = 0.135512975844814643 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *90 ->RegressionTreeLeave(
+missing_leave = *91 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1444,7 +1456,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *91 ->RegressionTreeNode(
+right_node = *92 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1454,7 +1466,7 @@
 split_feature_value = 0.0689879291310910303 ;
 after_split_error = 0.351230694980696034 ;
 missing_node = *0 ;
-missing_leave = *92 ->RegressionTreeLeave(
+missing_leave = *93 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1467,7 +1479,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *93 ->RegressionTreeNode(
+left_node = *94 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1477,7 +1489,7 @@
 split_feature_value = 0.510088881577538289 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *94 ->RegressionTreeLeave(
+missing_leave = *95 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1496,7 +1508,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *95 ->RegressionTreeNode(
+right_node = *96 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1506,7 +1518,7 @@
 split_feature_value = 0.141930657011306749 ;
 after_split_error = 0.314935988620199336 ;
 missing_node = *0 ;
-missing_leave = *96 ->RegressionTreeLeave(
+missing_leave = *97 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1519,7 +1531,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *97 ->RegressionTreeNode(
+left_node = *98 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1529,7 +1541,7 @@
 split_feature_value = 0.122353510242232788 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *98 ->RegressionTreeLeave(
+missing_leave = *99 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1548,7 +1560,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *99 ->RegressionTreeNode(
+right_node = *100 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1558,7 +1570,7 @@
 split_feature_value = 0.16348885181551448 ;
 after_split_error = 0.249176005273566148 ;
 missing_node = *0 ;
-missing_leave = *100 ->RegressionTreeLeave(
+missing_leave = *101 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1612,7 +1624,7 @@
 initial_sum_weights = 1.00000000000000244 ;
 example_weights = 150 [ 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00506327259806789182 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.013756488510808379 0.000521219238036399468 0.00496216976946178487 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.116188573346790752 0.0331580145383834901 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0760448245655967053 0.00119536787918376715 0.000521219238036399468 0.00052121923803639946!
 8 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.00119536787918376715 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.00506327259806789182 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00119536787918376715 0.00119536787918376715 0.000521219238036399468 0.000521219238036399468 0.00496216976946178487 0.00216366726200823171 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.00216366726200823171 0.000521219238036399468 0.00119536787918376715 0.000521219238036399468 0.000521219238036399468 0.0321920345819545345 0.048203934903343168 0.000521219238036399468 0.00216366726200823171 0.0321920345819545345 !
 0.021649179367261618 0.000521219238036399468 0.000521219238036!
 399468 0
.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00216366726200823171 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.0210184819737942973 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.00331388591284825307 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00216366726200823171 0.00506327259806789182 0.0210184819737942973 0.0331580145383834901 0.0321920345819545345 0.049650!
 380756869221 0.000521219238036399468 0.00521520520832954079 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00119536787918376715 0.000521219238036399468 ] ;
 learners_error = 5 [ 0.0933333333333331433 0.0908613445378149004 0.194130827514584103 0.135907417796919783 0.303636927486942543 ] ;
-weak_learner_template = *101 ->RegressionTree(
+weak_learner_template = *102 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1620,19 +1632,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *102 ->RegressionTreeLeave(
-id = 0 ;
-missing_leave = 0 ;
-loss_function_weight = 0 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output_confidence_target = 1  )
-;
+leave_template = *7  ;
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10047"
+__REVISION__ = "PL10066"
 conf                                          = False
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-03-27 15:53:00 UTC (rev 10068)
@@ -239,8 +239,20 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *7 ->RegressionTreeNode(
+leave_template = *7 ->RegressionTreeLeave(
+id = 0 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
+;
+root = *8 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -250,7 +262,7 @@
 split_feature_value = 0.188677163392186542 ;
 after_split_error = 0.165982151128678812 ;
 missing_node = *0 ;
-missing_leave = *8 ->RegressionTreeLeave(
+missing_leave = *9 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -263,7 +275,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *9 ->RegressionTreeNode(
+left_node = *10 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -273,7 +285,7 @@
 split_feature_value = 8.88178419700125232e-16 ;
 after_split_error = 0.0733752620545073397 ;
 missing_node = *0 ;
-missing_leave = *10 ->RegressionTreeLeave(
+missing_leave = *11 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -286,7 +298,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *11 ->RegressionTreeNode(
+left_node = *12 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -296,7 +308,7 @@
 split_feature_value = 0.185353117285409069 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *12 ->RegressionTreeLeave(
+missing_leave = *13 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -315,7 +327,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *13 ->RegressionTreeNode(
+right_node = *14 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -325,7 +337,7 @@
 split_feature_value = 3.44335671087492301e-13 ;
 after_split_error = 0.0528301886792452852 ;
 missing_node = *0 ;
-missing_leave = *14 ->RegressionTreeLeave(
+missing_leave = *15 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -338,7 +350,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *15 ->RegressionTreeNode(
+left_node = *16 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -348,7 +360,7 @@
 split_feature_value = 2.63677968348474678e-15 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *16 ->RegressionTreeLeave(
+missing_leave = *17 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -367,7 +379,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *17 ->RegressionTreeNode(
+right_node = *18 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -377,7 +389,7 @@
 split_feature_value = 0.0166672042033834122 ;
 after_split_error = 0.0452830188679245321 ;
 missing_node = *0 ;
-missing_leave = *18 ->RegressionTreeLeave(
+missing_leave = *19 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -400,7 +412,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *19 ->RegressionTreeNode(
+right_node = *20 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -410,7 +422,7 @@
 split_feature_value = 0.170219082142077621 ;
 after_split_error = 0.0448775543115166875 ;
 missing_node = *0 ;
-missing_leave = *20 ->RegressionTreeLeave(
+missing_leave = *21 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -423,7 +435,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *21 ->RegressionTreeNode(
+left_node = *22 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -433,7 +445,7 @@
 split_feature_value = 0.141699018667860999 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *22 ->RegressionTreeLeave(
+missing_leave = *23 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -452,7 +464,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *23 ->RegressionTreeNode(
+right_node = *24 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -462,7 +474,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0332075471698113078 ;
 missing_node = *0 ;
-missing_leave = *24 ->RegressionTreeLeave(
+missing_leave = *25 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -506,7 +518,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*25 ->RegressionTree(
+*26 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -514,8 +526,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *26 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *27 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -525,7 +537,7 @@
 split_feature_value = 0.482293993618237549 ;
 after_split_error = 0.260686628807433929 ;
 missing_node = *0 ;
-missing_leave = *27 ->RegressionTreeLeave(
+missing_leave = *28 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -538,7 +550,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *28 ->RegressionTreeNode(
+left_node = *29 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -548,7 +560,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.0130872483221476481 ;
 missing_node = *0 ;
-missing_leave = *29 ->RegressionTreeLeave(
+missing_leave = *30 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -561,7 +573,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *30 ->RegressionTreeNode(
+left_node = *31 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -571,7 +583,7 @@
 split_feature_value = 0.414475818795681961 ;
 after_split_error = 0.0120805369127516774 ;
 missing_node = *0 ;
-missing_leave = *31 ->RegressionTreeLeave(
+missing_leave = *32 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -590,7 +602,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *32 ->RegressionTreeNode(
+right_node = *33 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -600,7 +612,7 @@
 split_feature_value = 1.66171551518878857e-09 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *33 ->RegressionTreeLeave(
+missing_leave = *34 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -621,7 +633,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *34 ->RegressionTreeNode(
+right_node = *35 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -631,7 +643,7 @@
 split_feature_value = 0.999999999538717876 ;
 after_split_error = 0.145761307736665707 ;
 missing_node = *0 ;
-missing_leave = *35 ->RegressionTreeLeave(
+missing_leave = *36 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -644,7 +656,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *36 ->RegressionTreeNode(
+left_node = *37 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -654,7 +666,7 @@
 split_feature_value = 0.725372806051693186 ;
 after_split_error = 0.033557046979865772 ;
 missing_node = *0 ;
-missing_leave = *37 ->RegressionTreeLeave(
+missing_leave = *38 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -667,7 +679,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *38 ->RegressionTreeNode(
+left_node = *39 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -677,7 +689,7 @@
 split_feature_value = 0.00609705032829710447 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *39 ->RegressionTreeLeave(
+missing_leave = *40 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -696,7 +708,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *40 ->RegressionTreeNode(
+right_node = *41 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -706,7 +718,7 @@
 split_feature_value = 0.999935792696207582 ;
 after_split_error = 0.0100671140939597309 ;
 missing_node = *0 ;
-missing_leave = *41 ->RegressionTreeLeave(
+missing_leave = *42 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -727,7 +739,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *42 ->RegressionTreeNode(
+right_node = *43 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -737,7 +749,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0132194427496440392 ;
 missing_node = *0 ;
-missing_leave = *43 ->RegressionTreeLeave(
+missing_leave = *44 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -781,7 +793,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*44 ->RegressionTree(
+*45 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -789,8 +801,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *45 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *46 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -800,7 +812,7 @@
 split_feature_value = 0.995245802370935517 ;
 after_split_error = 0.376402584155050013 ;
 missing_node = *0 ;
-missing_leave = *46 ->RegressionTreeLeave(
+missing_leave = *47 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -813,7 +825,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *47 ->RegressionTreeNode(
+left_node = *48 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -823,7 +835,7 @@
 split_feature_value = 0.994286124455373455 ;
 after_split_error = 0.31500238638171546 ;
 missing_node = *0 ;
-missing_leave = *48 ->RegressionTreeLeave(
+missing_leave = *49 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -836,7 +848,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *49 ->RegressionTreeNode(
+left_node = *50 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -846,7 +858,7 @@
 split_feature_value = 8.32667268468867405e-16 ;
 after_split_error = 0.24550237059167368 ;
 missing_node = *0 ;
-missing_leave = *50 ->RegressionTreeLeave(
+missing_leave = *51 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -859,7 +871,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *51 ->RegressionTreeNode(
+left_node = *52 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -869,7 +881,7 @@
 split_feature_value = 0.180062798802320762 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *52 ->RegressionTreeLeave(
+missing_leave = *53 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -888,7 +900,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *53 ->RegressionTreeNode(
+right_node = *54 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -898,7 +910,7 @@
 split_feature_value = 0.242853945270868343 ;
 after_split_error = 0.19876061032130396 ;
 missing_node = *0 ;
-missing_leave = *54 ->RegressionTreeLeave(
+missing_leave = *55 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -911,7 +923,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *55 ->RegressionTreeNode(
+left_node = *56 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -921,7 +933,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0107349298100743173 ;
 missing_node = *0 ;
-missing_leave = *56 ->RegressionTreeLeave(
+missing_leave = *57 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -940,7 +952,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *57 ->RegressionTreeNode(
+right_node = *58 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -950,7 +962,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.165519772456188596 ;
 missing_node = *0 ;
-missing_leave = *58 ->RegressionTreeLeave(
+missing_leave = *59 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -973,7 +985,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *59 ->RegressionTreeNode(
+right_node = *60 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -983,7 +995,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *60 ->RegressionTreeLeave(
+missing_leave = *61 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1004,7 +1016,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *61 ->RegressionTreeNode(
+right_node = *62 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1014,7 +1026,7 @@
 split_feature_value = 0.893509913423101043 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *62 ->RegressionTreeLeave(
+missing_leave = *63 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1056,7 +1068,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*63 ->RegressionTree(
+*64 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1064,8 +1076,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *64 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *65 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1075,7 +1087,7 @@
 split_feature_value = 0.575210824891620121 ;
 after_split_error = 0.29407734793231588 ;
 missing_node = *0 ;
-missing_leave = *65 ->RegressionTreeLeave(
+missing_leave = *66 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1088,7 +1100,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *66 ->RegressionTreeNode(
+left_node = *67 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1098,7 +1110,7 @@
 split_feature_value = 0.861099804177139827 ;
 after_split_error = 0.181076457428609006 ;
 missing_node = *0 ;
-missing_leave = *67 ->RegressionTreeLeave(
+missing_leave = *68 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1111,7 +1123,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *68 ->RegressionTreeNode(
+left_node = *69 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1121,7 +1133,7 @@
 split_feature_value = 0.591195353843563365 ;
 after_split_error = 0.158960974397215959 ;
 missing_node = *0 ;
-missing_leave = *69 ->RegressionTreeLeave(
+missing_leave = *70 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1134,7 +1146,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *70 ->RegressionTreeNode(
+left_node = *71 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1144,7 +1156,7 @@
 split_feature_value = 0.187798288368589333 ;
 after_split_error = 0.111558538404175941 ;
 missing_node = *0 ;
-missing_leave = *71 ->RegressionTreeLeave(
+missing_leave = *72 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1163,7 +1175,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *72 ->RegressionTreeNode(
+right_node = *73 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1173,7 +1185,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0131695580600227936 ;
 missing_node = *0 ;
-missing_leave = *73 ->RegressionTreeLeave(
+missing_leave = *74 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1194,7 +1206,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *74 ->RegressionTreeNode(
+right_node = *75 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1204,7 +1216,7 @@
 split_feature_value = 0.999999941905052481 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *75 ->RegressionTreeLeave(
+missing_leave = *76 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1225,7 +1237,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *76 ->RegressionTreeNode(
+right_node = *77 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1235,7 +1247,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0130600399056774452 ;
 missing_node = *0 ;
-missing_leave = *77 ->RegressionTreeLeave(
+missing_leave = *78 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1248,7 +1260,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *78 ->RegressionTreeNode(
+left_node = *79 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1258,7 +1270,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *79 ->RegressionTreeLeave(
+missing_leave = *80 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1277,7 +1289,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *80 ->RegressionTreeNode(
+right_node = *81 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1287,7 +1299,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0123042505592841078 ;
 missing_node = *0 ;
-missing_leave = *81 ->RegressionTreeLeave(
+missing_leave = *82 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1331,7 +1343,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*82 ->RegressionTree(
+*83 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1339,8 +1351,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *83 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *84 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1350,7 +1362,7 @@
 split_feature_value = 0.00132212183813046336 ;
 after_split_error = 0.430920430920432751 ;
 missing_node = *0 ;
-missing_leave = *84 ->RegressionTreeLeave(
+missing_leave = *85 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1363,7 +1375,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *85 ->RegressionTreeNode(
+left_node = *86 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1373,7 +1385,7 @@
 split_feature_value = 0.0480606101777627803 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *86 ->RegressionTreeLeave(
+missing_leave = *87 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1392,7 +1404,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *87 ->RegressionTreeNode(
+right_node = *88 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1402,7 +1414,7 @@
 split_feature_value = 0.0173550052261667587 ;
 after_split_error = 0.387008857597093914 ;
 missing_node = *0 ;
-missing_leave = *88 ->RegressionTreeLeave(
+missing_leave = *89 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1415,7 +1427,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *89 ->RegressionTreeNode(
+left_node = *90 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1425,7 +1437,7 @@
 split_feature_value = 0.135512975844814643 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *90 ->RegressionTreeLeave(
+missing_leave = *91 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1444,7 +1456,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *91 ->RegressionTreeNode(
+right_node = *92 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1454,7 +1466,7 @@
 split_feature_value = 0.0689879291310910303 ;
 after_split_error = 0.351230694980696034 ;
 missing_node = *0 ;
-missing_leave = *92 ->RegressionTreeLeave(
+missing_leave = *93 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1467,7 +1479,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *93 ->RegressionTreeNode(
+left_node = *94 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1477,7 +1489,7 @@
 split_feature_value = 0.510088881577538289 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *94 ->RegressionTreeLeave(
+missing_leave = *95 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1496,7 +1508,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *95 ->RegressionTreeNode(
+right_node = *96 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1506,7 +1518,7 @@
 split_feature_value = 0.141930657011306749 ;
 after_split_error = 0.314935988620199336 ;
 missing_node = *0 ;
-missing_leave = *96 ->RegressionTreeLeave(
+missing_leave = *97 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1519,7 +1531,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *97 ->RegressionTreeNode(
+left_node = *98 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1529,7 +1541,7 @@
 split_feature_value = 0.122353510242232788 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *98 ->RegressionTreeLeave(
+missing_leave = *99 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1548,7 +1560,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *99 ->RegressionTreeNode(
+right_node = *100 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1558,7 +1570,7 @@
 split_feature_value = 0.16348885181551448 ;
 after_split_error = 0.249176005273566148 ;
 missing_node = *0 ;
-missing_leave = *100 ->RegressionTreeLeave(
+missing_leave = *101 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1612,7 +1624,7 @@
 initial_sum_weights = 1.00000000000000244 ;
 example_weights = 150 [ 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00506463385355691853 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0137597517091007995 0.000521450777360935947 0.00496250851640710373 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.116156001516510926 0.0331601320626456475 0.000521450777360935947 0.00331503764440590001 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0481987748833352936 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0760294877744166625 0.00119558135134761748 0.000521450777360935947 0.000521450777360935!
 947 0.000521450777360935947 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590001 0.00119558135134761748 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.00506463385355691853 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00119558135134761748 0.00119558135134761748 0.000521450777360935947 0.000521450777360935947 0.00496250851640710373 0.00216438966752365231 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.00216438966752365231 0.000521450777360935947 0.00119558135134761748 0.000521450777360935947 0.000521450777360935947 0.0321975776211230746 0.0481987748833352936 0.000521450777360935947 0.00216438966752365231 0.03219757762112307!
 46 0.0216502661232893662 0.000521450777360935947 0.00052145077!
 73609359
47 0.000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00216438966752365231 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590001 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0481987748833352936 0.000521450777360935947 0.000521450777360935947 0.0481987748833352936 0.0210218138668946085 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590001 0.000521450777360935947 0.000521450777360935947 0.0481987748833352936 0.00331503764440590001 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00216438966752365231 0.00506463385355691853 0.0210218138668946085 0.0331601320626456475 0.0321975776211230746 0!
 .0496396890224618387 0.000521450777360935947 0.00521604231874619323 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00119558135134761748 0.000521450777360935947 ] ;
 learners_error = 5 [ 0.0933333333333331433 0.0908630479652438872 0.194138154481255237 0.135911099894630349 0.303623351488696902 ] ;
-weak_learner_template = *101 ->RegressionTree(
+weak_learner_template = *102 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1620,19 +1632,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *102 ->RegressionTreeLeave(
-id = 0 ;
-missing_leave = 0 ;
-loss_function_weight = 0 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output_confidence_target = 1  )
-;
+leave_template = *7  ;
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10047"
+__REVISION__ = "PL10066"
 conf                                          = True
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-03-27 15:53:00 UTC (rev 10068)
@@ -239,8 +239,20 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *7 ->RegressionTreeNode(
+leave_template = *7 ->RegressionTreeLeave(
+id = 0 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
+;
+root = *8 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -250,7 +262,7 @@
 split_feature_value = 0.188677163392186542 ;
 after_split_error = 0.165982151128678812 ;
 missing_node = *0 ;
-missing_leave = *8 ->RegressionTreeLeave(
+missing_leave = *9 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -263,7 +275,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *9 ->RegressionTreeNode(
+left_node = *10 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -273,7 +285,7 @@
 split_feature_value = 8.88178419700125232e-16 ;
 after_split_error = 0.0733752620545073397 ;
 missing_node = *0 ;
-missing_leave = *10 ->RegressionTreeLeave(
+missing_leave = *11 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -286,7 +298,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *11 ->RegressionTreeNode(
+left_node = *12 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -296,7 +308,7 @@
 split_feature_value = 0.185353117285409069 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *12 ->RegressionTreeLeave(
+missing_leave = *13 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -315,7 +327,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *13 ->RegressionTreeNode(
+right_node = *14 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -325,7 +337,7 @@
 split_feature_value = 3.44335671087492301e-13 ;
 after_split_error = 0.0528301886792452852 ;
 missing_node = *0 ;
-missing_leave = *14 ->RegressionTreeLeave(
+missing_leave = *15 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -338,7 +350,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *15 ->RegressionTreeNode(
+left_node = *16 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -348,7 +360,7 @@
 split_feature_value = 2.63677968348474678e-15 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *16 ->RegressionTreeLeave(
+missing_leave = *17 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -367,7 +379,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *17 ->RegressionTreeNode(
+right_node = *18 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -377,7 +389,7 @@
 split_feature_value = 0.0166672042033834122 ;
 after_split_error = 0.0452830188679245321 ;
 missing_node = *0 ;
-missing_leave = *18 ->RegressionTreeLeave(
+missing_leave = *19 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -400,7 +412,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *19 ->RegressionTreeNode(
+right_node = *20 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -410,7 +422,7 @@
 split_feature_value = 0.170219082142077621 ;
 after_split_error = 0.0448775543115166875 ;
 missing_node = *0 ;
-missing_leave = *20 ->RegressionTreeLeave(
+missing_leave = *21 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -423,7 +435,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *21 ->RegressionTreeNode(
+left_node = *22 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -433,7 +445,7 @@
 split_feature_value = 0.141699018667860999 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *22 ->RegressionTreeLeave(
+missing_leave = *23 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -452,7 +464,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *23 ->RegressionTreeNode(
+right_node = *24 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -462,7 +474,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0332075471698113078 ;
 missing_node = *0 ;
-missing_leave = *24 ->RegressionTreeLeave(
+missing_leave = *25 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -506,7 +518,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*25 ->RegressionTree(
+*26 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -514,8 +526,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *26 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *27 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -525,7 +537,7 @@
 split_feature_value = 0.482293993618237549 ;
 after_split_error = 0.260686628807433929 ;
 missing_node = *0 ;
-missing_leave = *27 ->RegressionTreeLeave(
+missing_leave = *28 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -538,7 +550,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *28 ->RegressionTreeNode(
+left_node = *29 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -548,7 +560,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.0130872483221476481 ;
 missing_node = *0 ;
-missing_leave = *29 ->RegressionTreeLeave(
+missing_leave = *30 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -561,7 +573,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *30 ->RegressionTreeNode(
+left_node = *31 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -571,7 +583,7 @@
 split_feature_value = 0.414475818795681961 ;
 after_split_error = 0.0120805369127516774 ;
 missing_node = *0 ;
-missing_leave = *31 ->RegressionTreeLeave(
+missing_leave = *32 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -590,7 +602,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *32 ->RegressionTreeNode(
+right_node = *33 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -600,7 +612,7 @@
 split_feature_value = 1.66171551518878857e-09 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *33 ->RegressionTreeLeave(
+missing_leave = *34 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -621,7 +633,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *34 ->RegressionTreeNode(
+right_node = *35 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -631,7 +643,7 @@
 split_feature_value = 0.999999999538717876 ;
 after_split_error = 0.145761307736665707 ;
 missing_node = *0 ;
-missing_leave = *35 ->RegressionTreeLeave(
+missing_leave = *36 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -644,7 +656,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *36 ->RegressionTreeNode(
+left_node = *37 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -654,7 +666,7 @@
 split_feature_value = 0.725372806051693186 ;
 after_split_error = 0.033557046979865772 ;
 missing_node = *0 ;
-missing_leave = *37 ->RegressionTreeLeave(
+missing_leave = *38 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -667,7 +679,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *38 ->RegressionTreeNode(
+left_node = *39 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -677,7 +689,7 @@
 split_feature_value = 0.00609705032829710447 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *39 ->RegressionTreeLeave(
+missing_leave = *40 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -696,7 +708,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *40 ->RegressionTreeNode(
+right_node = *41 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -706,7 +718,7 @@
 split_feature_value = 0.999935792696207582 ;
 after_split_error = 0.0100671140939597309 ;
 missing_node = *0 ;
-missing_leave = *41 ->RegressionTreeLeave(
+missing_leave = *42 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -727,7 +739,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *42 ->RegressionTreeNode(
+right_node = *43 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -737,7 +749,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0132194427496440392 ;
 missing_node = *0 ;
-missing_leave = *43 ->RegressionTreeLeave(
+missing_leave = *44 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -781,7 +793,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*44 ->RegressionTree(
+*45 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -789,8 +801,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *45 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *46 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -800,7 +812,7 @@
 split_feature_value = 0.995245802370935517 ;
 after_split_error = 0.376402584155050013 ;
 missing_node = *0 ;
-missing_leave = *46 ->RegressionTreeLeave(
+missing_leave = *47 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -813,7 +825,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *47 ->RegressionTreeNode(
+left_node = *48 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -823,7 +835,7 @@
 split_feature_value = 0.994286124455373455 ;
 after_split_error = 0.31500238638171546 ;
 missing_node = *0 ;
-missing_leave = *48 ->RegressionTreeLeave(
+missing_leave = *49 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -836,7 +848,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *49 ->RegressionTreeNode(
+left_node = *50 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -846,7 +858,7 @@
 split_feature_value = 8.32667268468867405e-16 ;
 after_split_error = 0.24550237059167368 ;
 missing_node = *0 ;
-missing_leave = *50 ->RegressionTreeLeave(
+missing_leave = *51 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -859,7 +871,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *51 ->RegressionTreeNode(
+left_node = *52 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -869,7 +881,7 @@
 split_feature_value = 0.180062798802320762 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *52 ->RegressionTreeLeave(
+missing_leave = *53 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -888,7 +900,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *53 ->RegressionTreeNode(
+right_node = *54 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -898,7 +910,7 @@
 split_feature_value = 0.242853945270868343 ;
 after_split_error = 0.19876061032130396 ;
 missing_node = *0 ;
-missing_leave = *54 ->RegressionTreeLeave(
+missing_leave = *55 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -911,7 +923,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *55 ->RegressionTreeNode(
+left_node = *56 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -921,7 +933,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0107349298100743173 ;
 missing_node = *0 ;
-missing_leave = *56 ->RegressionTreeLeave(
+missing_leave = *57 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -940,7 +952,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *57 ->RegressionTreeNode(
+right_node = *58 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -950,7 +962,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.165519772456188596 ;
 missing_node = *0 ;
-missing_leave = *58 ->RegressionTreeLeave(
+missing_leave = *59 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -973,7 +985,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *59 ->RegressionTreeNode(
+right_node = *60 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -983,7 +995,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *60 ->RegressionTreeLeave(
+missing_leave = *61 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1004,7 +1016,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *61 ->RegressionTreeNode(
+right_node = *62 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1014,7 +1026,7 @@
 split_feature_value = 0.893509913423101043 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *62 ->RegressionTreeLeave(
+missing_leave = *63 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1056,7 +1068,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*63 ->RegressionTree(
+*64 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1064,8 +1076,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *64 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *65 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1075,7 +1087,7 @@
 split_feature_value = 0.575210824891620121 ;
 after_split_error = 0.29407734793231588 ;
 missing_node = *0 ;
-missing_leave = *65 ->RegressionTreeLeave(
+missing_leave = *66 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1088,7 +1100,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *66 ->RegressionTreeNode(
+left_node = *67 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1098,7 +1110,7 @@
 split_feature_value = 0.861099804177139827 ;
 after_split_error = 0.181076457428609006 ;
 missing_node = *0 ;
-missing_leave = *67 ->RegressionTreeLeave(
+missing_leave = *68 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1111,7 +1123,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *68 ->RegressionTreeNode(
+left_node = *69 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1121,7 +1133,7 @@
 split_feature_value = 0.591195353843563365 ;
 after_split_error = 0.158960974397215959 ;
 missing_node = *0 ;
-missing_leave = *69 ->RegressionTreeLeave(
+missing_leave = *70 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1134,7 +1146,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *70 ->RegressionTreeNode(
+left_node = *71 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1144,7 +1156,7 @@
 split_feature_value = 0.187798288368589333 ;
 after_split_error = 0.111558538404175941 ;
 missing_node = *0 ;
-missing_leave = *71 ->RegressionTreeLeave(
+missing_leave = *72 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1163,7 +1175,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *72 ->RegressionTreeNode(
+right_node = *73 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1173,7 +1185,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0131695580600227936 ;
 missing_node = *0 ;
-missing_leave = *73 ->RegressionTreeLeave(
+missing_leave = *74 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1194,7 +1206,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *74 ->RegressionTreeNode(
+right_node = *75 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1204,7 +1216,7 @@
 split_feature_value = 0.999999941905052481 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *75 ->RegressionTreeLeave(
+missing_leave = *76 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1225,7 +1237,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *76 ->RegressionTreeNode(
+right_node = *77 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1235,7 +1247,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0130600399056774452 ;
 missing_node = *0 ;
-missing_leave = *77 ->RegressionTreeLeave(
+missing_leave = *78 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1248,7 +1260,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *78 ->RegressionTreeNode(
+left_node = *79 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1258,7 +1270,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *79 ->RegressionTreeLeave(
+missing_leave = *80 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1277,7 +1289,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *80 ->RegressionTreeNode(
+right_node = *81 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1287,7 +1299,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0123042505592841078 ;
 missing_node = *0 ;
-missing_leave = *81 ->RegressionTreeLeave(
+missing_leave = *82 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1331,7 +1343,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
-*82 ->RegressionTree(
+*83 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1339,8 +1351,8 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *0 ;
-root = *83 ->RegressionTreeNode(
+leave_template = *7  ;
+root = *84 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1350,7 +1362,7 @@
 split_feature_value = 0.00132212183813046336 ;
 after_split_error = 0.430920430920432751 ;
 missing_node = *0 ;
-missing_leave = *84 ->RegressionTreeLeave(
+missing_leave = *85 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1363,7 +1375,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *85 ->RegressionTreeNode(
+left_node = *86 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1373,7 +1385,7 @@
 split_feature_value = 0.0480606101777627803 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *86 ->RegressionTreeLeave(
+missing_leave = *87 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1392,7 +1404,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *87 ->RegressionTreeNode(
+right_node = *88 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1402,7 +1414,7 @@
 split_feature_value = 0.0173550052261667587 ;
 after_split_error = 0.387008857597093914 ;
 missing_node = *0 ;
-missing_leave = *88 ->RegressionTreeLeave(
+missing_leave = *89 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1415,7 +1427,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *89 ->RegressionTreeNode(
+left_node = *90 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1425,7 +1437,7 @@
 split_feature_value = 0.135512975844814643 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *90 ->RegressionTreeLeave(
+missing_leave = *91 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1444,7 +1456,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *91 ->RegressionTreeNode(
+right_node = *92 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1454,7 +1466,7 @@
 split_feature_value = 0.0689879291310910303 ;
 after_split_error = 0.351230694980696034 ;
 missing_node = *0 ;
-missing_leave = *92 ->RegressionTreeLeave(
+missing_leave = *93 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1467,7 +1479,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *93 ->RegressionTreeNode(
+left_node = *94 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -1477,7 +1489,7 @@
 split_feature_value = 0.510088881577538289 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *94 ->RegressionTreeLeave(
+missing_leave = *95 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1496,7 +1508,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *95 ->RegressionTreeNode(
+right_node = *96 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1506,7 +1518,7 @@
 split_feature_value = 0.141930657011306749 ;
 after_split_error = 0.314935988620199336 ;
 missing_node = *0 ;
-missing_leave = *96 ->RegressionTreeLeave(
+missing_leave = *97 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1519,7 +1531,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *97 ->RegressionTreeNode(
+left_node = *98 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1529,7 +1541,7 @@
 split_feature_value = 0.122353510242232788 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *98 ->RegressionTreeLeave(
+missing_leave = *99 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1548,7 +1560,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *99 ->RegressionTreeNode(
+right_node = *100 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -1558,7 +1570,7 @@
 split_feature_value = 0.16348885181551448 ;
 after_split_error = 0.249176005273566148 ;
 missing_node = *0 ;
-missing_leave = *100 ->RegressionTreeLeave(
+missing_leave = *101 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1612,7 +1624,7 @@
 initial_sum_weights = 1.00000000000000244 ;
 example_weights = 150 [ 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00506327259806789182 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.013756488510808379 0.000521219238036399468 0.00496216976946178487 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.116188573346790752 0.0331580145383834901 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0760448245655967053 0.00119536787918376715 0.000521219238036399468 0.00052121923803639946!
 8 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.00119536787918376715 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.00506327259806789182 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00119536787918376715 0.00119536787918376715 0.000521219238036399468 0.000521219238036399468 0.00496216976946178487 0.00216366726200823171 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.00216366726200823171 0.000521219238036399468 0.00119536787918376715 0.000521219238036399468 0.000521219238036399468 0.0321920345819545345 0.048203934903343168 0.000521219238036399468 0.00216366726200823171 0.0321920345819545345 !
 0.021649179367261618 0.000521219238036399468 0.000521219238036!
 399468 0
.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00216366726200823171 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.0210184819737942973 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.00331388591284825307 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00216366726200823171 0.00506327259806789182 0.0210184819737942973 0.0331580145383834901 0.0321920345819545345 0.049650!
 380756869221 0.000521219238036399468 0.00521520520832954079 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00119536787918376715 0.000521219238036399468 ] ;
 learners_error = 5 [ 0.0933333333333331433 0.0908613445378149004 0.194130827514584103 0.135907417796919783 0.303636927486942543 ] ;
-weak_learner_template = *101 ->RegressionTree(
+weak_learner_template = *102 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1620,19 +1632,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *102 ->RegressionTreeLeave(
-id = 0 ;
-missing_leave = 0 ;
-loss_function_weight = 0 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output_confidence_target = 1  )
-;
+leave_template = *7  ;
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10047"
+__REVISION__ = "PL10066"
 conf                                          = False
 pseudo                                        = True

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-03-27 15:53:00 UTC (rev 10068)
@@ -256,8 +256,20 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *0 ;
-root = *8 ->RegressionTreeNode(
+leave_template = *8 ->RegressionTreeLeave(
+id = 0 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2 ;
+output_confidence_target = 1  )
+;
+root = *9 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -267,7 +279,7 @@
 split_feature_value = 0.00125079586853901747 ;
 after_split_error = 0.074181818181818418 ;
 missing_node = *0 ;
-missing_leave = *9 ->RegressionTreeLeave(
+missing_leave = *10 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -280,7 +292,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *10 ->RegressionTreeNode(
+left_node = *11 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -290,7 +302,7 @@
 split_feature_value = 0.000357032461916012567 ;
 after_split_error = 0.0266666666666666684 ;
 missing_node = *0 ;
-missing_leave = *11 ->RegressionTreeLeave(
+missing_leave = *12 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -303,7 +315,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *12 ->RegressionTreeNode(
+left_node = *13 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -313,7 +325,7 @@
 split_feature_value = 0.113038628061597313 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *13 ->RegressionTreeLeave(
+missing_leave = *14 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -332,7 +344,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *14 ->RegressionTreeNode(
+right_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -342,7 +354,7 @@
 split_feature_value = 0.000981625552665510437 ;
 after_split_error = 0.0106666666666666646 ;
 missing_node = *0 ;
-missing_leave = *15 ->RegressionTreeLeave(
+missing_leave = *16 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -355,7 +367,7 @@
 loss_function_factor = 2 ;
 output_confidence_target = 1  )
 ;
-left_node = *16 ->RegressionTreeNode(
+left_node = *17 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -365,7 +377,7 @@
 split_feature_value = 0.000528285193333644099 ;
 after_split_error = 0.00666666666666666449 ;
 missing_node = *0 ;
-missing_leave = *17 ->RegressionTreeLeave(
+missing_leave = *18 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -384,7 +396,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *18 ->RegressionTreeNode(
+right_node = *19 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
@@ -394,7 +406,7 @@
 split_feature_value = 3.42448291945629535e-13 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *19 ->RegressionTreeLeave(
+missing_leave = *20 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -417,7 +429,7 @@
 right_leave = *0  )
 ;
 left_leave = *0 ;
-right_node = *20 ->RegressionTreeNode(
+right_node = *21 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
@@ -427,7 +439,7 @@
 split_feature_value = 1.54709578481515564e-13 ;
 after_split_error = 0.0218181818181818199 ;
 missing_node = *0 ;
-missing_leave = *21 ->RegressionTreeLeave(
+missing_leave = *22 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -475,7 +487,7 @@
 initial_sum_weights = 150 ;
 example_weights = 150 [ 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.0034013605442176956!
 2 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562!
  0.00340136054421769562 0.00340136054421769562 0.0034013605442!
 1769562 
0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00!
 340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 ] ;
 learners_error = 1 [ 0.0200000000000000004 ] ;
-weak_learner_template = *22 ->RegressionTree(
+weak_learner_template = *23 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -483,19 +495,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *23 ->RegressionTreeLeave(
-id = 0 ;
-missing_leave = 0 ;
-loss_function_weight = 0 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output_confidence_target = 1  )
-;
+leave_template = *8  ;
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;
@@ -559,7 +559,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *0 ;
+leave_template = *8  ;
 root = *26 ->RegressionTreeNode(
 missing_is_valid = 0 ;
 leave = *0 ;
@@ -786,19 +786,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *41 ->RegressionTreeLeave(
-id = 0 ;
-missing_leave = 0 ;
-loss_function_weight = 0 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output_confidence_target = 1  )
-;
+leave_template = *8  ;
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;
@@ -854,7 +842,7 @@
 finalized = 0  )
 ;
 forward_sub_learner_test_costs = 1 ;
-learner_template = *42 ->AdaBoost(
+learner_template = *41 ->AdaBoost(
 weak_learners = []
 ;
 voting_weights = []
@@ -865,7 +853,7 @@
 ;
 learners_error = []
 ;
-weak_learner_template = *43 ->RegressionTree(
+weak_learner_template = *42 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -873,19 +861,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *44 ->RegressionTreeLeave(
-id = 0 ;
-missing_leave = 0 ;
-loss_function_weight = 0 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output_confidence_target = 1  )
-;
+leave_template = *8  ;
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;
@@ -971,10 +947,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *45 ->HyperOptimize(
+strategy = 1 [ *43 ->HyperOptimize(
 which_cost = "E[test2.E[class_error]]" ;
 min_n_trials = 0 ;
-oracle = *46 ->EarlyStoppingOracle(
+oracle = *44 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL10047"
+__REVISION__ = "PL10066"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-03-27 15:53:00 UTC (rev 10068)
@@ -161,19 +161,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *12 ->RegressionTreeLeave(
-id = 0 ;
-missing_leave = 0 ;
-loss_function_weight = 0 ;
-verbosity = 0 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output_confidence_target = 1  )
-;
+leave_template = *9  ;
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;
@@ -229,7 +217,7 @@
 finalized = 0  )
 ;
 forward_sub_learner_test_costs = 1 ;
-learner_template = *13 ->AdaBoost(
+learner_template = *12 ->AdaBoost(
 weak_learners = []
 ;
 voting_weights = []
@@ -240,7 +228,7 @@
 ;
 learners_error = []
 ;
-weak_learner_template = *14 ->RegressionTree(
+weak_learner_template = *13 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -248,19 +236,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *15 ->RegressionTreeLeave(
-id = 0 ;
-missing_leave = 0 ;
-loss_function_weight = 0 ;
-verbosity = 0 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output_confidence_target = 1  )
-;
+leave_template = *9  ;
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;
@@ -346,10 +322,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *16 ->HyperOptimize(
+strategy = 1 [ *14 ->HyperOptimize(
 which_cost = "E[test2.E[class_error]]" ;
 min_n_trials = 0 ;
-oracle = *17 ->EarlyStoppingOracle(
+oracle = *15 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-27 15:53:00 UTC (rev 10068)
@@ -142,7 +142,7 @@
     deepCopyField(compute_train_stats, copies);
     deepCopyField(complexity_penalty_factor, copies);
     deepCopyField(multiclass_outputs, copies);
-    deepCopyField(leave_template, copies);
+//    deepCopyField(leave_template, copies);We don't need to deepCopy it as we only read it
     deepCopyField(sorted_train_set, copies);
     deepCopyField(root, copies);
     deepCopyField(priority_queue, copies);
@@ -279,7 +279,7 @@
     priority_queue = 0;
     split_cols = TVec<int>();
     split_values = Vec();
-    leave_template = 0;
+//    leave_template = 0; we need it to reload a saved learner.
     first_leave = 0;
     //we should not finalize the train_set and the sorted_train_set here 
     //as AdaBoost share it between different weak_learners!

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL10047"
+__REVISION__ = "PL10066"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10047"
+__REVISION__ = "PL10066"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10047"
+__REVISION__ = "PL10066"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL10047"
+__REVISION__ = "PL10066"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class.vmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt	2009-03-27 15:50:26 UTC (rev 10067)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt	2009-03-27 15:53:00 UTC (rev 10068)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10065"
+__REVISION__ = "PL10066"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat



From tihocan at mail.berlios.de  Fri Mar 27 19:14:41 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 27 Mar 2009 19:14:41 +0100
Subject: [Plearn-commits] r10069 - trunk/plearn/math
Message-ID: <200903271814.n2RIEf0E025639@sheep.berlios.de>

Author: tihocan
Date: 2009-03-27 19:14:38 +0100 (Fri, 27 Mar 2009)
New Revision: 10069

Modified:
   trunk/plearn/math/TMat_decl.h
Log:
Minor typo fix in comment

Modified: trunk/plearn/math/TMat_decl.h
===================================================================
--- trunk/plearn/math/TMat_decl.h	2009-03-27 15:53:00 UTC (rev 10068)
+++ trunk/plearn/math/TMat_decl.h	2009-03-27 18:14:38 UTC (rev 10069)
@@ -691,8 +691,8 @@
                 x[k] = row[j];
     }
 
-    //! copy a column to a C vector starting at x
-    //! This is fct is thead safe!
+    //! Copy a column to a C vector starting at x.
+    //! This function is thread safe.
     void copyColumnTo(int col, T* x) const
     {
         T* s = data()+col;



From nouiz at mail.berlios.de  Fri Mar 27 19:51:13 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 27 Mar 2009 19:51:13 +0100
Subject: [Plearn-commits] r10070 - trunk/plearn_learners/regressors
Message-ID: <200903271851.n2RIpDAo025249@sheep.berlios.de>

Author: nouiz
Date: 2009-03-27 19:51:12 +0100 (Fri, 27 Mar 2009)
New Revision: 10070

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h
Log:
gived names to the output.


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2009-03-27 18:51:12 UTC (rev 10070)
@@ -389,10 +389,17 @@
     }
     return costs;
 }
+
+TVec<string> RegressionTree::getOutputNames() const
+{
+    return leave_template->getOutputNames();
+}
+
 PP<RegressionTreeRegisters> RegressionTree::getSortedTrainingSet() const
 {
     return sorted_train_set;
 }
+
 void RegressionTree::computeOutput(const Vec& inputv, Vec& outputv) const
 {
     computeOutputAndNodes(inputv, outputv);

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2009-03-27 18:51:12 UTC (rev 10070)
@@ -109,6 +109,7 @@
     virtual int          outputsize() const {return leave_template->outputsize();}
     virtual TVec<string> getTrainCostNames() const;
     virtual TVec<string> getTestCostNames() const;
+    virtual TVec<string> getOutputNames() const;
     PP<RegressionTreeRegisters> getSortedTrainingSet() const;
     virtual void         computeOutput(const Vec& input, Vec& output) const;
     virtual void         computeOutputAndCosts(const Vec& input,

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-03-27 18:51:12 UTC (rev 10070)
@@ -231,6 +231,15 @@
     if(output_confidence_target) output[1] = conf;
 }
 
+TVec<string> RegressionTreeLeave::getOutputNames() const
+{
+    TVec<string> ret;
+    ret.append("val_pred");
+    if(output_confidence_target)
+        ret.append("confidence");
+    return ret;
+}
+
 void RegressionTreeLeave::printStats()
 {
     cout << " l " << length_;

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-03-27 18:51:12 UTC (rev 10070)
@@ -101,6 +101,7 @@
     inline int           getId()const{return id;}
     inline int           length()const{return length_;}
     virtual void         getOutputAndError(Vec& output, Vec& error)const;
+    virtual TVec<string> getOutputNames() const;
     virtual void         printStats();
     inline real          getWeightsSum(){return weights_sum;}
     inline real          getTargetsSum(){return targets_sum;}

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-03-27 18:51:12 UTC (rev 10070)
@@ -266,6 +266,15 @@
     if(output_confidence_target) output[1] = conf;
 }
 
+TVec<string> RegressionTreeMulticlassLeave::getOutputNames() const
+{
+    TVec<string> ret;
+    ret.append("class_pred");
+    if(output_confidence_target)
+        ret.append("confidence");
+    return ret;
+}
+
 void RegressionTreeMulticlassLeave::printStats()
 {
     cout << " l " << length_;

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2009-03-27 18:51:12 UTC (rev 10070)
@@ -85,6 +85,7 @@
     void         removeRow(int row, Vec outputv, Vec errorv);
     void         removeRow(int row, real target, real weight, Vec outputv, Vec errorv);
     void         getOutputAndError(Vec& output, Vec& error)const;
+    TVec<string> getOutputNames() const;
     void         printStats();
   
 private:

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-03-27 18:51:12 UTC (rev 10070)
@@ -232,6 +232,15 @@
     if(output_confidence_target) output[1] = conf;
 }
 
+TVec<string> RegressionTreeMulticlassLeaveFast::getOutputNames() const
+{
+    TVec<string> ret;
+    ret.append("class_pred");
+    if(output_confidence_target)
+        ret.append("confidence");
+    return ret;
+}
+
 void RegressionTreeMulticlassLeaveFast::printStats()
 {
     cout << " l " << length_;

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h	2009-03-27 18:51:12 UTC (rev 10070)
@@ -83,6 +83,7 @@
     void         removeRow(int row, Vec outputv, Vec errorv);
     void         removeRow(int row, real target, real weight, Vec outputv, Vec errorv);
     void         getOutputAndError(Vec& output, Vec& error)const;
+    TVec<string> getOutputNames() const;
     void         printStats();
   
 private:

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc	2009-03-27 18:51:12 UTC (rev 10070)
@@ -233,6 +233,17 @@
     }
 }
 
+TVec<string> RegressionTreeMulticlassLeaveProb::getOutputNames() const
+{
+    TVec<string> ret(nb_class+1);
+    ret[0]="class_pred";
+    for (int mc_ind = 0; mc_ind < nb_class;mc_ind++)
+    {
+        ret[mc_ind+1]="prob_class_"+tostring(mc_ind);
+    }
+    return ret;
+}
+
 void RegressionTreeMulticlassLeaveProb::printStats()
 {
     cout << " l " << length_;

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h	2009-03-27 18:14:38 UTC (rev 10069)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h	2009-03-27 18:51:12 UTC (rev 10070)
@@ -84,6 +84,7 @@
     void         removeRow(int row, Vec outputv, Vec errorv);
     void         removeRow(int row, real target, real weight, Vec outputv, Vec errorv);
     void         getOutputAndError(Vec& output, Vec& error)const;
+    TVec<string> getOutputNames() const;
     void         printStats();
   
 private:



From dumitruerhan at mail.berlios.de  Mon Mar 30 17:36:12 2009
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Mon, 30 Mar 2009 17:36:12 +0200
Subject: [Plearn-commits] r10071 - trunk/plearn/vmat
Message-ID: <200903301536.n2UFaC52003988@sheep.berlios.de>

Author: dumitruerhan
Date: 2009-03-30 17:36:12 +0200 (Mon, 30 Mar 2009)
New Revision: 10071

Modified:
   trunk/plearn/vmat/CompactFileVMatrix.cc
Log:
(nasty) memory leak

Modified: trunk/plearn/vmat/CompactFileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.cc	2009-03-27 18:51:12 UTC (rev 10070)
+++ trunk/plearn/vmat/CompactFileVMatrix.cc	2009-03-30 15:36:12 UTC (rev 10071)
@@ -488,6 +488,8 @@
                 current_v += max + 1;
         }
     }
+
+    free(buffer);
 }
 
 ///////////////



From dumitruerhan at mail.berlios.de  Mon Mar 30 18:15:14 2009
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Mon, 30 Mar 2009 18:15:14 +0200
Subject: [Plearn-commits] r10072 - trunk/plearn/vmat
Message-ID: <200903301615.n2UGFEqs009957@sheep.berlios.de>

Author: dumitruerhan
Date: 2009-03-30 18:15:13 +0200 (Mon, 30 Mar 2009)
New Revision: 10072

Modified:
   trunk/plearn/vmat/CompactFileVMatrix.cc
Log:
better fix for the memory leak

Modified: trunk/plearn/vmat/CompactFileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.cc	2009-03-30 15:36:12 UTC (rev 10071)
+++ trunk/plearn/vmat/CompactFileVMatrix.cc	2009-03-30 16:15:13 UTC (rev 10072)
@@ -409,12 +409,14 @@
 #endif
 
     unsigned char* buffer;
+    bool m_flag; // to check if we allocated the buffer using malloc
 
     if (in_ram_ && (cache_index[i/8] & (1 << (i%8)))) {
         buffer = (unsigned char*)(cache.data() + (i*compact_width_));
     }
     else {
         buffer = (unsigned char*)malloc(compact_width_);
+        m_flag = true;
 
 #ifdef USE_NSPR_FILE
         moveto(i);
@@ -489,7 +491,8 @@
         }
     }
 
-    free(buffer);
+   if (m_flag)
+        free(buffer);
 }
 
 ///////////////



From dumitruerhan at mail.berlios.de  Mon Mar 30 18:23:26 2009
From: dumitruerhan at mail.berlios.de (dumitruerhan at BerliOS)
Date: Mon, 30 Mar 2009 18:23:26 +0200
Subject: [Plearn-commits] r10073 - trunk/plearn/vmat
Message-ID: <200903301623.n2UGNQ5B011071@sheep.berlios.de>

Author: dumitruerhan
Date: 2009-03-30 18:23:26 +0200 (Mon, 30 Mar 2009)
New Revision: 10073

Modified:
   trunk/plearn/vmat/CompactFileVMatrix.cc
Log:
forgot to init variable

Modified: trunk/plearn/vmat/CompactFileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.cc	2009-03-30 16:15:13 UTC (rev 10072)
+++ trunk/plearn/vmat/CompactFileVMatrix.cc	2009-03-30 16:23:26 UTC (rev 10073)
@@ -409,7 +409,7 @@
 #endif
 
     unsigned char* buffer;
-    bool m_flag; // to check if we allocated the buffer using malloc
+    bool m_flag=false; // to check if we allocated the buffer using malloc
 
     if (in_ram_ && (cache_index[i/8] & (1 << (i%8)))) {
         buffer = (unsigned char*)(cache.data() + (i*compact_width_));



From tihocan at mail.berlios.de  Mon Mar 30 20:05:51 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 30 Mar 2009 20:05:51 +0200
Subject: [Plearn-commits] r10074 - trunk/python_modules/plearn/parallel
Message-ID: <200903301805.n2UI5psP029580@sheep.berlios.de>

Author: tihocan
Date: 2009-03-30 20:05:50 +0200 (Mon, 30 Mar 2009)
New Revision: 10074

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Minor typo/grammar fixes in output

Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-03-30 16:23:26 UTC (rev 10073)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-03-30 18:05:50 UTC (rev 10074)
@@ -727,7 +727,7 @@
         # Execute pre-batch
         self.exec_pre_batch()
 
-        print "[DBI] All the log will be in the directory: ",self.log_dir
+        print "[DBI] All logs will be in the directory: ",self.log_dir
         # Launch bqsubmit
         if not self.test:
             for t in self.tasks:
@@ -736,11 +736,11 @@
             self.p.wait()
             
             if self.p.returncode!=0:
-                raise DBIError("[DBI] ERROR: the bqsubmit returned an error code of"+str(self.p.returncode))
+                raise DBIError("[DBI] ERROR: bqsubmit returned an error code of"+str(self.p.returncode))
         else:
-            print "[DBI] Test mode, we generate all the file, but we do not execute bqsubmit"
+            print "[DBI] Test mode, we generated all files, but will not execute bqsubmit"
             if self.dolog:
-                print "[DBI] The scheduling time will not be logged when you will submit the generated file"
+                print "[DBI] The scheduling time will not be logged when you submit the generated file"
 
         # Execute post-batchs
         self.exec_post_batch()



From nouiz at mail.berlios.de  Mon Mar 30 22:40:43 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 30 Mar 2009 22:40:43 +0200
Subject: [Plearn-commits] r10075 - trunk/plearn/misc
Message-ID: <200903302040.n2UKehXv016780@sheep.berlios.de>

Author: nouiz
Date: 2009-03-30 22:40:43 +0200 (Mon, 30 Mar 2009)
New Revision: 10075

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
in plearn vmat diff, we compare the mateinfo(field type and filename)


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2009-03-30 18:05:50 UTC (rev 10074)
+++ trunk/plearn/misc/vmatmain.cc	2009-03-30 20:40:43 UTC (rev 10075)
@@ -286,6 +286,31 @@
             << m2.length() << " x " << m2.width() << endl;
         return -1;
     }
+    if(m1->getFieldInfos()!=m2->getFieldInfos()){
+        Array<VMField> a1=m1->getFieldInfos();
+        Array<VMField> a2=m2->getFieldInfos();
+        if(verbose)
+            pout << "Field infos differ:";
+        //compare fieldnames
+        for(int i=0;i<m1.width();i++){
+            if(a1[i].name!=a2[i].name){
+                ++ndiff;
+                if(verbose)
+                    pout << " " << a1[i].name << "!=" << a2[i].name;
+            }
+        }
+        //compare fieldtype
+        for(int i=0;i<m1.width();i++){
+            if(a1[i].fieldtype!=a2[i].fieldtype){
+                ++ndiff;
+                if(verbose)
+                    pout << " " << a1[i].fieldtype << "!=" << a2[i].fieldtype;
+            }
+        }
+        if(verbose)
+            pout <<endl;
+        
+    }
     int l = m1.length();
     int w = m1.width();
     Vec v1(w);



From nouiz at mail.berlios.de  Mon Mar 30 22:41:48 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 30 Mar 2009 22:41:48 +0200
Subject: [Plearn-commits] r10076 - in trunk:
 plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results
 plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata
 !
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test1_outputs.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test2_outputs.pmat.metadata
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_!
 RegressionTree_MultiClassProb/expected_results/expdir/Split0/t! est1_out
Message-ID: <200903302041.n2UKfmrm016872@sheep.berlios.de>

Author: nouiz
Date: 2009-03-30 22:41:47 +0200 (Mon, 30 Mar 2009)
New Revision: 10076

Removed:
   trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/FieldInfo/
   trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/test_processing_vmatrix.pymat.metadata/
Modified:
   trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat
   trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/fieldnames
   trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt
Log:
updated the test following the last commit.


Modified: trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,26 +1,26 @@
-.0	0
-.1	0
-.2	0
-.3	0
-.4	0
-.5	0
-.6	0
-.7	0
-.8	0
-.9	0
-.10	0
-.11	0
-.12	0
-.13	0
-.14	0
-.15	0
-.16	0
-.17	0
-.18	0
-.19	0
-.20	0
-.21	0
-.22	0
-.23	0
-.24	0
-.25	0
+output0	0
+output1	0
+weekday	0
+weekday0	0
+weekday1	0
+weekday2	0
+weekday3	0
+oddweek	0
+month	0
+month1	0
+month2	0
+month3	0
+month4	0
+month5	0
+month6	0
+month7	0
+month8	0
+month9	0
+month10	0
+month11	0
+monthfrac	0
+endofmonth	0
+begofmonth	0
+0	0
+1	0
+2	0

Modified: trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/sizes
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/sizes	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn/vmat/test/.pytest/PL_processing_and_plearneroutput_vmatrix/expected_results/processing_vmatrix_output.pmat.metadata/sizes	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1 +1 @@
-26 0 0 
+26 0 0 0 

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-out0	0
-out1	0
+val_pred	0
+confidence	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-out0	0
-out1	0
+val_pred	0
+confidence	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL10066"
+__REVISION__ = "PL10068"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-out0	0
-out1	0
+class_pred	0
+confidence	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-out0	0
-out1	0
+class_pred	0
+confidence	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10066"
+__REVISION__ = "PL10068"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-out0	0
-out1	0
+class_pred	0
+confidence	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-out0	0
-out1	0
+class_pred	0
+confidence	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassFast/expected_results/expdir/metainfos.txt	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10066"
+__REVISION__ = "PL10068"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-out0	0
-out1	0
+val_pred	0
+confidence	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-out0	0
-out1	0
+val_pred	0
+confidence	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL10066"
+__REVISION__ = "PL10068"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class.vmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,3 +1,3 @@
-out0	0
-out1	0
-out2	0
+class_pred	0
+prob_class_0	0
+prob_class_1	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,3 +1,3 @@
-out0	0
-out1	0
-out2	0
+class_pred	0
+prob_class_0	0
+prob_class_1	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt	2009-03-30 20:40:43 UTC (rev 10075)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassProb/expected_results/expdir/metainfos.txt	2009-03-30 20:41:47 UTC (rev 10076)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL10066"
+__REVISION__ = "PL10068"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat



From nouiz at mail.berlios.de  Mon Mar 30 22:45:25 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 30 Mar 2009 22:45:25 +0200
Subject: [Plearn-commits] r10077 - trunk/plearn_learners/cgi
Message-ID: <200903302045.n2UKjPJc017248@sheep.berlios.de>

Author: nouiz
Date: 2009-03-30 22:45:25 +0200 (Mon, 30 Mar 2009)
New Revision: 10077

Modified:
   trunk/plearn_learners/cgi/ConfigParsing.cc
Log:
DichotomizeVMatrix don't support regex.


Modified: trunk/plearn_learners/cgi/ConfigParsing.cc
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-30 20:41:47 UTC (rev 10076)
+++ trunk/plearn_learners/cgi/ConfigParsing.cc	2009-03-30 20:45:25 UTC (rev 10077)
@@ -129,7 +129,10 @@
             f_imputation << endl;
         }
         if(!r[5].empty() && !remove){
-            f_dichotomize <<r[0]<<" : ["<< (r[5]) << " ]"<<endl;
+            string s = r[0];
+            if(s[s.length()-1]=='*')
+                s=s.substr(0,s.length()-1);
+            f_dichotomize << s <<" : ["<< r[5] << " ]"<<endl;
         }
     }
 }



