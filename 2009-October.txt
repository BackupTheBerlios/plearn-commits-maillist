From nouiz at mail.berlios.de  Tue Oct  6 16:35:54 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 6 Oct 2009 16:35:54 +0200
Subject: [Plearn-commits] r10301 - trunk/plearn_learners/regressors
Message-ID: <200910061435.n96EZswK012939@sheep.berlios.de>

Author: nouiz
Date: 2009-10-06 16:35:52 +0200 (Tue, 06 Oct 2009)
New Revision: 10301

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
allow to reload old version of save RegressionTreeNode.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-09-17 19:32:46 UTC (rev 10300)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-10-06 14:35:52 UTC (rev 10301)
@@ -55,8 +55,12 @@
                         "It may be an expanded node pointing to 3 children nodes: a leave for missing values on the splitting attribute,\n"
                         "a left leave for samples with values below the value of the splitting attribute, and a right leave for the others,\n"
     );
+
 int RegressionTreeNode::dummy_int = 0;
 Vec RegressionTreeNode::tmp_vec;
+PP<RegressionTreeLeave> RegressionTreeNode::dummy_leave_template;
+PP<RegressionTreeRegisters> RegressionTreeNode::dummy_train_set;
+
 RegressionTreeNode::RegressionTreeNode():
     missing_is_valid(0),
     split_col(-1),
@@ -163,6 +167,25 @@
     declareStaticOption(ol, "inputsize", &RegressionTreeNode::dummy_int,
                   OptionBase::learntoption | OptionBase::nosave,
                   "DEPRECATED The inputsize of the train set\n");
+    declareStaticOption(ol, "inputsize", &RegressionTreeNode::dummy_int,
+                  OptionBase::learntoption | OptionBase::nosave,
+                  "DEPRECATED The inputsize of the train set\n");
+    declareStaticOption(ol, "loss_function_weight", 
+                  &RegressionTreeNode::dummy_int,
+                  OptionBase::learntoption | OptionBase::nosave,
+                  "DEPRECATED Only to reload old saved learner\n");
+    declareStaticOption(ol, "verbosity", 
+                  &RegressionTreeNode::dummy_int,
+                  OptionBase::learntoption | OptionBase::nosave,
+                  "DEPRECATED Only to reload old saved learner\n");
+    declareStaticOption(ol, "leave_template", 
+                  &RegressionTreeNode::dummy_leave_template,
+                  OptionBase::learntoption | OptionBase::nosave,
+                  "DEPRECATED Only to reload old saved learner\n");
+    declareStaticOption(ol, "train_set", 
+                  &RegressionTreeNode::dummy_train_set,
+                  OptionBase::learntoption | OptionBase::nosave,
+                  "DEPRECATED Only to reload old saved learner\n");
 
     inherited::declareOptions(ol);
 }

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2009-09-17 19:32:46 UTC (rev 10300)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2009-10-06 14:35:52 UTC (rev 10301)
@@ -88,8 +88,11 @@
     PP<RegressionTreeNode> right_node;
     PP<RegressionTreeLeave> right_leave;
     
+    //only there to reload old version. Put static to use less space.
     static int dummy_int;
     static Vec tmp_vec;
+    static PP<RegressionTreeLeave> dummy_leave_template;
+    static PP<RegressionTreeRegisters> dummy_train_set;
 public:  
     RegressionTreeNode();
     RegressionTreeNode(int missing_is_valid);



From ducharme at mail.berlios.de  Tue Oct  6 21:03:53 2009
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 6 Oct 2009 21:03:53 +0200
Subject: [Plearn-commits] r10302 - trunk/python_modules/plearn/table
Message-ID: <200910061903.n96J3r6c025606@sheep.berlios.de>

Author: ducharme
Date: 2009-10-06 21:03:52 +0200 (Tue, 06 Oct 2009)
New Revision: 10302

Modified:
   trunk/python_modules/plearn/table/date.py
Log:
"daydiff" function now works with YYYYMMDD dates


Modified: trunk/python_modules/plearn/table/date.py
===================================================================
--- trunk/python_modules/plearn/table/date.py	2009-10-06 14:35:52 UTC (rev 10301)
+++ trunk/python_modules/plearn/table/date.py	2009-10-06 19:03:52 UTC (rev 10302)
@@ -204,5 +204,5 @@
     elif type(date)==int:
         return dateint_to_date(date)
         
-def daydiff(cyymmdd1, cyymmdd2):
-    return (CYYMMDD_to_date(cyymmdd1)-CYYMMDD_to_date(cyymmdd2)).days
+def daydiff(date1, date2):
+    return (dateint_to_date(date1) - dateint_to_date(date2)).days



From nouiz at mail.berlios.de  Tue Oct  6 22:33:55 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 6 Oct 2009 22:33:55 +0200
Subject: [Plearn-commits] r10303 - trunk/commands/PLearnCommands
Message-ID: <200910062033.n96KXtOs001517@sheep.berlios.de>

Author: nouiz
Date: 2009-10-06 22:33:55 +0200 (Tue, 06 Oct 2009)
New Revision: 10303

Modified:
   trunk/commands/PLearnCommands/VMatCommand.cc
Log:
print the column of the maximum value.


Modified: trunk/commands/PLearnCommands/VMatCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatCommand.cc	2009-10-06 19:03:52 UTC (rev 10302)
+++ trunk/commands/PLearnCommands/VMatCommand.cc	2009-10-06 20:33:55 UTC (rev 10303)
@@ -138,7 +138,7 @@
         "   or: vmat mtime <dataset>\n"
         "       Print the mtime of a dataset\n"
         "   or: vmat pmat_float_save <dataset>...\n"
-        "       Print the number of byte saved if we transform the mat to pmat in float format and the maximum diff?rence and maximum relative diff?rence in value."
+        "       Print the size of the new pmat file in float format, the size of saved bytes, the maximum value difference, the maximum relative difference in value and the column of the maximum value and maximum relative value."
         "<dataset> is a parameter understandable by getDataSet: \n"
 
         + getDataSetHelp()
@@ -187,28 +187,35 @@
                 
             VMat vm = getDataSet(dataspec);
             int64_t orig_size = vm->getSizeOnDisk();
+            int saved_size=-1;
+            int new_size=-1;
+            if(orig_size!=-1){
+                FileVMatrix n = FileVMatrix(dataspec+"dummy",vm.length(),vm.width(),true,false);
+                new_size=n.getSizeOnDisk();
+                saved_size=orig_size-new_size;
+            }
 
-            if(orig_size==-1)
-                cout<<"-1 -1 -1"<<endl;
-            else{
-                FileVMatrix n = FileVMatrix(dataspec+"dummy",vm.length(),vm.width(),true,false);
-                int64_t new_size = n.getSizeOnDisk();
-                Vec v(vm->width());
-                double max_diff=0;
-                double max_rel_diff=0;
-                for(int i=0;i<vm->length();i++){
-                    vm->getRow(i,v);
-                    for(int j=0;j<vm->width();j++){
-                        double diff = v[j]-float(v[j]);
-                        if(max_diff<diff)
-                            max_diff=diff;
-                        double rel_diff = diff/v[j];
-                        if(max_rel_diff<rel_diff)
-                            max_rel_diff=rel_diff;
+            Vec v(vm->width());
+            double max_diff=0;
+            double max_rel_diff=0;
+            int col_max = -1;
+            int col_max_rel = -1;
+            for(int i=0;i<vm->length();i++){
+                vm->getRow(i,v);
+                for(int j=0;j<vm->width();j++){
+                    double diff = v[j]-float(v[j]);
+                    if(max_diff<diff){
+                        max_diff=diff;
+                        col_max = j;
                     }
+                    double rel_diff = diff/v[j];
+                    if(max_rel_diff<rel_diff){
+                        max_rel_diff=rel_diff;
+                        col_max_rel = j;
+                    }
                 }
-                cout << orig_size-new_size <<" "<< max_diff <<" "<<max_rel_diff<<endl;
             }
+            cout<<"new_size="<<new_size<<" saved_size="<< saved_size <<" max_difference="<< max_diff <<" max_relatif_difference="<<max_rel_diff<<" col_max="<<col_max<<" col_max_rel="<<col_max_rel<<endl;
         }
     }
     else



From tihocan at mail.berlios.de  Thu Oct  8 16:56:40 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 8 Oct 2009 16:56:40 +0200
Subject: [Plearn-commits] r10304 - trunk/python_modules/plearn/learners
Message-ID: <200910081456.n98Euecu003287@sheep.berlios.de>

Author: tihocan
Date: 2009-10-08 16:56:40 +0200 (Thu, 08 Oct 2009)
New Revision: 10304

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
Changes that Jerome forgot to commit before leaving

Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2009-10-06 20:33:55 UTC (rev 10303)
+++ trunk/python_modules/plearn/learners/SVM.py	2009-10-08 14:56:40 UTC (rev 10304)
@@ -670,6 +670,16 @@
                                     calling svm_model. This allow to use locally modified versions of libsvm.
                                     e.g: dict(max_iter= 10000) for the libsvm version installed at ApStat.
 
+        'intern_kfold': <list> of <list> (or None) list of indices to use for the internal k-folds (if provided).
+                        The list has to be exactly of length n_fold (for safety), and each element is a list of
+                        length two (first list for train indices, second list of test indices).
+
+        'recompute_stats_at_each_train': <bool> Do we re-estimate train statistics each time we call train()
+                        The value of this option has an impact only in the following cases:
+                        * if balanceC is activated
+                          (weights of penalty C will be computed at each train).
+                        * if you normalize inputs.
+
     learnt options:
 
         'best_model':
@@ -733,6 +743,8 @@
                         'results_filename',
                         'preproc_optionnames',
                         'preproc_optionvalues',
+                        'intern_kfold',
+                        'recompute_stats_at_each_train',
                         \
                         'validtype',
                         'param',
@@ -766,8 +778,10 @@
         self.train_stats     = None
         self.validtype       = 'simple'
         self.multiclass_strategy = 'onevsone'
-        
-        self.n_fold   = 5
+
+        self.n_fold = 5
+        self.intern_kfold = None
+        self.recompute_stats_at_each_train = False
         self.balanceC = False
         self.balance_classes = False
         self.normalize_inputs = False
@@ -1171,6 +1185,8 @@
             print "launching libsvm with param %s " % param    
         
         trainset = self.train_inputspec(dataspec)
+        if self.recompute_stats_at_each_train:
+            self.stats_are_uptodate= False
         self.get_data_stats( trainset )
 
         train_samples, train_targets = self.get_svminputlist( trainset, True )
@@ -1453,58 +1469,81 @@
             print "   with param %s" % param
         
         trainset = self.train_inputspec(dataspec)
-        trainset_class = [None]*nclasses
-        N=[0]*nclasses
-        Nfold=[0]*nclasses
-        Nlastfold=[0]*nclasses
-        for c in range(nclasses):
-            trainset_class[c] = ClassSubsetVMatrix( source = trainset,
-                                                    classes = [c],)
-            N[c] = trainset_class[c].length
-            Nfold[c] = int(N[c] / n_fold)
-            if Nfold[c] == 0:
-                raise ValueError, "%d-fold cross-validation is not possible as class %d has only %d samples" % \
-                                 (n_fold, c, N[c])
-            Nlastfold[c] = N[c] - Nfold[c] * (n_fold-1)
+        if not self.intern_kfold:
+            # determining an internal k-fold that keep the class proportions inside each split
+            trainset_class = [None]*nclasses
+            N=[0]*nclasses
+            Nfold=[0]*nclasses
+            Nlastfold=[0]*nclasses
+            for c in range(nclasses):
+                trainset_class[c] = ClassSubsetVMatrix( source = trainset,
+                                                        classes = [c],)
+                N[c] = trainset_class[c].length
+                Nfold[c] = int(N[c] / n_fold)
+                if Nfold[c] == 0:
+                    raise ValueError, "%d-fold cross-validation is not possible as class %d has only %d samples" % \
+                                     (n_fold, c, N[c])
+                Nlastfold[c] = N[c] - Nfold[c] * (n_fold-1)
+            sub_trainset_class = [None]*nclasses
+            sub_testset_class = [None]*nclasses
         
         validstats = None
         if return_outputs:
             validoutputs = numpy.zeros((trainset.length, nclasses))
-        sub_trainset_class = [None]*nclasses
-        sub_testset_class = [None]*nclasses
-        for i in range(n_fold):
-            test_indices = []
-            for c in range(nclasses):
-                if i < n_fold-1:
-                    test_indices_class = range( i*Nfold[c], (i+1)*Nfold[c] )
-                    train_indices_class = range( 0,i*Nfold[c])+range((i+1)*Nfold[c], N[c] )
-                else:
-                    test_indices_class = range( i*Nfold[c], N[c] )
-                    train_indices_class = range( 0, N[c]-Nlastfold[c] )
-                sub_testset_class[c] = SelectRowsVMatrix(
-                                    source = trainset_class[c],
-                                    indices = test_indices_class,)
-                sub_trainset_class[c] = SelectRowsVMatrix(
-                                    source = trainset_class[c],
-                                    indices = train_indices_class,)
-                test_indices += test_indices_class
-            sub_testset = ConcatRowsVMatrix( sources = sub_testset_class,
-                                             fully_check_mappings = False,)
-            sub_trainset = ConcatRowsVMatrix( sources = sub_trainset_class,
-                                             fully_check_mappings = False,)
+            
+        for i_fold in range(n_fold):
+        
+            if self.intern_kfold:
+                assert isinstance(self.intern_kfold,list)
+                assert len(self.intern_kfold) == n_fold
+                assert isinstance(self.intern_kfold[0],list)
+                assert len(self.intern_kfold[0]) == 2
+                train_indices = self.intern_kfold[i_fold][0]
+                test_indices = self.intern_kfold[i_fold][1]
+                
+                print "loading suggested k-fold train: ", len(train_indices), "  -- test: ", len(test_indices)
+                
+                sub_testset = SelectRowsVMatrix(
+                                source = trainset,
+                                indices = test_indices,)
+                sub_trainset= SelectRowsVMatrix(
+                                source = trainset,
+                                indices = train_indices,)
+            else:
+                test_indices = []
+                for c in range(nclasses):
+                    if i_fold < n_fold-1:
+                        test_indices_class = range( i_fold*Nfold[c], (i_fold+1)*Nfold[c] )
+                        train_indices_class = range( 0,i_fold*Nfold[c])+range((i_fold+1)*Nfold[c], N[c] )
+                    else:
+                        test_indices_class = range( i_fold*Nfold[c], N[c] )
+                        train_indices_class = range( 0, N[c]-Nlastfold[c] )
+                    sub_testset_class[c] = SelectRowsVMatrix(
+                                        source = trainset_class[c],
+                                        indices = test_indices_class,)
+                    sub_trainset_class[c] = SelectRowsVMatrix(
+                                        source = trainset_class[c],
+                                        indices = train_indices_class,)
+                    test_indices += test_indices_class
+                sub_testset = ConcatRowsVMatrix( sources = sub_testset_class,
+                                                 fully_check_mappings = False,)
+                sub_trainset = ConcatRowsVMatrix( sources = sub_trainset_class,
+                                                 fully_check_mappings = False,)
             if return_outputs:
-                validstats, outputs = self.simplevalid(  {self.trainset_key:sub_trainset,
-                                                self.validset_key:sub_testset},
-                                                param,
-                                                validstats = validstats, verbose = False, return_outputs = True)
-                #validoutputs[test_indices,:] = outputs
+                validstats, outputs = self.simplevalid( 
+                                        {   self.trainset_key:sub_trainset,
+                                            self.validset_key:sub_testset},
+                                        param,
+                                        validstats = validstats, verbose = False, return_outputs = True)
+                for i, o in enumerate(outputs):
+                    validoutputs[test_indices[i]] = o
                 validoutputs = numpy.array(list(validoutputs)+list(outputs))
             else:
-                validstats = self.simplevalid(  {self.trainset_key:sub_trainset,
-                                                self.validset_key:sub_testset},
-                                                param,
-                                                validstats = validstats, verbose = False)
-
+                validstats = self.simplevalid(
+                                        {   self.trainset_key:sub_trainset,
+                                            self.validset_key:sub_testset},
+                                        param,
+                                        validstats = validstats, verbose = False)
         if return_outputs:
             return validstats, validoutputs
         else:



From ducharme at mail.berlios.de  Thu Oct  8 22:23:57 2009
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Thu, 8 Oct 2009 22:23:57 +0200
Subject: [Plearn-commits] r10305 - trunk/python_modules/plearn/gui_tools
Message-ID: <200910082023.n98KNvaF030629@sheep.berlios.de>

Author: ducharme
Date: 2009-10-08 22:23:57 +0200 (Thu, 08 Oct 2009)
New Revision: 10305

Modified:
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
Meilleur check pour la variable _HAS_DISPLAY_


Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2009-10-08 14:56:40 UTC (rev 10304)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2009-10-08 20:23:57 UTC (rev 10305)
@@ -40,13 +40,20 @@
 import datetime
 
 _HAS_DISPLAY_ = os.environ.has_key("DISPLAY")
+## Check the usability of the display
+if _HAS_DISPLAY_:
+    try:
+        from enthought.pyface.api import confirm, YES
+    except:
+        _HAS_DISPLAY_ = False
+        #print '_HAS_DISPLAY_ = False'
 
 ## Traits
 from enthought.traits.api          import *
 if _HAS_DISPLAY_:
+    from enthought.pyface.api          import confirm, YES
     from enthought.traits.ui.api       import *
     from enthought.traits.ui.menu      import NoButtons
-    from enthought.pyface.api          import confirm, YES
     from console_logger import ConsoleLogger
 else:
     ## Dummy definitions for some Traits UI stuff
@@ -62,12 +69,11 @@
     confirm   = None
     YES       = None
     Directory = Str
+    File      = Str
 
     class ConsoleLogger(object): pass
     class Handler(object): pass
-    
 
-
 ## If there is no display, set the matplotlib backend to Agg
 if not _HAS_DISPLAY_:
     import matplotlib



From tihocan at mail.berlios.de  Thu Oct 15 22:02:31 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 15 Oct 2009 22:02:31 +0200
Subject: [Plearn-commits] r10306 - trunk/plearn/vmat
Message-ID: <200910152002.n9FK2VMr006011@sheep.berlios.de>

Author: tihocan
Date: 2009-10-15 22:02:30 +0200 (Thu, 15 Oct 2009)
New Revision: 10306

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Declared length() as remote method (under name getLength since length is already taken)

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2009-10-08 20:23:57 UTC (rev 10305)
+++ trunk/plearn/vmat/VMatrix.cc	2009-10-15 20:02:30 UTC (rev 10306)
@@ -216,6 +216,11 @@
          RetDoc ("The content of this VMatrix as a Mat")));
 
     declareMethod(
+            rmm, "getLength", &VMatrix::length,
+            (BodyDoc("Return length of this VMatrix.\n"),
+             RetDoc("The length of this VMatrix.")));
+
+    declareMethod(
         rmm, "declareField", &VMatrix::declareField,
         (BodyDoc("Declares the field infos for a given column (index).\n"),
          ArgDoc ("fieldindex", "The column index.\n"),



From tihocan at mail.berlios.de  Thu Oct 15 22:02:57 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 15 Oct 2009 22:02:57 +0200
Subject: [Plearn-commits] r10307 - trunk/python_modules/plearn/learners
Message-ID: <200910152002.n9FK2v8F006078@sheep.berlios.de>

Author: tihocan
Date: 2009-10-15 22:02:57 +0200 (Thu, 15 Oct 2009)
New Revision: 10307

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
Cosmetic change

Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2009-10-15 20:02:30 UTC (rev 10306)
+++ trunk/python_modules/plearn/learners/SVM.py	2009-10-15 20:02:57 UTC (rev 10307)
@@ -1501,7 +1501,7 @@
                 train_indices = self.intern_kfold[i_fold][0]
                 test_indices = self.intern_kfold[i_fold][1]
                 
-                print "loading suggested k-fold train: ", len(train_indices), "  -- test: ", len(test_indices)
+                print "Loading suggested intern k-fold --  train: ", len(train_indices), "  -- test: ", len(test_indices)
                 
                 sub_testset = SelectRowsVMatrix(
                                 source = trainset,



From tihocan at mail.berlios.de  Thu Oct 22 18:05:09 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 22 Oct 2009 18:05:09 +0200
Subject: [Plearn-commits] r10308 - trunk/python_modules/plearn/pybridge
Message-ID: <200910221605.n9MG59dB024363@sheep.berlios.de>

Author: tihocan
Date: 2009-10-22 18:05:08 +0200 (Thu, 22 Oct 2009)
New Revision: 10308

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
Calling len(vmat) now calls the underlying length() method instead of accessing directly the length field

Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2009-10-15 20:02:57 UTC (rev 10307)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2009-10-22 16:05:08 UTC (rev 10308)
@@ -212,7 +212,7 @@
     supplies __len__ and __getitem__ methods.
     """
     def __len__(self):
-        return self.length
+        return self.getLength()
 
     def __getitem__(self, key):
         class len_thunk(object):



